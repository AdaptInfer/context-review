## Explicit Adaptivity: Structured Estimation of $f(c)$

The limitations of population-based modeling and the need for context-adaptive approaches, as outlined above, motivate the explicit modeling of parameter variation as a function of observed context.

Modern datasets often exhibit significant heterogeneity across individuals, spatial locations, time periods, or experimental conditions, which fundamentally challenges the traditional assumption of globally shared parameters [@botzer2025publication; @fan2025network]. In response, a range of models has been developed that allow sample-specific parameters $\theta_i$ to depend on observed context $c_i$, by placing structured assumptions on the function $f(c)$ that maps context to parameter [@hastie1993varying; @li2024semi]. This section focuses on explicit modeling approaches for $f(c)$, in which the relationship between context and parameter is estimated directly or regularized through structural constraints [@wang2025boosted; @cheng2020xgboost].

Such approaches provide a clear and interpretable foundation for context-aware inference, which is especially valuable when personalization or local adaptivity is required [@murakami2025fast; @englert2025spatially]. The varying-coefficient model (VCM) is a central example, modeling the conditional parameter $\theta_i = f(c_i)$ as a smooth or otherwise structured function. This section begins by reviewing classical VCM formulations and then discusses recent developments that extend these ideas to high-dimensional, structured, or non-Euclidean context spaces [@hu2024varying; @luo2024urban; @shi2021graph].

### Varying-Coefficient Models

Varying-coefficient models (VCMs) offer a systematic approach for modeling heterogeneity that depends on observed covariates, by allowing model parameters to change smoothly as a function of context [@hastie1993varying; @botzer2025publication]. In this framework, each observation $x_i$ has an associated parameter $\theta_i = f(c_i)$, where $f$ is an unknown function mapping the observed context $c_i$ into the parameter space. Classical VCMs restrict $f$ to be smooth and low-dimensional, which provides a tractable setting for both estimation and theoretical analysis [@li2024semi; @fan2025network].

This modeling strategy was originally proposed in the context of semiparametric regression by Hastie and Tibshirani [@hastie1993varying], and it has since found widespread applications in areas such as crime analysis, environmental science, and personalized prediction problems [@botzer2025publication; @murakami2025fast]. The core assumption is that the coefficients vary smoothly with covariates, which justifies the use of techniques such as kernel regression, local polynomial fitting, or spline-based methods for estimation [@li2024semi].

Under the standard VCM specification, the model can be written as
$$
y_i = \sum_{j=1}^{p} \beta_j(c_i) x_{ij} + \varepsilon_i,
$$
where each coefficient function $\beta_j(c)$ is smooth but otherwise unspecified. By moving beyond global or fixed-effect models, VCMs achieve greater modeling flexibility and can capture complex patterns of heterogeneity, while avoiding the instability that comes with fully nonparametric estimation in high-dimensional settings [@botzer2025publication; @fan2025network].

### Recent Advances in Varying-Coefficient Models

Varying-coefficient models have developed far beyond their origins in classical smoothing frameworks and are now recognized as a broad and flexible family of tools for modeling context-dependent relationships [@botzer2025publication; @hastie1993varying; @li2024semi]. Recent work has enriched the VCM framework on multiple fronts, introducing stronger theoretical guarantees, new strategies for adaptive estimation, methods that extend to high-dimensional covariates, and integration with modern machine learning approaches [@fan2025network; @wang2025boosted; @cheng2020xgboost].

One notable advance has been the formalization of VCMs as a general framework for heterogeneity modeling, positioning them not only as instruments for continuous moderation but also as foundational components for structured models such as additive models, interaction effects, and even certain causal inference structures [@botzer2025publication; @fan2025network]. The theory underpinning VCMs has become more robust, benefiting from rigorous analyses of convergence rates, asymptotic distributions, and identifiability conditions under varying degrees of smoothness [@li2024semi; @fan2025network].

On the methodological side, VCMs have embraced a wider spectrum of estimation techniques beyond traditional kernel smoothing. Researchers have adopted local polynomial regression, penalized splines, basis expansion, wavelet-based methods, and reproducing kernel Hilbert space approaches [@hastie1993varying; @li2024semi; @botzer2025publication]. These tools permit the model to adapt flexibly to local features of the data, with overfitting controlled through regularization or the data-driven selection of tuning parameters [@li2024semi].

Addressing the challenge of high-dimensionality, recent developments have introduced sparse VCMs that impose structural assumptions on the coefficient functions. These models employ penalties such as group lasso, fused lasso, or hierarchical regularization to select relevant predictors and reveal important context-specific interactions [@cheng2020xgboost; @wang2025boosted; @fan2025network]. As a result, interpretable modeling is made feasible even when the number of covariates or the complexity of the context is substantial [@botzer2025publication; @fan2025network].

VCMs have also been extended to handle structured data, including spatial, temporal, and network domains. Domain-specific information is incorporated into the estimation, for example, through Laplacian regularization or graph-based smoothing, so that the resulting models respect the underlying data topology [@hu2024varying; @luo2024urban; @murakami2025fast; @shi2021graph; @englert2025spatially]. This structured modeling approach links VCMs with techniques in manifold learning, graphical models, and structured additive frameworks [@botzer2025publication].

From a broader perspective, VCMs are now central to many applied fields. Their flexibility has made them especially useful in applications such as personalized medicine, adaptive treatment strategies, behavioral science, and forecasting in dynamic environments [@botzer2025publication; @murakami2025fast; @fan2025network]. Because these models can capture effect modification by context, they are well suited to scientific questions where heterogeneity is of primary importance [@botzer2025publication; @li2024semi; @fan2025network].

In summary, the ongoing development of VCMs has established them as a powerful and interpretable class of models. The combination of rigorous statistical theory, advanced computational tools, and the ability to incorporate domain-specific knowledge ensures that VCMs will remain at the forefront of research into context-adaptive modeling [@botzer2025publication; @fan2025network].

### Flexible Functional Forms

In order to move beyond the limitations of simple linear or low-degree polynomial specifications for $f(c)$, researchers have proposed a wide range of more flexible representations to model context-dependent variation in parameters [@hastie1993varying; @botzer2025publication; @li2024semi]. The principal goal of these approaches is to increase the expressiveness of the model while retaining interpretability and maintaining theoretical guarantees.

**Basis Expansions and Spline Representations.**
A classical extension of the varying-coefficient model represents each coefficient function as a linear combination of pre-specified basis functions [@hastie1993varying; @botzer2025publication]:
$$
\beta_j(c) = \sum_{k=1}^K \alpha_{jk} \phi_k(c),
$$
where $\phi_k(c)$ might include polynomial, Fourier, or B-spline bases, and the coefficients $\alpha_{jk}$ are estimated from the data.

Among these, B-splines are particularly favored for their ability to achieve local support and smoothness control. In practical applications, penalized splines (or P-splines) are commonly employed, introducing a roughness penalty to balance model fit and smoothness [@botzer2025publication; @li2024semi]:
$$
\min_\alpha \sum_{i=1}^n \left( y_i - \sum_j x_{ij} \sum_k \alpha_{jk} \phi_k(c_i) \right)^2 + \lambda \sum_j \int [\beta_j''(c)]^2 dc.
$$
This penalization framework allows for effective regularization, especially in complex or high-dimensional settings.

**Learned Context via Latent Space Models.**
A significant recent development extends VCMs to cases where the context $c_i$ is not directly observed but can be inferred from structured data, such as a network. The Network VCM, for example, models coefficients as functions of latent "locations" of network nodes that are estimated simultaneously with the model parameters [@fan2025network].
$$
\beta_i = B z_i + \gamma,
$$
where $z_i$ is the unobserved latent context vector for node $i$, and its relationship with the coefficients is learned via a factor-loading matrix $B$. This approach powerfully generalizes the VCM framework to handle relational data and unobserved sources of heterogeneity.

**Exponential Family and Generalized VC Models.**
Flexible forms of $f(c)$ have also been proposed within the framework of generalized linear models, in which the response $y$ follows an exponential family distribution and the canonical parameter is allowed to depend on $c$ [@botzer2025publication; @hastie1993varying; @li2024semi]:
$$
g(\mu_i) = x_i^\top \beta(c_i),
$$
where $g$ is a known link function. Estimation in these models is typically carried out using local likelihood methods or generalized additive modeling, providing both theoretical soundness and practical flexibility [@botzer2025publication].

These advances in functional specification have greatly increased the scope of varying-coefficient models and have enabled their application to a wider variety of scientific and engineering problems.

### Integration with State-of-the-Art Machine Learning

Varying-coefficient models offer a principled framework for context-aware modeling. Nevertheless, classical estimation techniques often lack sufficient expressive power when the context $c$ is high-dimensional, unstructured, or involves complex objects such as images or text. Recent advances in machine learning have introduced flexible and scalable tools to approximate the coefficient function $f(c)$, greatly extending the applicability of VCMs to more challenging data regimes [@botzer2025publication; @fan2025network].

#### Deep Neural Networks for Modeling $f(c)$

One important development is the use of deep neural networks to learn mappings from context variables $c$ to the coefficients $\beta(c)$. This approach is particularly powerful for handling high-dimensional contexts such as images, audio, or temporal signals. Rather than relying on hand-designed basis expansions, neural networks can automatically capture complex and nonlinear dependencies between context and regression coefficients.

For example, in biomedical prediction, convolutional neural networks can extract relevant features from imaging data, which are then used to modulate regression coefficients in outcome models. In time-series analysis, recurrent neural networks can capture temporal context from longitudinal medical records to personalize predictions. These strategies have enabled context-aware modeling in applications where traditional VCMs are inadequate.

Mathematically, the model can be written as
$$
\beta_j(c_i) = f_j^{\text{NN}}(c_i), \quad y_i = \sum_j \beta_j(c_i) x_{ij} + \epsilon_i,
$$
where $f_j^{\text{NN}}$ denotes a neural network for the $j$th coefficient. Model parameters are typically optimized end-to-end with gradient-based methods. Recent research has shown that neural-feature-augmented VCMs can significantly improve generalization, and neural additive models further facilitate interpretation while retaining flexibility.

#### Boosted Trees and Ensemble Methods for Modeling $f(c)$

Beyond neural networks, tree-based ensemble methods such as gradient boosting decision trees (GBDTs) provide a compelling alternative for estimating context-varying coefficients. These models handle tabular and heterogeneous data effectively, accommodating nonlinearities, missing values, and categorical variables [@wang2025boosted; @cheng2020xgboost; @botzer2025publication].

In practice, researchers may fit a separate GBDT for each $f_j(c)$ or use multi-output boosting strategies to jointly predict all coefficients. Once context-specific coefficients $\beta_j(c_i)$ are estimated, standard linear predictions follow:
$$
y_i = \sum_j \beta_j(c_i) x_{ij} + \epsilon_i.
$$
Tree ensemble approaches are effective in applications such as individualized risk assessment and heterogeneous treatment effect estimation [@wang2025boosted; @cheng2020xgboost]. Their ability to capture thresholding and non-monotonicity is particularly valuable where parametric models fall short.

Compared to deep learning, boosted trees often require less data and less extensive tuning. In addition, their structure supports post-hoc explanation using methods such as SHAP, which is important for interpretability in applied contexts [@wang2025boosted].

#### Additive Models and Hybrid Architectures

A complementary line of research combines the strengths of linear and nonlinear approaches by adopting additive model architectures. Here, the outcome is expressed as a sum of context-dependent components, each governed by its own function $f_j(c)$ [@botzer2025publication; @li2024semi; @wang2025boosted]:
$$
y_i = \sum_{j=1}^p f_j(c_i) x_{ij} + \epsilon_i,
$$
where each $f_j(c_i)$ may be estimated using kernel methods, splines, tree ensembles, or neural networks. This structure preserves interpretability while permitting flexible adaptation to local context.

Recent hybrid models further augment additive structures by combining a global linear baseline with context-dependent corrections. For instance, the coefficients may be modeled as
$$
\beta_j(c_i) = \beta_j + r_j(c_i),
$$
where $\beta_j$ captures the global trend and $r_j(c_i)$ captures local adaptation. Such decompositions are valuable in personalized medicine and recommendation systems where both overall effects and individual variability are relevant [@botzer2025publication].

These models can be trained using iterative procedures such as backfitting or boosting, alternating between updating global and individualized components. This framework creates a flexible and interpretable spectrum between classical linear models and complex nonlinear architectures.

Integrating modern machine learning tools with the varying-coefficient framework has made context-adaptive models more powerful and broadly applicable. Deep neural networks, tree ensembles, and hybrid additive models now enable researchers to capture high-dimensional, nonlinear, and heterogeneous relationships that were previously out of reach [@botzer2025publication; @fan2025network; @wang2025boosted]. These advances are especially important in contexts where covariates are unstructured or the relationship between features and outcomes is highly complex.

Nonetheless, this flexibility requires caution. Risks such as overfitting, identifiability issues, and reduced statistical guarantees must be managed with regularization, careful model selection, and post-hoc interpretability methods. The synthesis of these approaches marks an active area of research focused on building robust and interpretable context-aware models [@botzer2025publication; @fan2025network].

### Structured Data: Spatio-Temporal, Graph, and Imaging Models

In many scientific and applied fields, context variables do not appear as simple, unstructured covariates in Euclidean space. Instead, they are embedded in systems with explicit structure, such as geographic regions, time series, or nodes in a social graph. These forms of structure introduce dependencies and constraints that classic varying-coefficient models often do not capture. Incorporating these structures into the modeling process allows for a more accurate representation of heterogeneous effects, particularly when the variation follows domain-specific topology or interaction patterns [@hu2024varying; @luo2024urban; @murakami2025fast; @botzer2025publication].

To address this challenge, recent research has extended the varying-coefficient framework to settings with spatial correlation, temporal evolution, graph-based relationships, and high-dimensional imaging data. Such models typically encode structural assumptions in the estimation of the coefficient function $f(c)$, whether by regularization, basis construction, or model architecture. This strategy maintains the interpretability of context-aware models while also leveraging known domain structure to improve efficiency and generalization [@botzer2025publication; @fan2025network].

The following sections summarize core methodological directions for structured VCMs, including spatial and graph regularization, temporally varying coefficients, and approaches for high-dimensional imaging or networked contexts. Applications span environmental studies, neuroimaging, and social science, reflecting the breadth of these methods in practice [@hu2024varying; @luo2024urban; @murakami2025fast].

#### Motivation and Challenges

Structured data create unique obstacles for context-aware modeling. In spatial analyses, nearby locations often experience similar exposures or share population characteristics, resulting in spatial autocorrelation in both predictors and coefficients. Overlooking this structure can produce noisy estimates and lead to misleading conclusions [@hu2024varying; @luo2024urban].

In temporal domains, predictor effects may evolve as behaviors shift, policies change, or biological systems adapt. Modeling time as a generic covariate is rarely sufficient to capture these dynamics [@botzer2025publication].

In applications such as neuroimaging and genomics, context can be indexed by nodes in a network or graph. The structure among these nodes determines which coefficients should be similar, raising additional modeling complexities [@murakami2025fast; @shi2021graph; @englert2025spatially].

When data are high-dimensional, as in medical imaging or spatial transcriptomics, each sample may be associated with thousands of features that have their own spatial or anatomical relationships. Accurately estimating coefficient functions in these settings requires accounting for spatial continuity or domain-specific constraints [@murakami2025fast].

To address such challenges, structured VCMs use assumptions such as graph-based smoothness, temporal regularization, or spatial decomposition. The following sections describe representative methods and applications for each scenario [@shi2021graph; @englert2025spatially; @botzer2025publication].

#### Spatial and Graph-Regularized VCMs

In spatial and networked data, context variables often reflect underlying connectivity. To incorporate this, spatial or graph-regularized VCMs impose smoothness constraints across a predefined connectivity structure. Typically, a penalty is introduced on the variation of coefficients across connected nodes. Suppose $W$ is a similarity or adjacency matrix, and $L = D - W$ is its graph Laplacian. The objective becomes
$$
\min_{\{\beta_j(\cdot)\}} \sum_{i=1}^n \ell(y_i, x_i^\top \beta(c_i)) + \lambda \sum_{j=1}^p \beta_j^\top L \beta_j,
$$
where $\beta_j = (\beta_j(c_1), \ldots, \beta_j(c_n))^\top$. This encourages neighboring nodes or regions to share similar coefficients [@shi2021graph; @englert2025spatially].

Such approaches have been successfully applied in spatial epidemiology [@hu2024varying], urban economics [@luo2024urban], and imaging genetics [@li2024semi]. Recent methodological developments include multitask Laplacian regularization and learning adaptive Laplacian weights directly from the data [@englert2025spatially; @shi2021graph].

#### Temporally Varying-Coefficient Models

Temporally varying-coefficient models are designed to capture how regression coefficients change with time. For an observation $(x_i, y_i, t_i)$, the model takes the form
$$
y_i = \sum_{j=1}^p x_{ij} \beta_j(t_i) + \varepsilon_i,
$$
where $t_i$ denotes the time index. Classical estimation strategies include kernel smoothing and spline-based expansions. To ensure smoothness, a common objective is
$$
\min_{\beta_j(\cdot)} \sum_{i=1}^n \ell(y_i, x_i^\top \beta(t_i)) + \lambda \sum_{j=1}^p \int \left( \frac{d^2 \beta_j(t)}{dt^2} \right)^2 dt.
$$

Such models are widely used in policy evaluation, financial econometrics, and public health. Extensions include space-time varying models $\beta_j(t, s)$ and latent trajectory estimation via Gaussian processes [@botzer2025publication; @hu2024varying; @fan2025network].

#### Latent Space and Manifold-Structured Contexts

In modern settings, the context $c$ may itself be a latent variable derived from high-dimensional data, such as a node's position in a social network [@fan2025network]. For example, user preferences or cognitive states may be encoded as lower-dimensional embeddings. A common procedure is to transform each observation $o_i$ into a latent context $z_i = g(o_i)$ and then model
$$
y_i = \sum_{j=1}^p x_{ij} \beta_j(z_i) + \varepsilon_i.
$$

Manifold-regularized models further require that points close on the manifold have similar coefficients. The regularization may take the form
$$
\sum_{i,j} w_{ij} \| \beta_j(c_i) - \beta_j(c_j) \|^2,
$$
where $w_{ij}$ encodes similarity [@shi2021graph; @englert2025spatially].

Applications include personalized medicine, education analytics, and urban mobility, among others [@botzer2025publication; @fan2025network]. Nevertheless, the quality of the learned representations is critical. Inadequate encoding or uncertainty can reduce model reliability, emphasizing the need for careful design and robust methodology [@botzer2025publication].
