## Theoretical Foundations and Advances in Varying-Coefficient Models

### Principles of Adaptivity
What does it mean for a model to be adaptive? When is it good for a model to be adaptive? While the appeal of adaptivity lies in flexibility and personalized inference, not all adaptivity is good adaptivity. In this section, we formalize the core principles that underlie adaptive modeling.

#### 1. Adaptivity requires flexibility
A model cannot adapt unless it has the capacity to represent multiple behaviors. Flexibility may take the form of nonlinearity, hierarchical structure, or modular components that allow different responses in different settings.

- Interaction effects in regression models [@doi:10.1145/2783258.2788613]
- Hierarchical models that allow for varying effects across groups
- Meta-learning and mixtures-of-experts models that learn to adapt based on context
- Varying-coefficient models that allow coefficients to change with context [@doi:10.1111/j.2517-6161.1993.tb01939.x]

#### 2. Adaptivity requires a signal of heterogeneity
- Varying-coefficient models adapt parameters based on observed context [@doi:10.1111/j.2517-6161.1993.tb01939.x]
- Contextual bandits adapt actions to context features [@arxiv:1811.04383]
- Multi-domain models adapt across known environments or inferred partitions [@arXiv:2010.07249]

#### 3. Modularity improves adaptivity
Adaptive systems are easier to design, debug, and interpret when built from modular parts. Modularity supports targeted adaptation, transferability, and disentanglement.

- []

#### 4. Adaptivity implies selectivity
Adaptation must be earned. Overreacting to limited data leads to overfitting. The best adaptive methods include mechanisms for deciding when not to adapt.
- Lepski's method [@arxiv:1508.00249]
- Aggregation of classifiers [@doi:10.1007/978-3-540-45167-9_23]

#### 5. Adaptivity is bounded by data efficiency
[@arxiv:1911.12568]


#### When Adaptivity Fails: Common Failure Modes
Even when all the ingredients are present, adaptivity can backfire. Common failure modes include:

- Spurious Adaptation: Adapting to unstable or confounded features [@arXiv:2010.05761]
- Overfitting in Low-Data Contexts: Attempting fine-grained adaptation with insufficient signal
- Modularity Mis-specification: Adapting in the wrong units or groupings [@arXiv:1911.08731]
- Feedback Loops: Models that change the data distribution they rely on [@doi:10.1145/3097983.3098066]



### Advances in Varying-Coefficient Models
TODO: Outlining key theoretical and methodological breakthroughs.

Relevant references:

- [@doi:10.3390/publications13020019]

#### Flexible Functional Forms

Relevant references:

- [@doi:10.5705/ss.202024.0118]

#### Integration with State-of-the-Art Machine Learning
TODO: Enhancing VC models with modern ML technologies (e.g. deep learning, boosted trees, etc).

Relevant references:

- [@doi:10.1007/s00180-025-01603-8]
- [@arxiv:2003.06416]
- [@arxiv:2004.13912]

#### Structured data (Spatio-Temporal, Graphs, etc.)

Related references:

- [@doi:10.1080/01621459.2025.2470481]
- [@arxiv:2502.14651]
- [@doi:10.1111/gean.70005]
- [@doi:10.1016/j.jeconom.2024.105883]
- [@doi:10.1016/j.regsciurbeco.2024.104009]
- [@arXiv:2111.01104]

### Context-Invariant Training
TODO: The converse of VC models, exploring the implications of training context-invariant models.
e.g. out-of-distribution generalization, robustness to adversarial attacks.

Relevant references:

- Invariant Risk Minimization [@arXiv:1907.02893]
- Out-of-Distribution Generalization via Risk Extrapolation [@arXiv:2003.00688]
- The Risks of Invariant Risk Minimization [@arXiv:2010.05761]
- Conditional Variance Penalties and Domain Adaptation [@arXiv:1710.11469]
- Can Subpopulation Shifts Explain Disagreement in Model Generalization? [@arXiv:2106.04486]

#### Adversarial Robustness as Context-Invariant Training
Related references:

- Towards Deep Learning Models Resistant to Adversarial Attacks [@arXiv:1706.06083]
- Robustness May Be at Odds with Accuracy [@arXiv:1805.12152]

#### Training methods for Context-Invariant Models
- Just Train Twice: Improving Group Robustness without Training Group Information [@arXiv:2002.10384]
- Environment Inference for Invariant Learning [@arXiv:2110.14048]
- Distributionally Robust Neural Networks for Group Shifts [@arXiv:1911.08731]