## Opportunities for Foundation Models

### Expanding Frameworks
TODO: Define foundation models, Explore how foundation models are redefining possibilities within statistical models.


### Foundation models as context
TODO: Show recent progress and ongoing directions in using foundation models as context.

## Foundation Models and Their Connection to ContextualizedML

Foundation models are flexible, transformer-based neural networks trained on large datasets using self-supervised learning, adaptable to various downstream tasks \[@doi:10.48550/arXiv.2108.07258]. Recent advances, particularly in natural language processing, are exemplified by large language models (LLMs) like GPT-4 \[@doi:10.48550/arXiv.2303.08774] and LLaMA-3.1 \[@doi:10.48550/arXiv.2407.21783]. Foundation models have also shown significant progress in domains such as text-vision models \[@doi.org/10.48550/arXiv.2103.00020], text embedding models \[@doi:10.48550/arXiv.1810.04805], and tabular data \[@doi:10.48550/arXiv.2207.01848].

### Contextualization in Foundation Models

Foundation models closely align with ContextualizedML through their ability to dynamically adapt to new contexts. Users typically interact with LLMs via prompting, providing a contextual query to specify desired behavior \[@doi:10.1145/3560815]. Similarly, the mixture-of-experts (MoE) architecture leverages contextualization by using a learned routing function to apply specific context-sensitive models to different input segments \[@doi:10.1007/s10462-012-9338-y].

### Utilizing Foundation Models in ContextualizedML

Foundation models contribute to ContextualizedML on several levels:

* **Feature Level:** They transform unstructured data into interpretable features through prompting, enabling structured analysis \[@doi:10.48550/arXiv.2302.12343; @doi:10.48550/arXiv.2305.12696; @doi:10.18653/v1/2023.emnlp-main.384].
* **Representation Level:** Prompted embeddings facilitate training interpretable or black-box models, contextualizing data effectively \[@doi:10.48550/arXiv.2208.01066; @doi:10.48550/arXiv.2212.09741; @doi:10.1038/s41467-023-43713-1].
* **Post-hoc Descriptions:** Foundation models can explain black-box model decisions through natural-language parameters assigned to different input regions \[@doi:10.48550/arXiv.2409.08466].

## Key Models and Architectures

### LMPriors

LMPriors leverage language models as task-specific priors to enhance performance in machine learning tasks such as feature selection, causal discovery, and reinforcement learning \[@doi:10.48550/arXiv.2210.12530]. By employing knowledge-rich prompts, LMPriors outperform conventional models like LassoNet in causal inference accuracy (88.7%) using fewer but semantically meaningful features. In reinforcement learning, they improve decision-making safety and efficacy through contextual reward shaping.

However, LMPriors depend heavily on well-crafted prompts and carefully curated input data. Poor prompts can introduce bias, potentially leading to ethical issues or unsafe behaviors. Thus, meticulous input and prompt design are crucial.

### FLAN-MoE

FLAN-MoE (Fine-tuned Language model with Mixture of Experts) combines instruction tuning with MoE architectures, enabling efficient contextualization across diverse NLP tasks \[@doi:10.48550/arXiv.2305.14705]. A dynamic gating mechanism selects specialized experts based on input context, reducing computational overhead and enhancing generalization capabilities, including zero-shot and few-shot learning.

This model improves interpretability by clearly associating experts with specific tasks. Possible future directions include refining gating mechanisms and exploring multilingual instruction tuning.

### Mixture of In-Context Experts (MoICE)

MoICE addresses challenges in long-context understanding through dynamic routing and multiple Rotary Position Embeddings (RoPE) \[@doi:10.48550/arXiv.2210.12530]. RoPE encodes positional information in token embeddings, resolving token-ordering issues common to standard attention mechanisms. Dynamic routing selects relevant RoPE angles per token, ensuring context-specific adaptability.

MoICE emphasizes efficiency by training only routing components, keeping the base LLM parameters fixed. This minimizes memory usage and computational load. The modelâ€™s dynamic RoPE selection also enhances interpretability, offering transparency in attention distribution and decision-making processes. MoICE excels in tasks requiring extended context management and retrieval-augmented generation (RAG).



