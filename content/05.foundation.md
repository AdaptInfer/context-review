## Opportunities for Foundation Models

### Expanding Frameworks
TODO: Define foundation models, Explore how foundation models are redefining possibilities within statistical models.


### Foundation models as context
TODO: Show recent progress and ongoing directions in using foundation models as context.

Foundation models are versatile neural network architectures, typically based on transformer models and trained extensively using large-scale, self-supervised methods on diverse datasets [@doi:10.48550/arXiv.2108.07258]. Recent progress, particularly in natural language processing (NLP), has underscored their significance, with large language models (LLMs) such as GPT-4 [@doi:10.48550/arXiv.2303.08774] and LLaMA-3.1 [@doi:10.48550/arXiv.2407.21783] demonstrating remarkable proficiency across a wide variety of NLP tasks. Moreover, foundation models have successfully extended beyond NLP, excelling in domains like text-vision [@doi:10.48550/arXiv.2103.00020], text embedding [@doi:10.48550/arXiv.1810.04805], and tabular data modeling [@doi:10.48550/arXiv.2207.01848].

An essential aspect connecting foundation models to Contextualized Machine Learning (ContextualizedML) is their innate reliance on contextualization to adapt dynamically. For instance, prompting an LLM involves providing it with specific queries that guide its response, effectively contextualizing the model's internal parameters for each task [@doi:10.1145/3560815]. Similarly, mixture-of-experts (MoE) architectures achieve efficient computation by learning routing functions that dynamically select specialized expert models according to input contexts [@doi:10.1007/s10462-012-9338-y].

Foundation models can significantly enhance ContextualizedML through various levels of integration:

Feature-Level Contextualization:
Foundation models enable structured and interpretable features from otherwise unstructured data. Prompting LLMs with targeted questions about texts produces meaningful and interpretable feature representations [@doi:10.48550/arXiv.2302.12343, @doi:10.48550/arXiv.2305.12696, @doi:10.18653/v1/2023.emnlp-main.384].

Representation-Level Contextualization:
Foundation models serve as contextual bases for downstream models. For instance, prompted foundation models can be integrated with interpretable structures like decision trees or linear models [@doi:10.48550/arXiv.2208.01066]. Additionally, prompted embeddings can enrich training procedures for both black-box models [@doi:10.48550/arXiv.2212.09741] and simpler, n-gram-based approaches [@doi:10.1038/s41467-023-43713-1].

Post-hoc Contextualization for Interpretability:
Foundation models also facilitate interpretability by providing natural-language explanations of decisions from complex models, enhancing transparency into their decision-making processes [@doi:10.48550/arXiv.2409.08466].

Recent advancements have further extended the capabilities and efficiency of foundation models through innovative approaches:

FLAN-MoE (Fine-tuned Language Model with Mixture of Experts) [@doi:10.48550/arXiv.2305.14705] introduces a novel architecture combining instruction tuning with sparse mixtures of experts. It employs dynamic gating to selectively activate specialized experts based on input contexts, thereby efficiently addressing broader NLP tasks. Instruction tuning further boosts its capability in few-shot and zero-shot generalization scenarios, outperforming dense state-of-the-art counterparts. Moreover, FLAN-MoE enhances interpretability through clearly identifiable expert activations, offering insights into model decisions. Future developments might explore more sophisticated gating mechanisms and multilingual instruction tuning to improve generalization further.

LMPriors (Pre-Trained Language Models as Task-Specific Priors) [@doi:10.48550/arXiv.2210.12530] represents another significant contribution. It leverages the rich contextual knowledge from foundation models such as GPT-3 to guide downstream tasks like causal discovery, feature selection, and reinforcement learning. Unlike traditional sparsity-based methods, LMPriors integrate semantic understanding from pre-trained models to significantly improve causal inference (achieving 88.7% accuracy in causal discovery), reinforcement learning safety, and decision-making efficiency. However, the efficacy of LMPriors heavily depends on careful prompt construction, as poorly designed prompts can introduce bias and ethical concerns, thus necessitating rigorous control over input quality.

MoICE (Mixture of In-Context Experts) [@doi:10.48550/arXiv.2210.12530] addresses challenges associated with long-context modeling and dynamic contextualization. MoICE incorporates a dynamic routing mechanism integrated within each attention head, utilizing multiple Rotary Position Embeddings (RoPE) angles to explicitly encode positional information within token embeddings. This resolves traditional limitations of attention mechanisms regarding token order. Its design dynamically assigns RoPE angles contextually, ensuring thorough processing regardless of token positions. Efficiency is achieved through router-only training, significantly reducing memory overhead during model inference. MoICEâ€™s architecture also provides clear interpretability by illustrating attention distributions across inputs, offering transparent insights into the decision-making process. Consequently, MoICE significantly improves performance in tasks involving long-context understanding and retrieval-augmented generation (RAG), demonstrating effectiveness in both open-ended and closed-ended scenarios.


