## Making Implicit Adaptivity Explicit: Local Models, Surrogates and Post Hoc Approximations

### Motivation
Building on the prior discussion of implicit adaptivity, we now turn to methods that expose, approximate, or control those adaptive mechanisms.  
Implicit adaptivity allows powerful models, including foundation models, to adjust behavior without explicitly representing a mapping from context to parameters [@Bommasani2021OnTO]. This flexibility hides why and how adaptation occurs, limits modular reuse, and complicates auditing, personalization, and failure diagnosis. Making adaptivity explicit improves alignment with downstream goals, enables modular composition, and supports debugging and error attribution. It also fits the call for a more rigorous science of interpretability with defined objectives and evaluation criteria [@DoshiVelez2017TowardsAR; @Bordt2024RethinkingEM].  
This chapter reviews practical approaches for surfacing structure, the assumptions they rely on, and how to evaluate their faithfulness and utility.

### Approaches
Efforts to make implicit adaptation explicit span complementary strategies that differ in assumptions, granularity, and computational cost. We group them into six families:

1. surrogate modeling for local approximation,  
2. prototype- and neighbor-based reasoning,  
3. diagnostics for amortized inference,  
4. disentanglement and bottleneck methods,  
5. parameter extraction and probing, and  
6. emerging approaches that leverage large language models as post-hoc explainers.

#### Surrogate Modeling
This line of work approximates a black-box $h(x,c)$ with an interpretable model in a small neighborhood, so that local behavior and a local view of $f(c)$ can be inspected. A formal template is

$$
\hat{g}\_{x\_0,c\_0} = \arg\min_{g \in \mathcal{G}} \, \mathbb{E}\_{(x,c) \sim \mathcal{N}_{x_0,c_0}} \left[ \ell\big(h(x,c), g(x,c)\big) \right] + \Omega(g),
$$

where $\mathcal N_{x_0,c_0}$ defines a locality (e.g., kernel weights), $\ell$ measures fidelity, and $\Omega$ controls complexity. A convenient local goodness-of-fit is

$$
R^2_{\text{local}}
= 1 - \frac{\sum_i w_i\,\big(h_i - g_i\big)^2}{\sum_i w_i\,\big(h_i - \bar h\big)^2},
\qquad
w_i \propto \kappa\!\big((x_i,c_i),(x_0,c_0)\big).
$$

LIME perturbs inputs and fits a locality-weighted linear surrogate [@Ribeiro2016WhySI]; SHAP / DeepSHAP provide additive attributions based on Shapley values [@Lundberg2017AUA]. Integrated Gradients and DeepLIFT link attribution to path-integrated sensitivity or reference-based contributions [@Sundararajan2017AxiomaticAF; @Shrikumar2017LearningIF]. These methods are most reliable when the model is near-linear in the chosen neighborhood and perturbations remain near the data manifold; consequently, a rigorous analysis involves stating the neighborhood definition, reporting the surrogate’s goodness-of-fit, and assessing stability across seeds and baselines.

#### Prototype and Nearest-Neighbor Methods
Here, a decision is grounded by reference to similar cases in representation space, which supports case-based explanations and modular updates. ProtoPNet learns a library of visual prototypes to implement “this looks like that” reasoning [@Chen2018ThisLL]. Deep $k$-nearest neighbors audits predictions by querying neighbors in activation space and can flag distribution shift [@Papernot2018DeepKN]. Influence functions link a prediction to influential training points for data-centric debugging [@Koh2017UnderstandingBP]. This line of work connects naturally to exemplar models and contextual bandits, where decisions are justified via comparisons to context-matched exemplars. Reports should include prototype coverage and diversity, neighbor quality checks, and the effect of editing prototypes or influential examples.

#### Amortization Diagnostics
For amortized inference systems (e.g., VAEs), the encoder $q_{\phi}(\theta\mid x)$ can be treated as an implicit $f(c)$. Diagnostics measure amortization gaps and identify suboptimal inference or collapse [@Cremer2018InferenceSI]. Useful outputs include calibration under shift and posterior predictive checks, together with ablations that vary encoder capacity or add limited iterative refinement. This clarifies when the learned mapping is faithful versus when it underfits the target posterior.

#### Disentangled and Bottlenecked Representations
The aim is to expose factors that align with distinct contextual causes, making changes traceable and controllable. $\beta$-VAE encourages more factorized latents [@Higgins2016betaVAELB], while the Deep Variational Information Bottleneck promotes predictive minimality that can suppress spurious context [@Alemi2017DeepVI]. Concept-based methods such as TCAV and ACE map latent directions to human concepts and test sensitivity at the concept level [@Kim2017InterpretabilityBF; @Ghorbani2019TowardsAC]. Fully unsupervised disentanglement is often ill-posed without inductive bias or weak supervision [@Locatello2018ChallengingCA]. Reports should include concept validity tests, factor stability across runs, and simple interventions that demonstrate controllability.

#### Parameter Extraction and Probing
This family locates where adaptation is encoded and exposes handles for inspection or edits. Linear probes test what is linearly decodable from intermediate layers [@Alain2016UnderstandingIL]; edge probing examines specific linguistic structure in contextualized representations [@Tenney2019WhatDY]. Model editing methods such as ROME can modify stored factual associations directly in weights [@Meng2022LocatingAE], while “knowledge neurons” seek units linked to particular facts [@Dai2021KnowledgeNI]. Evaluations should include pre/post-edit behavior, the locality and persistence of edits, and any side effects on unrelated capabilities.

#### LLMs as Post-hoc Explainers
Recent work uses in-context prompting to elicit rationales, counterfactuals, or error hypotheses from large language models for a target system [@Kroeger2023InContextEH]. These explanations can be useful but must be validated for faithfulness, for example by checking agreement with surrogate attributions, reproducing input–output behavior, and testing stability to prompt variations. Explanations should be treated as statistical estimators with stated objectives and evaluation criteria [@Bordt2024RethinkingEM].

### Trade-offs

#### Fidelity vs. Interpretability
High-fidelity surrogates capture the target model’s behavior more accurately, yet they often grow in complexity and lose readability. A crisp statement of the design goal is

$$
\min\_{g\in\mathcal G}\ \underbrace{\phi\_{\text{fid}}(g;U)}\_{\text{faithfulness on use set }U}
\+ \lambda\\underbrace{\psi\_{\text{simplicity}}(g)}\_{\text{sparsity / size / semantic load}},
$$

where $\phi_{\text{fid}}$ can be local $R^2$, AUC, or rank correlation with $h$, and $\psi_{\text{simplicity}}$ can be sparsity, tree depth, rule count, or active concept count. If a simple surrogate underfits, consider structured regularization (e.g., monotonic constraints, grouped sparsity, concept bottlenecks). If a complex surrogate is needed, accompany it with readable summaries (partial dependence snapshots, distilled rule sets, compact concept reports).

#### Local vs. Global Scope
Local surrogates aim for $g_{x_0,c_0}\approx h$ only on $\mathcal N_{x_0,c_0}$, whereas a global surrogate seeks $g_{\text{global}}\approx h$ across the domain, potentially smoothing away distinct regimes. Hybrid schemes combine both:

$$
g(x,c)=\sum_{k=1}^{K} w_k(x,c)\, g_k(x,c),
\qquad \sum_k w_k(x,c)=1,\quad w_k\ge 0,
$$

with local experts $g_k$ and soft assignment $w_k$. Report the neighborhood definition, coverage (fraction of test cases with acceptable local fit), and disagreements between local and global views; flag regions where the global surrogate is unreliable.

#### Approximation vs. Control
Coarse modularization makes control and auditing simpler because edits act on a small number of levers, yet residual error can be large. Fine-grained extraction, such as neuron- or weight-level edits, can achieve precise behavioral changes but may introduce unintended side effects. Define the intended edit surface in advance (concepts, features, prototypes, submodules, parameters). For coarse modules, measure the residual gap to the base model and verify that edits improve target behavior without harming unaffected cases. For fine-grained edits, quantify locality and collateral effects using a held-out audit suite with counterfactuals, canary tasks, and out-of-distribution probes. Maintain versioned edits, enable rollback, and document the scope of validity.

### Open Questions

#### Reusable Modules
A central question is whether we can isolate portable skills or routines from large models and reuse them across tasks without degrading overall capability [@Bommasani2021OnTO]. Concretely, a reusable module should satisfy portability, isolation, composability, and stability. Promising directions include concept bottlenecks that expose human-aligned interfaces, prototype libraries as swappable reference sets, sparse adapters that confine changes to limited parameter subsets, and routing mechanisms that select modules based on context. Evaluation should track transfer performance, sample efficiency, interference on held-out capabilities, and robustness under domain shift.

#### Performance Gains
When does making structure explicit improve robustness or efficiency compared to purely implicit adaptation? Benefits are most likely when domain priors are reliable, data are scarce, or safety constraints limit free-form behavior. Explicit structure is promising when context topology is known (spatial or graph), when spurious correlations should be suppressed, and when explanations must be auditable. To assess this, fix capacity and training budget and vary only the explicit structure (prototypes, disentanglement, bottlenecks). Stress tests should cover diverse distributional challenges, including covariate shift, concept shift, long-tail classes, and adversarially correlated features. Account for costs such as concept annotation, extra hyperparameters, and potential in-domain accuracy loss.

#### Abstraction Level
Another open issue is the appropriate level at which to represent structure: parameters (weights, neurons), functions (local surrogates, concept scorers, routing policies), or latent causes (disentangled or causal factors). Choose based on the use case. For safety patches, lower-level handles allow precise edits but require guardrails and monitoring. For scientific or policy communication, function- or concept-level interfaces are often more stable and auditable. Optimize three objectives in tension: faithfulness to the underlying model, usability for the target audience, and stability under shift. Tooling should support movement between levels (e.g., distilling weight-level edits into concept summaries or lifting local surrogates into compact global reports).

### Notes on Classical Post-hoc Methods
LIME, SHAP, and gradient-based methods such as Integrated Gradients and DeepLIFT remain common tools for context-adaptive interpretation. Their usefulness depends on careful design and transparent reporting. Explanations should be treated as statistical estimators with stated objectives and evaluation criteria [@DoshiVelez2017TowardsAR; @Bordt2024RethinkingEM].

#### Scope and locality
Local surrogate methods require a clear definition of the neighborhood in which the explanation is valid. The sampling scheme, kernel width, and surrogate capacity determine which aspects of the black box can be recovered. When context variables are present, the explanation should be conditioned on the relevant context and the valid region should be described.

#### Attribution methods in practice
Attribution based on gradients is sensitive to baseline selection, path construction, input scaling, and preprocessing. Baselines should have clear domain meaning, and sensitivity analyses should show how conclusions change under alternative baselines. For perturbation-based surrogates, report the perturbation distribution and any constraints that keep samples on the data manifold.

#### Faithfulness and robustness
Faithfulness and robustness should be checked rather than assumed. Useful checks include deletion and insertion curves, counterfactual tests, randomization tests, stability under small input and seed perturbations, and for local surrogates a local goodness-of-fit such as a neighborhood $R^2$. The evaluation metric should match the stated objective of the explanation [@Bordt2024RethinkingEM; @DoshiVelez2017TowardsAR].

#### Minimal reporting checklist
- Data slice and context used for the explanation, with a description of the locality or neighborhood.  
- Surrogate specification, including model family, regularization, and kernel or sampling parameters.  
- Faithfulness metrics, for example local $R^2$, deletion and insertion area, counterfactual validity.  
- Sensitivity analyses over baselines, random seeds, and small perturbations, with uncertainty estimates.  
- Computational budget and constraints that may affect explanation quality.  
- Known limitations and failure modes observed during validation.

#### From post hoc analysis to design
When the goal is control, auditing, or policy communication, insights from post-hoc analysis can inform the design of explicit context-to-parameter structure. In such cases, use post-hoc findings to specify prototypes, bottlenecks, or concept interfaces that are trained and validated directly, rather than relying only on after-the-fact rationales [@DoshiVelez2017TowardsAR; @Bordt2024RethinkingEM]. Taken together, these tools bridge black-box adaptation and structured inference and prepare the ground for designs where context-to-parameter structure is specified and trained end to end.

#### Implications for classical models
These tools can also clarify how traditional models, for example, logistic regression with interaction terms or generalized additive models to admit a local adaptation view: a simple global form paired with context-sensitive weights or features. Reading such models through the lens of local surrogates and concept interfaces helps align classical estimation with modern, context-adaptive practice.
