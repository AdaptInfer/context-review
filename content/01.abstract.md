## Abstract {.page_break_before}

Modern predictive systems are expected to adapt their behavior to the specific situation they are facing. 
A clinical model should not treat every patient the same; a retrieval-augmented model should change its answer when given different evidence; a mixture-of-experts model should route different inputs to different experts. 
We call this capability **context-adaptive inference**: before predicting, the system uses information about the current context to specialize its parameters or computation for that instance.

This article provides a unified view of context-adaptive inference across three traditions that are usually treated separately: 
(i) explicit adaptation in statistics (e.g. varying-coefficient models, local regression, hierarchical sharing), 
(ii) rapid task-specific adaptation in meta-learning and transfer, and 
(iii) implicit adaptation in large foundation models via prompting, retrieval, and expert routing. 
We formalize these approaches under a common objective: to map context $c$ to adapted parameters 
$\theta(c)$, then to predict via $f(x; \theta(c))$. 
Under squared loss, linear prediction heads, and fixed features, we prove that explicit parameter adaptation and implicit routing are mathematically equivalent to kernel ridge regression on joint features of inputs and context. 
Building on this bridge, we propose practical design principles and evaluation metrics including adaptation-efficiency, routing stability, and context-specific robustness to guide when to specialize, how to constrain that specialization, and how to audit context-adaptive models in deployment. 
Finally, we identify open problems in identifiability, robustness under distribution shift, and efficient large-scale adaptation, outlining design principles for methods that are scalable, reliable, and transparent in real-world settings.