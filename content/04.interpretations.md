## Context-Adaptive Interpretations of Context-Invariant Models

In the previous section, we discussed the importance of context in model parameters. 
Such context-adaptive models can be learned by explicitly modeling the impact of contextual variables on model parameters, or learned implicitly in a model containing interaction effects between the context and the input features.
In this section, we will focus on recent progress in understanding how context influences interpretations of statistical models, even when the model was not originally designed to incorporate context.

TODO: Discussing the implications of context-adaptive interpretations for traditional models. Related work including LIME/DeepLift/DeepSHAP.

### Context-Aware Efficiency Principles and Design

The efficiency of context-adaptive methods hinges on several key design principles that balance computational tractability with statistical accuracy. These principles guide the development of methods that can scale to large datasets while maintaining interpretability and robustness.

Context-aware efficiency often relies on sparsity assumptions that limit the number of context-dependent parameters. This can be achieved through group sparsity, which encourages entire groups of context-dependent parameters to be zero simultaneously [@Yuan2006ModelSA], hierarchical regularization that applies different regularization strengths to different levels of context specificity [@tibshirani1996regression;@Gelman2006DataAU], and adaptive thresholding that dynamically adjusts sparsity levels based on context complexity.

Efficient context-adaptive inference can be achieved through computational strategies that allocate resources based on context. Early stopping terminates optimization early for contexts where convergence is rapid [@Bottou2016OptimizationMF], while context-dependent sampling uses different sampling strategies for different contexts [@Balseiro2018ContextualBW]. Caching and warm-starting leverage solutions from similar contexts to accelerate optimization, particularly effective when contexts exhibit smooth variation [@Boyd2011DistributedOA].

The design of context-aware methods often involves balancing computational efficiency with interpretability. Linear context functions are more interpretable but may require more parameters, while explicit context encoding improves interpretability but may increase computational cost. Local context modeling provides better interpretability but may be less efficient for large-scale applications. These trade-offs must be carefully considered based on the specific requirements of the application domain, as demonstrated in recent work on adaptive optimization methods [@Kingma2014AdamAM].

### Adaptivity is bounded by data efficiency

Recent work underscores a practical limit: stronger adaptivity demands more informative data per context. When contexts are fine-grained or rapidly shifting, the effective sample size within each context shrinks, and models risk overfitting local noise rather than learning stable, transferable structure. Empirically, few-shot behaviors in foundation models improve with scale yet remain sensitive to prompt composition and example distribution, indicating that data efficiency constraints persist even when capacity is abundant [@Brown2020LanguageMA; @Wei2022EmergentAO; @Min2022RethinkingTR]. Complementary scaling studies quantify how performance depends on data, model size, and compute, implying that adaptive behaviors are ultimately limited by sample budgets per context and compute allocation [@Kaplan2020ScalingLF; @Hoffmann2022TrainingCO; @Arora2024BayesianSL]. In classical and modern pipelines alike, improving data efficiency hinges on pooling information across related contexts (via smoothness, structural coupling, or amortized inference) while enforcing capacity control and early stopping to avoid brittle, context-specific artifacts [@Bottou2016OptimizationMF]. These considerations motivate interpretation methods that report not only attributions but also context-conditional uncertainty and stability, clarifying when adaptive behavior is supported by evidence versus when it reflects data scarcity.

#### Formalization: data-efficiency constraints on adaptivity

Let contexts take values in a measurable space \(\mathcal{C}\), and suppose the per-context parameter is \(\theta(c) \in \Theta\). For observation \((x,y,c)\), consider a conditional model \(p_\theta(y\mid x,c)\) with loss \(\ell(\theta; x,y,c)\). For a context neighborhood \(\mathcal{N}_\delta(c) = \{c': d(c,c') \le \delta\}\) under metric \(d\), define the effective sample size available to estimate \(\theta(c)\) by
\[
N_\text{eff}(c,\delta) \,=\, \sum_{i=1}^n w_\delta(c_i,c),\quad w_\delta(c_i,c) \propto K\!\left(\tfrac{d(c_i,c)}{\delta}\right),\ \sum_i w_\delta(c_i,c)=1,
\]
where \(K\) is a kernel. A kernel-regularized estimator with smoothness penalty \(\mathcal{R}(\theta)=\int \|\nabla_c \theta(c)\|^2\,\mathrm{d}c\) solves
\[
\widehat{\theta} \,=\, \arg\min_{\theta\in\Theta}\; \frac{1}{n}\sum_{i=1}^n \ell(\theta; x_i,y_i,c_i) \, + \, \lambda\, \mathcal{R}(\theta).
\]
Assuming local Lipschitzness in \(c\) and \(L\)-smooth, \(\mu\)-strongly convex risk in \(\theta\), a standard bias–variance decomposition yields for each component \(j\)
\[
\mathbb{E}\big[\|\widehat{\theta}_j(c)-\theta_j(c)\|^2\big] \;\lesssim\; \underbrace{\tfrac{\sigma^2}{N_\text{eff}(c,\delta)}}_{\text{variance}}\; +\; \underbrace{\delta^{2\alpha}}_{\text{approx. bias}}\; +\; \underbrace{\lambda^2}_{\text{reg. bias}},\quad \alpha>0,
\]
which exhibits the adaptivity–data trade-off: finer locality (small \(\delta\)) increases resolution but reduces \(N_\text{eff}\), inflating variance. Practical procedures pick \(\delta\) and \(\lambda\) to balance these terms (e.g., via validation), and amortized approaches replace \(\theta(c)\) by \(f_\phi(c)\) with shared parameters \(\phi\) to increase \(N_\text{eff}\) through parameter sharing.

For computation, an early-stopped first-order method with step size \(\eta\) and \(T(c)\) context-dependent iterations satisfies (for smooth, strongly convex risk) the bound
\[
\mathcal{L}(\theta^{(T(c))}) - \mathcal{L}(\theta^*) \;\le\; (1-\eta\mu)^{T(c)}\,\big(\mathcal{L}(\theta^{(0)})-\mathcal{L}(\theta^*)\big) \, + \, \tfrac{\eta L\sigma^2}{2\mu N_\text{eff}(c,\delta)},
\]
linking compute allocation \(T(c)\) and data availability \(N_\text{eff}(c,\delta)\) to the attainable excess risk at context \(c\).
