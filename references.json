[
  {
    "id": "1HROmiXOz",
    "type": "article-journal",
    "title": "Understanding intermediate layers using linear classifier probes",
    "author": [
      {
        "family": "Alain",
        "given": "Guillaume"
      },
      {
        "family": "Bengio",
        "given": "Yoshua"
      }
    ],
    "container-title": "ArXiv",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "volume": "abs/1610.01644",
    "URL": "https://api.semanticscholar.org/CorpusID:9794990",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Alain2016UnderstandingIL"
  },
  {
    "id": "FPJnDqnE",
    "type": "article-journal",
    "title": "Deep Variational Information Bottleneck",
    "author": [
      {
        "family": "Alemi",
        "given": "Alexander A."
      },
      {
        "family": "Fischer",
        "given": "Ian"
      },
      {
        "family": "Dillon",
        "given": "Joshua V."
      }
    ],
    "container-title": "ArXiv",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "volume": "abs/1612.00410",
    "URL": "https://api.semanticscholar.org/CorpusID:204922497",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Alemi2017DeepVI"
  },
  {
    "id": "rXJuRh4F",
    "type": "article-journal",
    "title": "Selective Test-Time Adaptation for Unsupervised Anomaly Detection using Neural Implicit Representations",
    "author": [
      {
        "family": "Ambekar",
        "given": "Sameer"
      },
      {
        "family": "Schnabel",
        "given": "Julia A."
      },
      {
        "family": "Bercea",
        "given": "Cosmin I."
      }
    ],
    "container-title": "ArXiv",
    "issued": {
      "date-parts": [
        [
          2024
        ]
      ]
    },
    "volume": "abs/2410.03306",
    "URL": "https://api.semanticscholar.org/CorpusID:273163004",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Ambekar2024SelectiveTA"
  },
  {
    "id": "eWO4aUQ6",
    "type": "article-journal",
    "title": "Bayesian scaling laws for in-context learning",
    "author": [
      {
        "family": "Arora",
        "given": "Aryaman"
      },
      {
        "family": "Jurafsky",
        "given": "Daniel"
      },
      {
        "family": "Potts",
        "given": "Christopher"
      },
      {
        "family": "Goodman",
        "given": "Noah D."
      }
    ],
    "container-title": "ArXiv",
    "issued": {
      "date-parts": [
        [
          2024
        ]
      ]
    },
    "volume": "abs/2410.16531",
    "URL": "https://api.semanticscholar.org/CorpusID:273507537",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Arora2024BayesianSL"
  },
  {
    "id": "PGLlyugi",
    "type": "article-journal",
    "title": "Transformers as Support Vector Machines",
    "author": [
      {
        "family": "Ataee Tarzanagh",
        "given": "Davoud"
      },
      {
        "family": "Li",
        "given": "Yingcong"
      },
      {
        "family": "Thrampoulidis",
        "given": "Christos"
      },
      {
        "family": "Oymak",
        "given": "Samet"
      }
    ],
    "container-title": "ArXiv",
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "volume": "abs/2308.16898",
    "URL": "https://api.semanticscholar.org/CorpusID:261395206",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: AtaeeTarzanagh2023TransformersAS"
  },
  {
    "id": "ql9MY89d",
    "type": "manuscript",
    "author": [
      {
        "family": "Balseiro",
        "given": "Santiago R."
      },
      {
        "family": "Golrezaei",
        "given": "Negin"
      },
      {
        "family": "Mahdian",
        "given": "Mohammad"
      },
      {
        "family": "Mirrokni",
        "given": "Vahab S."
      },
      {
        "family": "Schneider",
        "given": "Jon"
      }
    ],
    "title": "Contextual Bandits with Cross-learning",
    "archive": "arXiv",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Balseiro2018ContextualBW"
  },
  {
    "id": "3looVq4U",
    "type": "manuscript",
    "author": [
      {
        "family": "Bommasani",
        "given": "Rishi"
      },
      {
        "family": "Hudson",
        "given": "Drew A."
      },
      {
        "family": "Adeli",
        "given": "Ehsan"
      },
      {
        "family": "Altman",
        "given": "Russ"
      },
      {
        "family": "Arora",
        "given": "Simran"
      }
    ],
    "title": "On the Opportunities and Risks of Foundation Models",
    "archive": "arXiv",
    "note": "and other authors\nLoaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Bommasani2021OnTO"
  },
  {
    "id": "byOqPJ9J",
    "type": "paper-conference",
    "title": "Rethinking Explainable Machine Learning as Applied Statistics",
    "author": [
      {
        "family": "Bordt",
        "given": "Sebastian"
      },
      {
        "family": "Raidl",
        "given": "Eric"
      },
      {
        "family": "von Luxburg",
        "given": "Ulrike"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2024
        ]
      ]
    },
    "URL": "https://api.semanticscholar.org/CorpusID:267412927",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Bordt2024RethinkingEM"
  },
  {
    "id": "n4I8NBgb",
    "type": "manuscript",
    "author": [
      {
        "family": "Bottou",
        "given": "LÃ©on"
      },
      {
        "family": "Curtis",
        "given": "Frank E."
      },
      {
        "family": "Nocedal",
        "given": "Jorge"
      }
    ],
    "title": "Optimization Methods for Large-Scale Machine Learning",
    "archive": "arXiv",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Bottou2016OptimizationMF"
  },
  {
    "id": "GQQU9PQb",
    "type": "article-journal",
    "title": "Publication Trends on the Varying Coefficients Model: Estimating the Actual (Under)Utilization of a Highly Acclaimed Method for Studying Statistical Interactions",
    "author": [
      {
        "family": "Botzer",
        "given": "Assaf"
      }
    ],
    "container-title": "Publ.",
    "issued": {
      "date-parts": [
        [
          2025
        ]
      ]
    },
    "volume": "13",
    "page": "19",
    "URL": "https://api.semanticscholar.org/CorpusID:277710491",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Botzer2025PublicationTO"
  },
  {
    "id": "gXylcdAj",
    "type": "article-journal",
    "author": [
      {
        "family": "Boyd",
        "given": "Stephen P."
      },
      {
        "family": "Parikh",
        "given": "Neal"
      },
      {
        "family": "Chu",
        "given": "Eric"
      },
      {
        "family": "Peleato",
        "given": "Borja"
      },
      {
        "family": "Eckstein",
        "given": "Jonathan"
      }
    ],
    "title": "Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers",
    "container-title": "Found. Trends Mach. Learn.",
    "volume": "3",
    "page": "1-122",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Boyd2011DistributedOA"
  },
  {
    "id": "nhdYYtrp",
    "type": "article-journal",
    "title": "Language Models are Few-Shot Learners",
    "author": [
      {
        "family": "Brown",
        "given": "Tom B."
      },
      {
        "family": "Mann",
        "given": "Benjamin"
      },
      {
        "family": "Ryder",
        "given": "Nick"
      },
      {
        "family": "Subbiah",
        "given": "Melanie"
      },
      {
        "family": "Kaplan",
        "given": "Jared"
      },
      {
        "family": "Dhariwal",
        "given": "Prafulla"
      },
      {
        "family": "Neelakantan",
        "given": "Arvind"
      },
      {
        "family": "Shyam",
        "given": "Pranav"
      },
      {
        "family": "Sastry",
        "given": "Girish"
      },
      {
        "family": "Askell",
        "given": "Amanda"
      },
      {
        "family": "Agarwal",
        "given": "Sandhini"
      },
      {
        "family": "Herbert-Voss",
        "given": "Ariel"
      },
      {
        "family": "Krueger",
        "given": "Gretchen"
      },
      {
        "family": "Henighan",
        "given": "T. J."
      },
      {
        "family": "Child",
        "given": "Rewon"
      },
      {
        "family": "Ramesh",
        "given": "Aditya"
      },
      {
        "family": "Ziegler",
        "given": "Daniel M."
      },
      {
        "family": "Wu",
        "given": "Jeff"
      },
      {
        "family": "Winter",
        "given": "Clemens"
      },
      {
        "family": "Hesse",
        "given": "Christopher"
      },
      {
        "family": "Chen",
        "given": "Mark"
      },
      {
        "family": "Sigler",
        "given": "Eric"
      },
      {
        "family": "Litwin",
        "given": "Mateusz"
      },
      {
        "family": "Gray",
        "given": "Scott"
      },
      {
        "family": "Chess",
        "given": "Benjamin"
      },
      {
        "family": "Clark",
        "given": "Jack"
      },
      {
        "family": "Berner",
        "given": "Christopher"
      },
      {
        "family": "McCandlish",
        "given": "Sam"
      },
      {
        "family": "Radford",
        "given": "Alec"
      },
      {
        "family": "Sutskever",
        "given": "Ilya"
      },
      {
        "family": "Amodei",
        "given": "Dario"
      }
    ],
    "container-title": "ArXiv",
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "volume": "abs/2005.14165",
    "URL": "https://api.semanticscholar.org/CorpusID:218971783",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Brown2020LanguageMA"
  },
  {
    "id": "1D89dIDtO",
    "type": "article-journal",
    "title": "Continuous Temporal Domain Generalization",
    "author": [
      {
        "family": "Cai",
        "given": "Zekun"
      },
      {
        "family": "Bai",
        "given": "Guangji"
      },
      {
        "family": "Jiang",
        "given": "Renhe"
      },
      {
        "family": "Song",
        "given": "Xuan"
      },
      {
        "family": "Zhao",
        "given": "Liang"
      }
    ],
    "container-title": "ArXiv",
    "issued": {
      "date-parts": [
        [
          2024
        ]
      ]
    },
    "volume": "abs/2405.16075",
    "URL": "https://api.semanticscholar.org/CorpusID:270063125",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Cai2024ContinuousTD"
  },
  {
    "id": "Bl30FFIi",
    "type": "paper-conference",
    "title": "BRITS: Bidirectional Recurrent Imputation for Time Series",
    "author": [
      {
        "family": "Cao",
        "given": "Wei"
      },
      {
        "family": "Wang",
        "given": "Dong"
      },
      {
        "family": "Li",
        "given": "Jian"
      },
      {
        "family": "Zhou",
        "given": "Hao"
      },
      {
        "family": "Li",
        "given": "Lei"
      },
      {
        "family": "Li",
        "given": "Yitan"
      }
    ],
    "container-title": "Advances in Neural Information Processing Systems 31 (NeurIPS 2018)",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "URL": "https://proceedings.neurips.cc/paper/2018/hash/734e6bfcd358e25ac1db0a4241b95651-Abstract.html",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Cao2018BRITS"
  },
  {
    "id": "R3MGlv2S",
    "type": "article-journal",
    "title": "Recurrent Neural Networks for Multivariate Time Series with Missing Values",
    "author": [
      {
        "family": "Che",
        "given": "Zhengping"
      },
      {
        "family": "Purushotham",
        "given": "Sanjay"
      },
      {
        "family": "Cho",
        "given": "Kyunghyun"
      },
      {
        "family": "Sontag",
        "given": "David"
      },
      {
        "family": "Liu",
        "given": "Yan"
      }
    ],
    "container-title": "Scientific Reports",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "volume": "8",
    "page": "6085",
    "DOI": "10.1038/s41598-018-24271-9",
    "URL": "https://www.nature.com/articles/s41598-018-24271-9",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Che2018GRUD"
  },
  {
    "id": "mcQbhuyn",
    "type": "paper-conference",
    "title": "XGBoost: A Scalable Tree Boosting System",
    "author": [
      {
        "family": "Chen",
        "given": "Tianqi"
      },
      {
        "family": "Guestrin",
        "given": "Carlos"
      }
    ],
    "container-title": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "page": "785-794",
    "DOI": "10.1145/2939672.2939785",
    "URL": "https://dl.acm.org/doi/10.1145/2939672.2939785",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Chen2016XGBoost"
  },
  {
    "id": "rwzMVtZm",
    "type": "paper-conference",
    "title": "This looks like that: deep learning for interpretable image recognition",
    "author": [
      {
        "family": "Chen",
        "given": "Chaofan"
      },
      {
        "family": "Li",
        "given": "Oscar"
      },
      {
        "family": "Barnett",
        "given": "Alina Jade"
      },
      {
        "family": "Su",
        "given": "Jonathan"
      },
      {
        "family": "Rudin",
        "given": "Cynthia"
      }
    ],
    "container-title": "Neural Information Processing Systems",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "URL": "https://api.semanticscholar.org/CorpusID:49482223",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Chen2018ThisLL"
  },
  {
    "id": "ySxuugBg",
    "type": "article-journal",
    "title": "LFME: A Simple Framework for Learning from Multiple Experts in Domain Generalization",
    "author": [
      {
        "family": "Chen",
        "given": "Liang"
      },
      {
        "family": "Zhang",
        "given": "Yong"
      },
      {
        "family": "Song",
        "given": "Yibing"
      },
      {
        "family": "Shen",
        "given": "Zhiqiang"
      },
      {
        "family": "Liu",
        "given": "Lingqiao"
      }
    ],
    "container-title": "ArXiv",
    "issued": {
      "date-parts": [
        [
          2024
        ]
      ]
    },
    "volume": "abs/2410.17020",
    "URL": "https://api.semanticscholar.org/CorpusID:273507138",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Chen2024LFMEAS"
  },
  {
    "id": "ynoNTdxR",
    "type": "paper-conference",
    "title": "Inference Suboptimality in Variational Autoencoders",
    "author": [
      {
        "family": "Cremer",
        "given": "Chris"
      },
      {
        "family": "Li",
        "given": "Xuechen"
      },
      {
        "family": "Duvenaud",
        "given": "David Kristjanson"
      }
    ],
    "container-title": "International Conference on Machine Learning",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "URL": "https://api.semanticscholar.org/CorpusID:3524184",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Cremer2018InferenceSI"
  },
  {
    "id": "4DoXpxVd",
    "type": "article-journal",
    "title": "Knowledge Neurons in Pretrained Transformers",
    "author": [
      {
        "family": "Dai",
        "given": "Damai"
      },
      {
        "family": "Dong",
        "given": "Li"
      },
      {
        "family": "Hao",
        "given": "Yaru"
      },
      {
        "family": "Sui",
        "given": "Zhifang"
      },
      {
        "family": "Wei",
        "given": "Furu"
      }
    ],
    "container-title": "ArXiv",
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "volume": "abs/2104.08696",
    "URL": "https://api.semanticscholar.org/CorpusID:233296761",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Dai2021KnowledgeNI"
  },
  {
    "id": "sbcsv77q",
    "type": "paper-conference",
    "title": "Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers",
    "author": [
      {
        "family": "Dai",
        "given": "Damai"
      },
      {
        "family": "Sun",
        "given": "Yutao"
      },
      {
        "family": "Dong",
        "given": "Li"
      },
      {
        "family": "Hao",
        "given": "Yaru"
      },
      {
        "family": "Ma",
        "given": "Shuming"
      },
      {
        "family": "Sui",
        "given": "Zhifang"
      },
      {
        "family": "Wei",
        "given": "Furu"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "URL": "https://api.semanticscholar.org/CorpusID:258686544",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Dai2022WhyCG"
  },
  {
    "id": "i1igC2BC",
    "type": "paper-conference",
    "title": "A Survey on In-context Learning",
    "author": [
      {
        "family": "Dong",
        "given": "Qingxiu"
      },
      {
        "family": "Li",
        "given": "Lei"
      },
      {
        "family": "Dai",
        "given": "Damai"
      },
      {
        "family": "Zheng",
        "given": "Ce"
      },
      {
        "family": "Wu",
        "given": "Zhiyong"
      },
      {
        "family": "Chang",
        "given": "Baobao"
      },
      {
        "family": "Sun",
        "given": "Xu"
      },
      {
        "family": "Xu",
        "given": "Jingjing"
      },
      {
        "family": "Li",
        "given": "Lei"
      },
      {
        "family": "Sui",
        "given": "Zhifang"
      }
    ],
    "container-title": "Conference on Empirical Methods in Natural Language Processing",
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "URL": "https://api.semanticscholar.org/CorpusID:255372865",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Dong2022ASO"
  },
  {
    "id": "4YiO662d",
    "type": "article-journal",
    "title": "Towards A Rigorous Science of Interpretable Machine Learning",
    "author": [
      {
        "family": "Doshi-Velez",
        "given": "Finale"
      },
      {
        "family": "Kim",
        "given": "Been"
      }
    ],
    "container-title": "arXiv: Machine Learning",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "URL": "https://api.semanticscholar.org/CorpusID:11319376",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: DoshiVelez2017TowardsAR"
  },
  {
    "id": "OngP0u23",
    "type": "paper-conference",
    "title": "Spatially Varying Coefficient Models for Estimating Heterogeneous Mixture Effects",
    "author": [
      {
        "family": "Englert",
        "given": "Jacob R."
      },
      {
        "family": "Chang",
        "given": "Howard"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025
        ]
      ]
    },
    "URL": "https://api.semanticscholar.org/CorpusID:276482233",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Englert2025SpatiallyVC"
  },
  {
    "id": "5HZh8pIu",
    "type": "article-journal",
    "title": "Network Varying Coefficient Model",
    "author": [
      {
        "family": "Fan",
        "given": "Xinyan"
      },
      {
        "family": "Fang",
        "given": "Kuangnan"
      },
      {
        "family": "Lan",
        "given": "Wei"
      },
      {
        "family": "Tsai",
        "given": "Chih-Ling"
      }
    ],
    "container-title": "Journal of the American Statistical Association",
    "issued": {
      "date-parts": [
        [
          2025
        ]
      ]
    },
    "URL": "https://api.semanticscholar.org/CorpusID:276599564",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Fan2025NetworkVC"
  },
  {
    "id": "wo0peFG5",
    "type": "paper-conference",
    "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
    "author": [
      {
        "family": "Finn",
        "given": "Chelsea"
      },
      {
        "family": "Abbeel",
        "given": "P."
      },
      {
        "family": "Levine",
        "given": "Sergey"
      }
    ],
    "container-title": "International Conference on Machine Learning",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "URL": "https://api.semanticscholar.org/CorpusID:6719686",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Finn2017ModelAgnosticMF"
  },
  {
    "id": "9W6kdR7s",
    "type": "article-journal",
    "title": "Shortcut learning in deep neural networks",
    "author": [
      {
        "family": "Geirhos",
        "given": "Robert"
      },
      {
        "family": "Jacobsen",
        "given": "JÃ¶rn-Henrik"
      },
      {
        "family": "Michaelis",
        "given": "Claudio"
      },
      {
        "family": "Zemel",
        "given": "Richard S."
      },
      {
        "family": "Brendel",
        "given": "Wieland"
      },
      {
        "family": "Bethge",
        "given": "Matthias"
      },
      {
        "family": "Wichmann",
        "given": "Felix"
      }
    ],
    "container-title": "Nature Machine Intelligence",
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "volume": "2",
    "page": "665-673",
    "URL": "https://api.semanticscholar.org/CorpusID:215786368",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Geirhos2020ShortcutLI"
  },
  {
    "id": "htzWcAM1",
    "type": "paper-conference",
    "author": [
      {
        "family": "Gelman",
        "given": "Andrew"
      },
      {
        "family": "Su",
        "given": "Yu-Sung"
      }
    ],
    "title": "Data Analysis Using Regression and Multilevel/Hierarchical Models",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Gelman2006DataAU"
  },
  {
    "id": "PHDk950u",
    "type": "paper-conference",
    "title": "Towards Automatic Concept-based Explanations",
    "author": [
      {
        "family": "Ghorbani",
        "given": "Amirata"
      },
      {
        "family": "Wexler",
        "given": "James"
      },
      {
        "family": "Zou",
        "given": "James Y."
      },
      {
        "family": "Kim",
        "given": "Been"
      }
    ],
    "container-title": "Neural Information Processing Systems",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "URL": "https://api.semanticscholar.org/CorpusID:184487319",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Ghorbani2019TowardsAC"
  },
  {
    "id": "pirD8JuJ",
    "type": "article-journal",
    "title": "VaryingâCoefficient Models",
    "author": [
      {
        "family": "Hastie",
        "given": "Trevor J."
      },
      {
        "family": "Tibshirani",
        "given": "Robert"
      }
    ],
    "container-title": "Journal of the Royal Statistical Society. Series B (Methodological)",
    "issued": {
      "date-parts": [
        [
          1993
        ]
      ]
    },
    "volume": "55",
    "page": "757-779",
    "URL": "https://api.semanticscholar.org/CorpusID:125170071",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Hastie1993VaryingCoefficientM"
  },
  {
    "id": "um27DIkT",
    "type": "paper-conference",
    "title": "beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework",
    "author": [
      {
        "family": "Higgins",
        "given": "Irina"
      },
      {
        "family": "Matthey",
        "given": "LoÃ¯c"
      },
      {
        "family": "Pal",
        "given": "Arka"
      },
      {
        "family": "Burgess",
        "given": "Christopher P."
      },
      {
        "family": "Glorot",
        "given": "Xavier"
      },
      {
        "family": "Botvinick",
        "given": "Matthew M."
      },
      {
        "family": "Mohamed",
        "given": "Shakir"
      },
      {
        "family": "Lerchner",
        "given": "Alexander"
      }
    ],
    "container-title": "International Conference on Learning Representations",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "URL": "https://api.semanticscholar.org/CorpusID:46798026",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Higgins2016betaVAELB"
  },
  {
    "id": "Zc8hnVfA",
    "type": "manuscript",
    "title": "Training Compute-Optimal Large Language Models",
    "author": [
      {
        "family": "Hoffmann",
        "given": "Jordan"
      },
      {
        "family": "Borgeaud",
        "given": "Sebastian"
      },
      {
        "family": "Mensch",
        "given": "Arthur"
      },
      {
        "family": "Buchatskaya",
        "given": "Elena"
      },
      {
        "family": "Cai",
        "given": "Trevor"
      },
      {
        "family": "Rutherford",
        "given": "Eliza"
      },
      {
        "family": "de Las Casas",
        "given": "Diego"
      },
      {
        "family": "Hendricks",
        "given": "Lisa Anne"
      },
      {
        "family": "Welbl",
        "given": "Johannes"
      },
      {
        "family": "Clark",
        "given": "Aidan"
      },
      {
        "family": "Hennigan",
        "given": "Tom"
      },
      {
        "family": "Noland",
        "given": "Eric"
      },
      {
        "family": "Millican",
        "given": "Katie"
      },
      {
        "family": "van den Driessche",
        "given": "George"
      },
      {
        "family": "Damoc",
        "given": "Bogdan"
      },
      {
        "family": "Guy",
        "given": "Aurelia"
      },
      {
        "family": "Osindero",
        "given": "Simon"
      },
      {
        "family": "Simonyan",
        "given": "Karen"
      },
      {
        "family": "Elsen",
        "given": "Erich"
      },
      {
        "family": "Rae",
        "given": "Jack W."
      },
      {
        "family": "Vinyals",
        "given": "Oriol"
      },
      {
        "family": "Sifre",
        "given": "Laurent"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "archive": "arXiv",
    "URL": "https://arxiv.org/abs/2203.15556",
    "note": "cs.CL\nLoaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Hoffmann2022TrainingCO"
  },
  {
    "id": "AhnjNoZf",
    "type": "article-journal",
    "title": "Meta-Learning in Neural Networks: A Survey",
    "author": [
      {
        "family": "Hospedales",
        "given": "Timothy M."
      },
      {
        "family": "Antoniou",
        "given": "Antreas"
      },
      {
        "family": "Micaelli",
        "given": "Paul"
      },
      {
        "family": "Storkey",
        "given": "Amos J."
      }
    ],
    "container-title": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "volume": "44",
    "page": "5149-5169",
    "URL": "https://api.semanticscholar.org/CorpusID:215744839",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Hospedales2020MetaLearningIN"
  },
  {
    "id": "7SNrxgEk",
    "type": "article-journal",
    "title": "Unsupervised Learning via Meta-Learning",
    "author": [
      {
        "family": "Hsu",
        "given": "Kyle"
      },
      {
        "family": "Levine",
        "given": "Sergey"
      },
      {
        "family": "Finn",
        "given": "Chelsea"
      }
    ],
    "container-title": "ArXiv",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "volume": "abs/1810.02334",
    "URL": "https://api.semanticscholar.org/CorpusID:52922125",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Hsu2018UnsupervisedLV"
  },
  {
    "id": "KBnuztMa",
    "type": "paper-conference",
    "title": "Harder Task Needs More Experts: Dynamic Routing in MoE Models",
    "author": [
      {
        "family": "Huang",
        "given": "Quzhe"
      },
      {
        "family": "An",
        "given": "Zhenwei"
      },
      {
        "family": "Nan",
        "given": "Zhuang"
      },
      {
        "family": "Tao",
        "given": "Mingxu"
      },
      {
        "family": "Zhang",
        "given": "Chen"
      },
      {
        "family": "Jin",
        "given": "Yang"
      },
      {
        "family": "Xu",
        "given": "Kun"
      },
      {
        "family": "Chen",
        "given": "Liwei"
      },
      {
        "family": "Huang",
        "given": "Songfang"
      },
      {
        "family": "Feng",
        "given": "Yansong"
      }
    ],
    "container-title": "Annual Meeting of the Association for Computational Linguistics",
    "issued": {
      "date-parts": [
        [
          2024
        ]
      ]
    },
    "URL": "https://api.semanticscholar.org/CorpusID:271895204",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Huang2024HarderTN"
  },
  {
    "id": "1DD1jAYZd",
    "type": "article-journal",
    "title": "Variational Autoencoder with Arbitrary Conditioning",
    "author": [
      {
        "family": "Ivanov",
        "given": "Oleg"
      },
      {
        "family": "Figurnov",
        "given": "Michael"
      },
      {
        "family": "Vetrov",
        "given": "Dmitry"
      }
    ],
    "container-title": "arXiv",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "volume": "abs/1806.02382",
    "URL": "https://arxiv.org/abs/1806.02382",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Ivanov2019VAEAC"
  },
  {
    "id": "5UsFy1cc",
    "type": "article-journal",
    "title": "Adaptive Mixtures of Local Experts",
    "author": [
      {
        "family": "Jacobs",
        "given": "Robert A."
      },
      {
        "family": "Jordan",
        "given": "Michael I."
      },
      {
        "family": "Nowlan",
        "given": "Steven J."
      },
      {
        "family": "Hinton",
        "given": "Geoffrey E."
      }
    ],
    "container-title": "Neural Computation",
    "issued": {
      "date-parts": [
        [
          1991
        ]
      ]
    },
    "volume": "3",
    "page": "79-87",
    "URL": "https://api.semanticscholar.org/CorpusID:572361",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Jacobs1991AdaptiveMO"
  },
  {
    "id": "1EAtiMm6o",
    "type": "manuscript",
    "title": "Scaling Laws for Neural Language Models",
    "author": [
      {
        "family": "Kaplan",
        "given": "Jared"
      },
      {
        "family": "McCandlish",
        "given": "Sam"
      },
      {
        "family": "Henighan",
        "given": "Tom"
      },
      {
        "family": "Brown",
        "given": "Tom B."
      },
      {
        "family": "Chess",
        "given": "Benjamin"
      },
      {
        "family": "Child",
        "given": "Rewon"
      },
      {
        "family": "Gray",
        "given": "Scott"
      },
      {
        "family": "Radford",
        "given": "Alec"
      },
      {
        "family": "Wu",
        "given": "Jeffrey"
      },
      {
        "family": "Amodei",
        "given": "Dario"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "archive": "arXiv",
    "URL": "https://arxiv.org/abs/2001.08361",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Kaplan2020ScalingLF"
  },
  {
    "id": "uaQfPsiI",
    "type": "paper-conference",
    "title": "Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)",
    "author": [
      {
        "family": "Kim",
        "given": "Been"
      },
      {
        "family": "Wattenberg",
        "given": "Martin"
      },
      {
        "family": "Gilmer",
        "given": "Justin"
      },
      {
        "family": "Cai",
        "given": "Carrie J."
      },
      {
        "family": "Wexler",
        "given": "James"
      },
      {
        "family": "ViÃ©gas",
        "given": "Fernanda B."
      },
      {
        "family": "Sayres",
        "given": "Rory"
      }
    ],
    "container-title": "International Conference on Machine Learning",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "URL": "https://api.semanticscholar.org/CorpusID:51737170",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Kim2017InterpretabilityBF"
  },
  {
    "id": "run7WIHy",
    "type": "paper-conference",
    "title": "Test-Time Adaptation Induces Stronger Accuracy and Agreement-on-the-Line",
    "author": [
      {
        "family": "Kim",
        "given": "Eungyeup"
      },
      {
        "family": "Sun",
        "given": "Mingjie"
      },
      {
        "family": "Baek",
        "given": "Christina"
      },
      {
        "family": "Raghunathan",
        "given": "Aditi"
      },
      {
        "family": "Kolter",
        "given": "J. Zico"
      }
    ],
    "container-title": "Neural Information Processing Systems",
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "URL": "https://api.semanticscholar.org/CorpusID:263830662",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Kim2023TestTimeAI"
  },
  {
    "id": "HVOSvrMU",
    "type": "article-journal",
    "title": "Auto-Encoding Variational Bayes",
    "author": [
      {
        "family": "Kingma",
        "given": "Diederik P."
      },
      {
        "family": "Welling",
        "given": "Max"
      }
    ],
    "container-title": "CoRR",
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "volume": "abs/1312.6114",
    "URL": "https://api.semanticscholar.org/CorpusID:216078090",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Kingma2013AutoEncodingVB"
  },
  {
    "id": "fV3bp5El",
    "type": "manuscript",
    "author": [
      {
        "family": "Kingma",
        "given": "Diederik P."
      },
      {
        "family": "Ba",
        "given": "Jimmy"
      }
    ],
    "title": "Adam: A Method for Stochastic Optimization",
    "archive": "arXiv",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Kingma2014AdamAM"
  },
  {
    "id": "4HmAqDnt",
    "type": "paper-conference",
    "title": "Understanding Black-box Predictions via Influence Functions",
    "author": [
      {
        "family": "Koh",
        "given": "Pang Wei"
      },
      {
        "family": "Liang",
        "given": "Percy"
      }
    ],
    "container-title": "International Conference on Machine Learning",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "URL": "https://api.semanticscholar.org/CorpusID:13193974",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Koh2017UnderstandingBP"
  },
  {
    "id": "ViAP9fh7",
    "type": "paper-conference",
    "title": "WILDS: A Benchmark of in-the-Wild Distribution Shifts",
    "author": [
      {
        "family": "Koh",
        "given": "Pang Wei"
      },
      {
        "family": "Sagawa",
        "given": "Shiori"
      },
      {
        "family": "Marklund",
        "given": "Henrik"
      },
      {
        "family": "Xie",
        "given": "Sang Michael"
      },
      {
        "family": "Zhang",
        "given": "Marvin"
      },
      {
        "family": "Balsubramani",
        "given": "Akshay"
      },
      {
        "family": "Hu",
        "given": "Weihua"
      },
      {
        "family": "Yasunaga",
        "given": "Michihiro"
      },
      {
        "family": "Phillips",
        "given": "Richard Lanas"
      },
      {
        "family": "Gao",
        "given": "Irena"
      },
      {
        "family": "Lee",
        "given": "Tony"
      },
      {
        "family": "David",
        "given": "Etiene"
      },
      {
        "family": "Stavness",
        "given": "Ian"
      },
      {
        "family": "Guo",
        "given": "Wei"
      },
      {
        "family": "Earnshaw",
        "given": "Berton A."
      },
      {
        "family": "Haque",
        "given": "Imran S."
      },
      {
        "family": "Beery",
        "given": "Sara Meghan"
      },
      {
        "family": "Leskovec",
        "given": "Jure"
      },
      {
        "family": "Kundaje",
        "given": "Anshul"
      },
      {
        "family": "Pierson",
        "given": "Emma"
      },
      {
        "family": "Levine",
        "given": "Sergey"
      },
      {
        "family": "Finn",
        "given": "Chelsea"
      },
      {
        "family": "Liang",
        "given": "Percy"
      }
    ],
    "container-title": "International Conference on Machine Learning",
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "URL": "https://api.semanticscholar.org/CorpusID:229156320",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Koh2020WILDSAB"
  },
  {
    "id": "I3kCk5k2",
    "type": "paper-conference",
    "title": "WILDS: A Benchmark of in-the-Wild Distribution Shifts",
    "author": [
      {
        "family": "Koh",
        "given": "Pang Wei"
      },
      {
        "family": "Sagawa",
        "given": "Shiori"
      },
      {
        "family": "Marklund",
        "given": "Henrik"
      },
      {
        "family": "Xie",
        "given": "Sang Michael"
      },
      {
        "family": "Zhang",
        "given": "Marvin"
      },
      {
        "family": "Balsubramani",
        "given": "Akshay"
      },
      {
        "family": "Hu",
        "given": "Weihua"
      },
      {
        "family": "Levine",
        "given": "Sergey"
      },
      {
        "family": "Finn",
        "given": "Chelsea"
      },
      {
        "family": "Liang",
        "given": "Percy"
      }
    ],
    "container-title": "Proceedings of the 38th International Conference on Machine Learning",
    "publisher": "PMLR",
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "volume": "139",
    "URL": "https://proceedings.mlr.press/v139/koh21a.html",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Koh2021WILDS"
  },
  {
    "id": "v0xAySK8",
    "type": "paper-conference",
    "title": "In-Context Explainers: Harnessing LLMs for Explaining Black Box Models",
    "author": [
      {
        "family": "Kroeger",
        "given": "Nicholas"
      },
      {
        "family": "Ley",
        "given": "Dan"
      },
      {
        "family": "Krishna",
        "given": "Satyapriya"
      },
      {
        "family": "Agarwal",
        "given": "Chirag"
      },
      {
        "family": "Lakkaraju",
        "given": "Himabindu"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "URL": "https://api.semanticscholar.org/CorpusID:263829193",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Kroeger2023InContextEH"
  },
  {
    "id": "a6BiplBF",
    "type": "paper-conference",
    "title": "The Selective Labels Problem: Evaluating Algorithmic Predictions in the Presence of Unobservables",
    "author": [
      {
        "family": "Lakkaraju",
        "given": "Himabindu"
      },
      {
        "family": "Kleinberg",
        "given": "Jon M."
      },
      {
        "family": "Leskovec",
        "given": "Jure"
      },
      {
        "family": "Ludwig",
        "given": "Jens"
      },
      {
        "family": "Mullainathan",
        "given": "Sendhil"
      }
    ],
    "container-title": "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "URL": "https://api.semanticscholar.org/CorpusID:2178920",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Lakkaraju2017TheSL"
  },
  {
    "id": "MoVIrux0",
    "type": "article-journal",
    "title": "Optimal pointwise adaptive methods in nonparametric estimation",
    "author": [
      {
        "family": "Lepski",
        "given": "Oleg V."
      },
      {
        "family": "Spokoiny",
        "given": "Vladimir G."
      }
    ],
    "container-title": "Annals of Statistics",
    "issued": {
      "date-parts": [
        [
          1997
        ]
      ]
    },
    "volume": "25",
    "page": "2512-2546",
    "URL": "https://api.semanticscholar.org/CorpusID:2635430",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Lepski1997OptimalPA"
  },
  {
    "id": "IeaH24Dv",
    "type": "article-journal",
    "author": [
      {
        "family": "Linardatos",
        "given": "Pantelis"
      },
      {
        "family": "Papastefanopoulos",
        "given": "Vasilis"
      },
      {
        "family": "Kotsiantis",
        "given": "Sotiris B."
      }
    ],
    "title": "Explainable AI: A Review of Machine Learning Interpretability Methods",
    "container-title": "Entropy",
    "volume": "23",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Linardatos2020ExplainableAA"
  },
  {
    "id": "jqcBCPQs",
    "type": "article-journal",
    "title": "A Class of Pattern-Mixture Models for Normal Incomplete Data",
    "author": [
      {
        "family": "Little",
        "given": "Roderick J. A."
      }
    ],
    "container-title": "Biometrika",
    "issued": {
      "date-parts": [
        [
          1994
        ]
      ]
    },
    "volume": "81",
    "issue": "3",
    "page": "471-483",
    "DOI": "10.1093/biomet/81.3.471",
    "URL": "https://academic.oup.com/biomet/article-abstract/81/3/471/256979",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Little1994PatternMixture"
  },
  {
    "id": "nhWQSRBg",
    "type": "article-journal",
    "title": "The weighted majority algorithm",
    "author": [
      {
        "family": "Littlestone",
        "given": "Nick"
      },
      {
        "family": "Warmuth",
        "given": "Manfred K."
      }
    ],
    "container-title": "30th Annual Symposium on Foundations of Computer Science",
    "issued": {
      "date-parts": [
        [
          1989
        ]
      ]
    },
    "page": "256-261",
    "URL": "https://api.semanticscholar.org/CorpusID:12843330",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Littlestone1989TheWM"
  },
  {
    "id": "z4KxhXha",
    "type": "article-journal",
    "title": "A Closer Look into Mixture-of-Experts in Large Language Models",
    "author": [
      {
        "family": "Lo",
        "given": "Ka Man"
      },
      {
        "family": "Huang",
        "given": "Zeyu"
      },
      {
        "family": "Qiu",
        "given": "Zihan"
      },
      {
        "family": "Wang",
        "given": "Zili"
      },
      {
        "family": "Fu",
        "given": "Jie"
      }
    ],
    "container-title": "ArXiv",
    "issued": {
      "date-parts": [
        [
          2024
        ]
      ]
    },
    "volume": "abs/2406.18219",
    "URL": "https://api.semanticscholar.org/CorpusID:270737825",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Lo2024ACL"
  },
  {
    "id": "oMsXuhzd",
    "type": "paper-conference",
    "title": "Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations",
    "author": [
      {
        "family": "Locatello",
        "given": "Francesco"
      },
      {
        "family": "Bauer",
        "given": "Stefan"
      },
      {
        "family": "Lucic",
        "given": "Mario"
      },
      {
        "family": "Gelly",
        "given": "Sylvain"
      },
      {
        "family": "Scholkopf",
        "given": "Bernhard"
      },
      {
        "family": "Bachem",
        "given": "Olivier"
      }
    ],
    "container-title": "International Conference on Machine Learning",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "URL": "https://api.semanticscholar.org/CorpusID:54089884",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Locatello2018ChallengingCA"
  },
  {
    "id": "mrmXRBJo",
    "type": "article-journal",
    "title": "Penalized Spline Estimation for Varying-Coefficient Models",
    "author": [
      {
        "family": "Lu",
        "given": "Yiqiang"
      },
      {
        "family": "Zhang",
        "given": "Riquan"
      },
      {
        "family": "Zhu",
        "given": "Liping"
      }
    ],
    "container-title": "Communications in Statistics - Theory and Methods",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "volume": "37",
    "page": "2249-2261",
    "URL": "https://api.semanticscholar.org/CorpusID:121809483",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Lu2008PenalizedSE"
  },
  {
    "id": "3XVsx394",
    "type": "paper-conference",
    "title": "A Unified Approach to Interpreting Model Predictions",
    "author": [
      {
        "family": "Lundberg",
        "given": "Scott M."
      },
      {
        "family": "Lee",
        "given": "Su-In"
      }
    ],
    "container-title": "Neural Information Processing Systems",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "URL": "https://api.semanticscholar.org/CorpusID:21889700",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Lundberg2017AUA"
  },
  {
    "id": "1FCZIJJbH",
    "type": "paper-conference",
    "title": "Locating and Editing Factual Associations in GPT",
    "author": [
      {
        "family": "Meng",
        "given": "Kevin"
      },
      {
        "family": "Bau",
        "given": "David"
      },
      {
        "family": "Andonian",
        "given": "Alex"
      },
      {
        "family": "Belinkov",
        "given": "Yonatan"
      }
    ],
    "container-title": "Neural Information Processing Systems",
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "URL": "https://api.semanticscholar.org/CorpusID:255825985",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Meng2022LocatingAE"
  },
  {
    "id": "cEMOLLg",
    "type": "article-journal",
    "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",
    "author": [
      {
        "family": "Min",
        "given": "Sewon"
      },
      {
        "family": "Lyu",
        "given": "Xinxi"
      },
      {
        "family": "Holtzman",
        "given": "Ari"
      },
      {
        "family": "Artetxe",
        "given": "Mikel"
      },
      {
        "family": "Lewis",
        "given": "Mike"
      },
      {
        "family": "Hajishirzi",
        "given": "Hannaneh"
      },
      {
        "family": "Zettlemoyer",
        "given": "Luke"
      }
    ],
    "container-title": "ArXiv",
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "volume": "abs/2202.12837",
    "URL": "https://api.semanticscholar.org/CorpusID:247155069",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Min2022RethinkingTR"
  },
  {
    "id": "ye7pP794",
    "type": "article-journal",
    "title": "Multiple imputation of incomplete multilevel data using Heckman selection models",
    "author": [
      {
        "family": "MuÃ±oz",
        "given": "Johanna"
      },
      {
        "family": "Audibert",
        "given": "Matthieu"
      },
      {
        "family": "Pavlou",
        "given": "Melina"
      },
      {
        "family": "Riley",
        "given": "Richard D."
      },
      {
        "family": "Debray",
        "given": "Thomas P. A."
      }
    ],
    "container-title": "arXiv",
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "volume": "abs/2301.05043",
    "URL": "https://arxiv.org/abs/2301.05043",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Munoz2023HeckmanMI"
  },
  {
    "id": "1BatMyNyU",
    "type": "article-journal",
    "title": "Fast SpatioâTemporally Varying Coefficient Modeling With Reluctant Interaction Selection",
    "author": [
      {
        "family": "Murakami",
        "given": "Daisuke"
      },
      {
        "family": "Shirota",
        "given": "Shinichiro"
      },
      {
        "family": "Kajita",
        "given": "Seiji"
      },
      {
        "family": "Kajita",
        "given": "Mami"
      }
    ],
    "container-title": "Geographical Analysis",
    "issued": {
      "date-parts": [
        [
          2024
        ]
      ]
    },
    "URL": "https://api.semanticscholar.org/CorpusID:273233361",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Murakami2024FastSV"
  },
  {
    "id": "5xvP4cZ1",
    "type": "article-journal",
    "title": "In-context Learning and Induction Heads",
    "author": [
      {
        "family": "Olsson",
        "given": "Catherine"
      },
      {
        "family": "Elhage",
        "given": "Nelson"
      },
      {
        "family": "Nanda",
        "given": "Neel"
      },
      {
        "family": "Joseph",
        "given": "Nicholas"
      },
      {
        "family": "Dassarma",
        "given": "Nova"
      },
      {
        "family": "Henighan",
        "given": "T. J."
      },
      {
        "family": "Mann",
        "given": "Benjamin"
      },
      {
        "family": "Askell",
        "given": "Amanda"
      },
      {
        "family": "Bai",
        "given": "Yuntao"
      },
      {
        "family": "Chen",
        "given": "Anna"
      },
      {
        "family": "Conerly",
        "given": "Tom"
      },
      {
        "family": "Drain",
        "given": "Dawn"
      },
      {
        "family": "Ganguli",
        "given": "Deep"
      },
      {
        "family": "Hatfield-Dodds",
        "given": "Zac"
      },
      {
        "family": "Hernandez",
        "given": "Danny"
      },
      {
        "family": "Johnston",
        "given": "Scott"
      },
      {
        "family": "Jones",
        "given": "Andy"
      },
      {
        "family": "Kernion",
        "given": "John"
      },
      {
        "family": "Lovitt",
        "given": "Liane"
      },
      {
        "family": "Ndousse",
        "given": "Kamal"
      },
      {
        "family": "Amodei",
        "given": "Dario"
      },
      {
        "family": "Brown",
        "given": "Tom B."
      },
      {
        "family": "Clark",
        "given": "Jack"
      },
      {
        "family": "Kaplan",
        "given": "Jared"
      },
      {
        "family": "McCandlish",
        "given": "Sam"
      },
      {
        "family": "Olah",
        "given": "Chris"
      }
    ],
    "container-title": "ArXiv",
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "volume": "abs/2209.11895",
    "URL": "https://api.semanticscholar.org/CorpusID:252532078",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Olsson2022IncontextLA"
  },
  {
    "id": "Hyh4oJHn",
    "type": "article-journal",
    "title": "Towards Modular LLMs by Building and Reusing a Library of LoRAs",
    "author": [
      {
        "family": "Ostapenko",
        "given": "Oleksiy"
      },
      {
        "family": "Su",
        "given": "Zhan"
      },
      {
        "family": "Ponti",
        "given": "E."
      },
      {
        "family": "Charlin",
        "given": "Laurent"
      },
      {
        "family": "Le Roux",
        "given": "Nicolas"
      },
      {
        "family": "Pereira",
        "given": "Matheus"
      },
      {
        "family": "Caccia",
        "given": "Lucas"
      },
      {
        "family": "Sordoni",
        "given": "Alessandro"
      }
    ],
    "container-title": "ArXiv",
    "issued": {
      "date-parts": [
        [
          2024
        ]
      ]
    },
    "volume": "abs/2405.11157",
    "URL": "https://api.semanticscholar.org/CorpusID:269922018",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Ostapenko2024TowardsML"
  },
  {
    "id": "TcDVxgHv",
    "type": "article-journal",
    "title": "Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning",
    "author": [
      {
        "family": "Papernot",
        "given": "Nicolas"
      },
      {
        "family": "Mcdaniel",
        "given": "Patrick"
      }
    ],
    "container-title": "ArXiv",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "volume": "abs/1803.04765",
    "URL": "https://api.semanticscholar.org/CorpusID:3882460",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Papernot2018DeepKN"
  },
  {
    "id": "eWa6TjX2",
    "type": "manuscript",
    "author": [
      {
        "family": "Petroni",
        "given": "Fabio"
      },
      {
        "family": "RocktÃ¤schel",
        "given": "Tim"
      },
      {
        "family": "Lewis",
        "given": "Patrick"
      },
      {
        "family": "Bakhtin",
        "given": "Anton"
      },
      {
        "family": "Wu",
        "given": "Yuxiang"
      },
      {
        "family": "Miller",
        "given": "Alexander H."
      },
      {
        "family": "Riedel",
        "given": "Sebastian"
      }
    ],
    "title": "Language Models as Knowledge Bases?",
    "archive": "arXiv",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Petroni2019LanguageMA"
  },
  {
    "id": "6oC3BWiG",
    "type": "paper-conference",
    "title": "âWhy Should I Trust You?â: Explaining the Predictions of Any Classifier",
    "author": [
      {
        "family": "Ribeiro",
        "given": "Marco Tulio"
      },
      {
        "family": "Singh",
        "given": "Sameer"
      },
      {
        "family": "Guestrin",
        "given": "Carlos"
      }
    ],
    "container-title": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "URL": "https://api.semanticscholar.org/CorpusID:13029170",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Ribeiro2016WhySI"
  },
  {
    "id": "10qodfWCa",
    "type": "article-journal",
    "title": "An Overview of Multi-Task Learning in Deep Neural Networks",
    "author": [
      {
        "family": "Ruder",
        "given": "Sebastian"
      }
    ],
    "container-title": "ArXiv",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "volume": "abs/1706.05098",
    "URL": "https://api.semanticscholar.org/CorpusID:10175374",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Ruder2017AnOO"
  },
  {
    "id": "5vHnjc4b",
    "type": "article-journal",
    "title": "Distributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Case Generalization",
    "author": [
      {
        "family": "Sagawa",
        "given": "Shiori"
      },
      {
        "family": "Koh",
        "given": "Pang Wei"
      },
      {
        "family": "Hashimoto",
        "given": "Tatsunori B."
      },
      {
        "family": "Liang",
        "given": "Percy"
      }
    ],
    "container-title": "ArXiv",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "volume": "abs/1911.08731",
    "URL": "https://api.semanticscholar.org/CorpusID:208176471",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Sagawa2019DistributionallyRN"
  },
  {
    "id": "J9mQL0g5",
    "type": "article-journal",
    "title": "Distributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Case Generalization",
    "author": [
      {
        "family": "Sagawa",
        "given": "Shiori"
      },
      {
        "family": "Koh",
        "given": "Pang Wei"
      },
      {
        "family": "Hashimoto",
        "given": "Tatsunori B."
      },
      {
        "family": "Liang",
        "given": "Percy"
      }
    ],
    "container-title": "arXiv",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "volume": "abs/1911.08731",
    "URL": "https://arxiv.org/abs/1911.08731",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Sagawa2019GroupDRO"
  },
  {
    "id": "1Fc0kIYQ4",
    "type": "article-journal",
    "title": "Scalable Multi-Domain Adaptation of Language Models using Modular Experts",
    "author": [
      {
        "family": "Schafhalter",
        "given": "Peter"
      },
      {
        "family": "Liao",
        "given": "Shun"
      },
      {
        "family": "Zhou",
        "given": "Yanqi"
      },
      {
        "family": "Yeh",
        "given": "Chih-Kuan"
      },
      {
        "family": "Kandoor",
        "given": "Arun"
      },
      {
        "family": "Laudon",
        "given": "James"
      }
    ],
    "container-title": "ArXiv",
    "issued": {
      "date-parts": [
        [
          2024
        ]
      ]
    },
    "volume": "abs/2410.10181",
    "URL": "https://api.semanticscholar.org/CorpusID:273346210",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Schafhalter2024ScalableMA"
  },
  {
    "id": "rQnxWUdp",
    "type": "paper-conference",
    "title": "Learning Important Features Through Propagating Activation Differences",
    "author": [
      {
        "family": "Shrikumar",
        "given": "Avanti"
      },
      {
        "family": "Greenside",
        "given": "Peyton"
      },
      {
        "family": "Kundaje",
        "given": "Anshul"
      }
    ],
    "container-title": "International Conference on Machine Learning",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "URL": "https://api.semanticscholar.org/CorpusID:3385018",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Shrikumar2017LearningIF"
  },
  {
    "id": "MfWOgMP0",
    "type": "paper-conference",
    "title": "Axiomatic Attribution for Deep Networks",
    "author": [
      {
        "family": "Sundararajan",
        "given": "Mukund"
      },
      {
        "family": "Taly",
        "given": "Ankur"
      },
      {
        "family": "Yan",
        "given": "Qiqi"
      }
    ],
    "container-title": "International Conference on Machine Learning",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "URL": "https://api.semanticscholar.org/CorpusID:16747630",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Sundararajan2017AxiomaticAF"
  },
  {
    "id": "dh6xe0MV",
    "type": "article-journal",
    "title": "What do you learn from context? Probing for sentence structure in contextualized word representations",
    "author": [
      {
        "family": "Tenney",
        "given": "Ian"
      },
      {
        "family": "Xia",
        "given": "Patrick"
      },
      {
        "family": "Chen",
        "given": "Berlin"
      },
      {
        "family": "Wang",
        "given": "Alex"
      },
      {
        "family": "Poliak",
        "given": "Adam"
      },
      {
        "family": "McCoy",
        "given": "R. Thomas"
      },
      {
        "family": "Kim",
        "given": "Najoung"
      },
      {
        "family": "Van Durme",
        "given": "Benjamin"
      },
      {
        "family": "Bowman",
        "given": "Samuel R."
      },
      {
        "family": "Das",
        "given": "Dipanjan"
      },
      {
        "family": "Pavlick",
        "given": "Ellie"
      }
    ],
    "container-title": "ArXiv",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "volume": "abs/1905.06316",
    "URL": "https://api.semanticscholar.org/CorpusID:108300988",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Tenney2019WhatDY"
  },
  {
    "id": "5ykU3Zv8",
    "type": "article-journal",
    "title": "The Missing Indicator Method: From Low to High Dimensions",
    "author": [
      {
        "family": "Van Ness",
        "given": "Mike"
      },
      {
        "family": "Bosschieter",
        "given": "Tomas M."
      },
      {
        "family": "Halpin-Gregorio",
        "given": "Roberto"
      },
      {
        "family": "Udell",
        "given": "Madeleine"
      }
    ],
    "container-title": "arXiv",
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "volume": "abs/2211.09259",
    "URL": "https://arxiv.org/abs/2211.09259",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: VanNess2022MissingIndicator"
  },
  {
    "id": "1BIpbSBB",
    "type": "article-journal",
    "title": "An overview of statistical learning theory",
    "author": [
      {
        "family": "Vapnik",
        "given": "Vladimir Naumovich"
      }
    ],
    "container-title": "IEEE Transactions on Neural Networks",
    "issued": {
      "date-parts": [
        [
          1999
        ]
      ]
    },
    "volume": "10 5",
    "page": "988-999",
    "URL": "https://api.semanticscholar.org/CorpusID:6294728",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Vapnik1999AnOO"
  },
  {
    "id": "1GERSXJ6W",
    "type": "paper-conference",
    "title": "Attention is All you Need",
    "author": [
      {
        "family": "Vaswani",
        "given": "Ashish"
      },
      {
        "family": "Shazeer",
        "given": "Noam M."
      },
      {
        "family": "Parmar",
        "given": "Niki"
      },
      {
        "family": "Uszkoreit",
        "given": "Jakob"
      },
      {
        "family": "Jones",
        "given": "Llion"
      },
      {
        "family": "Gomez",
        "given": "Aidan N."
      },
      {
        "family": "Kaiser",
        "given": "Lukasz"
      },
      {
        "family": "Polosukhin",
        "given": "Illia"
      }
    ],
    "container-title": "Neural Information Processing Systems",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "URL": "https://api.semanticscholar.org/CorpusID:13756489",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Vaswani2017AttentionIA"
  },
  {
    "id": "jqqbYcAr",
    "type": "manuscript",
    "author": [
      {
        "family": "Wei",
        "given": "Jason"
      },
      {
        "family": "Wang",
        "given": "Xuezhi"
      },
      {
        "family": "Schuurmans",
        "given": "Dale"
      },
      {
        "family": "Bosma",
        "given": "Maarten"
      },
      {
        "family": "Chi",
        "given": "Ed H."
      },
      {
        "family": "Xia",
        "given": "F."
      },
      {
        "family": "Le",
        "given": "Quoc"
      },
      {
        "family": "Zhou",
        "given": "Denny"
      }
    ],
    "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
    "archive": "arXiv",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Wei2022ChainOT"
  },
  {
    "id": "oCuEj46V",
    "type": "article-journal",
    "title": "Emergent Abilities of Large Language Models",
    "author": [
      {
        "family": "Wei",
        "given": "Jason"
      },
      {
        "family": "Tay",
        "given": "Yi"
      },
      {
        "family": "Bommasani",
        "given": "Rishi"
      },
      {
        "family": "Raffel",
        "given": "Colin"
      },
      {
        "family": "Zoph",
        "given": "Barret"
      },
      {
        "family": "Borgeaud",
        "given": "Sebastian"
      },
      {
        "family": "Yogatama",
        "given": "Dani"
      },
      {
        "family": "Bosma",
        "given": "Maarten"
      },
      {
        "family": "Zhou",
        "given": "Denny"
      },
      {
        "family": "Metzler",
        "given": "Donald"
      },
      {
        "family": "Chi",
        "given": "Ed H."
      },
      {
        "family": "Hashimoto",
        "given": "Tatsunori"
      },
      {
        "family": "Vinyals",
        "given": "Oriol"
      },
      {
        "family": "Liang",
        "given": "Percy"
      },
      {
        "family": "Dean",
        "given": "Jeff"
      },
      {
        "family": "Fedus",
        "given": "William"
      }
    ],
    "container-title": "ArXiv",
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "volume": "abs/2206.07682",
    "URL": "https://api.semanticscholar.org/CorpusID:249674500",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Wei2022EmergentAO"
  },
  {
    "id": "ijdAm3bf",
    "type": "article-journal",
    "title": "Mixture of LoRA Experts",
    "author": [
      {
        "family": "Wu",
        "given": "Xun"
      },
      {
        "family": "Huang",
        "given": "Shaohan"
      },
      {
        "family": "Wei",
        "given": "Furu"
      }
    ],
    "container-title": "ArXiv",
    "issued": {
      "date-parts": [
        [
          2024
        ]
      ]
    },
    "volume": "abs/2404.13628",
    "URL": "https://api.semanticscholar.org/CorpusID:269293160",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Wu2024MixtureOL"
  },
  {
    "id": "KTwv72iC",
    "type": "article-journal",
    "title": "An Explanation of In-context Learning as Implicit Bayesian Inference",
    "author": [
      {
        "family": "Xie",
        "given": "Sang Michael"
      },
      {
        "family": "Raghunathan",
        "given": "Aditi"
      },
      {
        "family": "Liang",
        "given": "Percy"
      },
      {
        "family": "Ma",
        "given": "Tengyu"
      }
    ],
    "container-title": "ArXiv",
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "volume": "abs/2111.02080",
    "URL": "https://api.semanticscholar.org/CorpusID:241035330",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Xie2021AnEO"
  },
  {
    "id": "SXfDe3KU",
    "type": "paper-conference",
    "title": "GAIN: Missing Data Imputation using Generative Adversarial Nets",
    "author": [
      {
        "family": "Yoon",
        "given": "Jinsung"
      },
      {
        "family": "Jordon",
        "given": "James"
      },
      {
        "family": "van der Schaar",
        "given": "Mihaela"
      }
    ],
    "container-title": "Proceedings of the 35th International Conference on Machine Learning",
    "publisher": "PMLR",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "volume": "80",
    "URL": "https://proceedings.mlr.press/v80/yoon18a.html",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Yoon2018GAIN"
  },
  {
    "id": "h3Uh8wYi",
    "type": "article-journal",
    "author": [
      {
        "family": "Yuan",
        "given": "Ming"
      },
      {
        "family": "Lin",
        "given": "Yi"
      }
    ],
    "title": "Model selection and estimation in regression with grouped variables",
    "container-title": "Journal of the Royal Statistical Society: Series B (Statistical Methodology)",
    "volume": "68",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Yuan2006ModelSA"
  },
  {
    "id": "dlBtVL5G",
    "type": "article-journal",
    "title": "A tree-based varying coefficient model",
    "author": [
      {
        "family": "Zakrisson",
        "given": "Henning"
      },
      {
        "family": "Lindholm",
        "given": "Mathias"
      }
    ],
    "container-title": "arXiv",
    "issued": {
      "date-parts": [
        [
          2024
        ]
      ]
    },
    "volume": "abs/2401.05982",
    "URL": "https://api.semanticscholar.org/CorpusID:266933390",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Zakrisson2024ATV"
  },
  {
    "id": "El7MAmHp",
    "type": "article-journal",
    "title": "Tree Boosted Varying Coefficient Models",
    "author": [
      {
        "family": "Zhou",
        "given": "Yichen"
      },
      {
        "family": "Hooker",
        "given": "Giles"
      }
    ],
    "container-title": "arXiv: Methodology",
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "URL": "https://api.semanticscholar.org/CorpusID:91184310",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Zhou2019TreeBV"
  },
  {
    "id": "195IMYRMU",
    "type": "paper-conference",
    "title": "Domain Adaptation under Missingness Shift",
    "author": [
      {
        "family": "Zhou",
        "given": "Helen"
      },
      {
        "family": "Balakrishnan",
        "given": "Sivaraman"
      },
      {
        "family": "Lipton",
        "given": "Zachary C."
      }
    ],
    "container-title": "Proceedings of the 26th International Conference on Artificial Intelligence and Statistics",
    "publisher": "PMLR",
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "volume": "206",
    "URL": "https://proceedings.mlr.press/v206/zhou23b.html",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Zhou2023DAMS"
  },
  {
    "id": "1GChRhAEq",
    "type": "article-journal",
    "author": [
      {
        "family": "Ang",
        "given": "Andrew"
      },
      {
        "family": "Bekaert",
        "given": "Geert"
      }
    ],
    "title": "Asset Allocation with Regime Shifts and Long-Horizon Risks",
    "container-title": "Review of Financial Studies",
    "volume": "27",
    "issue": "12",
    "page": "3734--3777",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: ang2014asset"
  },
  {
    "id": "JhS4KMR7",
    "URL": "https://arxiv.org/abs/1501.01332",
    "number": "1501.01332",
    "title": "Causal inference using invariant prediction: identification and confidence intervals",
    "issued": {
      "date-parts": [
        [
          2024,
          4,
          26
        ]
      ]
    },
    "author": [
      {
        "given": "Jonas",
        "family": "Peters"
      },
      {
        "given": "Peter",
        "family": "BÃÂ¼hlmann"
      },
      {
        "given": "Nicolai",
        "family": "Meinshausen"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "What is the difference of a prediction that is made with a causal model and a non-causal model? Suppose we intervene on the predictor variables or change the whole environment. The predictions from a causal model will in general work as well under interventions as for observational data. In contrast, predictions from a non-causal model can potentially be very wrong if we actively intervene on variables. Here, we propose to exploit this invariance of a prediction under a causal model for causal inference: given different experimental settings (for example various interventions) we collect all models that do show invariance in their predictive accuracy across settings and interventions. The causal model will be a member of this set of models with high probability. This approach yields valid confidence intervals for the causal relationships in quite general scenarios. We examine the example of structural equation models in more detail and provide sufficient assumptions under which the set of causal predictors becomes identifiable. We further investigate robustness properties of our approach under model misspecification and discuss possible extensions. The empirical properties are studied for various data sets, including large-scale gene perturbation experiments.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:1501.01332"
  },
  {
    "id": "1HqcEkGab",
    "URL": "https://arxiv.org/abs/1702.08734",
    "number": "1702.08734",
    "title": "Billion-scale similarity search with GPUs",
    "issued": {
      "date-parts": [
        [
          2018,
          6,
          7
        ]
      ]
    },
    "author": [
      {
        "given": "Jeff",
        "family": "Johnson"
      },
      {
        "given": "Matthijs",
        "family": "Douze"
      },
      {
        "given": "HervÃÂ©",
        "family": "JÃÂ©gou"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "Similarity search finds application in specialized database systems handling complex data such as images or videos, which are typically represented by high-dimensional features and require specific indexing structures. This paper tackles the problem of better utilizing GPUs for this task. While GPUs excel at data-parallel tasks, prior approaches are bottlenecked by algorithms that expose less parallelism, such as k-min selection, or make poor use of the memory hierarchy.\n  We propose a design for k-selection that operates at up to 55% of theoretical peak performance, enabling a nearest neighbor implementation that is 8.5x faster than prior GPU state of the art. We apply it in different similarity search scenarios, by proposing optimized design for brute-force, approximate and compressed-domain search based on product quantization. In all these setups, we outperform the state of the art by large margins. Our implementation enables the construction of a high accuracy k-NN graph on 95 million images from the Yfcc100M dataset in 35 minutes, and of a graph connecting 1 billion vectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced our approach for the sake of comparison and reproducibility.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:1702.08734"
  },
  {
    "id": "B0L8KJ8W",
    "URL": "https://arxiv.org/abs/1706.06083",
    "number": "1706.06083",
    "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
    "issued": {
      "date-parts": [
        [
          2019,
          9,
          6
        ]
      ]
    },
    "author": [
      {
        "given": "Aleksander",
        "family": "Madry"
      },
      {
        "given": "Aleksandar",
        "family": "Makelov"
      },
      {
        "given": "Ludwig",
        "family": "Schmidt"
      },
      {
        "given": "Dimitris",
        "family": "Tsipras"
      },
      {
        "given": "Adrian",
        "family": "Vladu"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/MadryLab/mnist_challenge and https://github.com/MadryLab/cifar10_challenge.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:1706.06083"
  },
  {
    "id": "L6xa6qzg",
    "URL": "https://arxiv.org/abs/1710.11469",
    "number": "1710.11469",
    "title": "Conditional Variance Penalties and Domain Shift Robustness",
    "issued": {
      "date-parts": [
        [
          2019,
          4,
          16
        ]
      ]
    },
    "author": [
      {
        "given": "Christina",
        "family": "Heinze-Deml"
      },
      {
        "given": "Nicolai",
        "family": "Meinshausen"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "When training a deep neural network for image classification, one can broadly distinguish between two types of latent features of images that will drive the classification. We can divide latent features into (i) \"core\" or \"conditionally invariant\" features $X^\\text{core}$ whose distribution $X^\\text{core}\\vert Y$, conditional on the class $Y$, does not change substantially across domains and (ii) \"style\" features $X^{\\text{style}}$ whose distribution $X^{\\text{style}} \\vert Y$ can change substantially across domains. Examples for style features include position, rotation, image quality or brightness but also more complex ones like hair color, image quality or posture for images of persons. Our goal is to minimize a loss that is robust under changes in the distribution of these style features. In contrast to previous work, we assume that the domain itself is not observed and hence a latent variable.\n  We do assume that we can sometimes observe a typically discrete identifier or \"$\\mathrm{ID}$ variable\". In some applications we know, for example, that two images show the same person, and $\\mathrm{ID}$ then refers to the identity of the person. The proposed method requires only a small fraction of images to have $\\mathrm{ID}$ information. We group observations if they share the same class and identifier $(Y,\\mathrm{ID})=(y,\\mathrm{id})$ and penalize the conditional variance of the prediction or the loss if we condition on $(Y,\\mathrm{ID})$. Using a causal framework, this conditional variance regularization (CoRe) is shown to protect asymptotically against shifts in the distribution of the style variables. Empirically, we show that the CoRe penalty improves predictive accuracy substantially in settings where domain changes occur in terms of image quality, brightness and color while we also look at more complex changes such as changes in movement and posture.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:1710.11469"
  },
  {
    "id": "ylSNfYug",
    "URL": "https://arxiv.org/abs/1805.12152",
    "number": "1805.12152",
    "title": "Robustness May Be at Odds with Accuracy",
    "issued": {
      "date-parts": [
        [
          2019,
          9,
          10
        ]
      ]
    },
    "author": [
      {
        "given": "Dimitris",
        "family": "Tsipras"
      },
      {
        "given": "Shibani",
        "family": "Santurkar"
      },
      {
        "given": "Logan",
        "family": "Engstrom"
      },
      {
        "given": "Alexander",
        "family": "Turner"
      },
      {
        "given": "Aleksander",
        "family": "Madry"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "We show that there may exist an inherent tension between the goal of adversarial robustness and that of standard generalization. Specifically, training robust models may not only be more resource-consuming, but also lead to a reduction of standard accuracy. We demonstrate that this trade-off between the standard accuracy of a model and its robustness to adversarial perturbations provably exists in a fairly simple and natural setting. These findings also corroborate a similar phenomenon observed empirically in more complex settings. Further, we argue that this phenomenon is a consequence of robust classifiers learning fundamentally different feature representations than standard classifiers. These differences, in particular, seem to result in unexpected benefits: the representations learned by robust models tend to align better with salient data characteristics and human perception.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:1805.12152"
  },
  {
    "id": "l5C1P3il",
    "URL": "https://arxiv.org/abs/1905.02175",
    "number": "1905.02175",
    "title": "Adversarial Examples Are Not Bugs, They Are Features",
    "issued": {
      "date-parts": [
        [
          2019,
          8,
          13
        ]
      ]
    },
    "author": [
      {
        "given": "Andrew",
        "family": "Ilyas"
      },
      {
        "given": "Shibani",
        "family": "Santurkar"
      },
      {
        "given": "Dimitris",
        "family": "Tsipras"
      },
      {
        "given": "Logan",
        "family": "Engstrom"
      },
      {
        "given": "Brandon",
        "family": "Tran"
      },
      {
        "given": "Aleksander",
        "family": "Madry"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "Adversarial examples have attracted significant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. We demonstrate that adversarial examples can be directly attributed to the presence of non-robust features: features derived from patterns in the data distribution that are highly predictive, yet brittle and incomprehensible to humans. After capturing these features within a theoretical framework, we establish their widespread existence in standard datasets. Finally, we present a simple setting where we can rigorously tie the phenomena we observe in practice to a misalignment between the (human-specified) notion of robustness and the inherent geometry of the data.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:1905.02175"
  },
  {
    "id": "1DeCFT6PA",
    "URL": "https://arxiv.org/abs/1907.02893",
    "number": "1907.02893",
    "title": "Invariant Risk Minimization",
    "issued": {
      "date-parts": [
        [
          2020,
          3,
          31
        ]
      ]
    },
    "author": [
      {
        "given": "Martin",
        "family": "Arjovsky"
      },
      {
        "given": "LÃÂ©on",
        "family": "Bottou"
      },
      {
        "given": "Ishaan",
        "family": "Gulrajani"
      },
      {
        "given": "David",
        "family": "Lopez-Paz"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "We introduce Invariant Risk Minimization (IRM), a learning paradigm to estimate invariant correlations across multiple training distributions. To achieve this goal, IRM learns a data representation such that the optimal classifier, on top of that data representation, matches for all training distributions. Through theory and experiments, we show how the invariances learned by IRM relate to the causal structures governing the data and enable out-of-distribution generalization.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:1907.02893"
  },
  {
    "id": "Jm8Kx8HW",
    "URL": "https://arxiv.org/abs/1911.08731",
    "number": "1911.08731",
    "title": "Distributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Case Generalization",
    "issued": {
      "date-parts": [
        [
          2020,
          4,
          3
        ]
      ]
    },
    "author": [
      {
        "given": "Shiori",
        "family": "Sagawa"
      },
      {
        "given": "Pang Wei",
        "family": "Koh"
      },
      {
        "given": "Tatsunori B.",
        "family": "Hashimoto"
      },
      {
        "given": "Percy",
        "family": "Liang"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "Overparameterized neural networks can be highly accurate on average on an i.i.d. test set yet consistently fail on atypical groups of the data (e.g., by learning spurious correlations that hold on average but not in such groups). Distributionally robust optimization (DRO) allows us to learn models that instead minimize the worst-case training loss over a set of pre-defined groups. However, we find that naively applying group DRO to overparameterized neural networks fails: these models can perfectly fit the training data, and any model with vanishing average training loss also already has vanishing worst-case training loss. Instead, the poor worst-case performance arises from poor generalization on some groups. By coupling group DRO models with increased regularization---a stronger-than-typical L2 penalty or early stopping---we achieve substantially higher worst-group accuracies, with 10-40 percentage point improvements on a natural language inference task and two image tasks, while maintaining high average accuracies. Our results suggest that regularization is important for worst-group generalization in the overparameterized regime, even if it is not needed for average generalization. Finally, we introduce a stochastic optimization algorithm, with convergence guarantees, to efficiently train group DRO models.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:1911.08731"
  },
  {
    "id": "11IMWGprl",
    "URL": "https://arxiv.org/abs/2003.00688",
    "number": "2003.00688",
    "title": "Out-of-Distribution Generalization via Risk Extrapolation (REx)",
    "issued": {
      "date-parts": [
        [
          2021,
          2,
          26
        ]
      ]
    },
    "author": [
      {
        "given": "David",
        "family": "Krueger"
      },
      {
        "given": "Ethan",
        "family": "Caballero"
      },
      {
        "given": "Joern-Henrik",
        "family": "Jacobsen"
      },
      {
        "given": "Amy",
        "family": "Zhang"
      },
      {
        "given": "Jonathan",
        "family": "Binas"
      },
      {
        "given": "Dinghuai",
        "family": "Zhang"
      },
      {
        "given": "Remi Le",
        "family": "Priol"
      },
      {
        "given": "Aaron",
        "family": "Courville"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "Distributional shift is one of the major obstacles when transferring machine learning prediction systems from the lab to the real world. To tackle this problem, we assume that variation across training domains is representative of the variation we might encounter at test time, but also that shifts at test time may be more extreme in magnitude. In particular, we show that reducing differences in risk across training domains can reduce a model's sensitivity to a wide range of extreme distributional shifts, including the challenging setting where the input contains both causal and anti-causal elements. We motivate this approach, Risk Extrapolation (REx), as a form of robust optimization over a perturbation set of extrapolated domains (MM-REx), and propose a penalty on the variance of training risks (V-REx) as a simpler variant. We prove that variants of REx can recover the causal mechanisms of the targets, while also providing some robustness to changes in the input distribution (\"covariate shift\"). By appropriately trading-off robustness to causally induced distributional shifts and covariate shift, REx is able to outperform alternative methods such as Invariant Risk Minimization in situations where these types of shift co-occur.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2003.00688"
  },
  {
    "id": "1FLMzrLE9",
    "URL": "https://arxiv.org/abs/2010.05761",
    "number": "2010.05761",
    "title": "The Risks of Invariant Risk Minimization",
    "issued": {
      "date-parts": [
        [
          2021,
          3,
          30
        ]
      ]
    },
    "author": [
      {
        "given": "Elan",
        "family": "Rosenfeld"
      },
      {
        "given": "Pradeep",
        "family": "Ravikumar"
      },
      {
        "given": "Andrej",
        "family": "Risteski"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "Invariant Causal Prediction (Peters et al., 2016) is a technique for out-of-distribution generalization which assumes that some aspects of the data distribution vary across the training set but that the underlying causal mechanisms remain constant. Recently, Arjovsky et al. (2019) proposed Invariant Risk Minimization (IRM), an objective based on this idea for learning deep, invariant features of data which are a complex function of latent variables; many alternatives have subsequently been suggested. However, formal guarantees for all of these works are severely lacking. In this paper, we present the first analysis of classification under the IRM objective--as well as these recently proposed alternatives--under a fairly natural and general model. In the linear case, we show simple conditions under which the optimal solution succeeds or, more often, fails to recover the optimal invariant predictor. We furthermore present the very first results in the non-linear regime: we demonstrate that IRM can fail catastrophically unless the test data are sufficiently similar to the training distribution--this is precisely the issue that it was intended to solve. Thus, in this setting we find that IRM and its alternatives fundamentally do not improve over standard Empirical Risk Minimization.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2010.05761"
  },
  {
    "id": "10151coVE",
    "URL": "https://arxiv.org/abs/2010.07249",
    "number": "2010.07249",
    "title": "Environment Inference for Invariant Learning",
    "issued": {
      "date-parts": [
        [
          2021,
          7,
          16
        ]
      ]
    },
    "author": [
      {
        "given": "Elliot",
        "family": "Creager"
      },
      {
        "given": "JÃÂ¶rn-Henrik",
        "family": "Jacobsen"
      },
      {
        "given": "Richard",
        "family": "Zemel"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "Learning models that gracefully handle distribution shifts is central to research on domain generalization, robust optimization, and fairness. A promising formulation is domain-invariant learning, which identifies the key issue of learning which features are domain-specific versus domain-invariant. An important assumption in this area is that the training examples are partitioned into \"domains\" or \"environments\". Our focus is on the more common setting where such partitions are not provided. We propose EIIL, a general framework for domain-invariant learning that incorporates Environment Inference to directly infer partitions that are maximally informative for downstream Invariant Learning. We show that EIIL outperforms invariant learning methods on the CMNIST benchmark without using environment labels, and significantly outperforms ERM on worst-group performance in the Waterbirds and CivilComments datasets. Finally, we establish connections between EIIL and algorithmic fairness, which enables EIIL to improve accuracy and calibration in a fair prediction problem.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2010.07249"
  },
  {
    "id": "1CkxORTSX",
    "URL": "https://arxiv.org/abs/2103.00315",
    "number": "2103.00315",
    "title": "Time-Varying Coefficient Model Estimation Through Radial Basis Functions",
    "issued": {
      "date-parts": [
        [
          2021,
          3,
          2
        ]
      ]
    },
    "author": [
      {
        "given": "Juan",
        "family": "Sosa"
      },
      {
        "given": "Lina",
        "family": "Buitrago"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "In this paper we estimate the dynamic parameters of a time-varying coefficient model through radial kernel functions in the context of a longitudinal study. Our proposal is based on a linear combination of weighted kernel functions involving a bandwidth, centered around a given set of time points. In addition, we study different alternatives of estimation and inference including a Frequentist approach using weighted least squares along with bootstrap methods, and a Bayesian approach through both Markov chain Monte Carlo and variational methods. We compare the estimation strategies mention above with each other, and our radial kernel functions proposal with an expansion based on regression spline, by means of an extensive simulation study considering multiples scenarios in terms of sample size, number of repeated measurements, and subject-specific correlation. Our experiments show that the capabilities of our proposal based on radial kernel functions are indeed comparable with or even better than those obtained from regression splines. We illustrate our methodology by analyzing data from two AIDS clinical studies.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2103.00315"
  },
  {
    "id": "HzzgJQN0",
    "URL": "https://arxiv.org/abs/2106.04486",
    "number": "2106.04486",
    "title": "Sketch-Based Anomaly Detection in Streaming Graphs",
    "issued": {
      "date-parts": [
        [
          2023,
          7,
          18
        ]
      ]
    },
    "author": [
      {
        "given": "Siddharth",
        "family": "Bhatia"
      },
      {
        "given": "Mohit",
        "family": "Wadhwa"
      },
      {
        "given": "Kenji",
        "family": "Kawaguchi"
      },
      {
        "given": "Neil",
        "family": "Shah"
      },
      {
        "given": "Philip S.",
        "family": "Yu"
      },
      {
        "given": "Bryan",
        "family": "Hooi"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "Given a stream of graph edges from a dynamic graph, how can we assign anomaly scores to edges and subgraphs in an online manner, for the purpose of detecting unusual behavior, using constant time and memory? For example, in intrusion detection, existing work seeks to detect either anomalous edges or anomalous subgraphs, but not both. In this paper, we first extend the count-min sketch data structure to a higher-order sketch. This higher-order sketch has the useful property of preserving the dense subgraph structure (dense subgraphs in the input turn into dense submatrices in the data structure). We then propose 4 online algorithms that utilize this enhanced data structure, which (a) detect both edge and graph anomalies; (b) process each edge and graph in constant memory and constant update time per newly arriving edge, and; (c) outperform state-of-the-art baselines on 4 real-world datasets. Our method is the first streaming approach that incorporates dense subgraph search to detect graph anomalies in constant memory and time.",
    "note": "license: http://creativecommons.org/licenses/by/4.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2106.04486"
  },
  {
    "id": "EXaywYVO",
    "URL": "https://arxiv.org/abs/2107.09044",
    "number": "2107.09044",
    "title": "Just Train Twice: Improving Group Robustness without Training Group Information",
    "issued": {
      "date-parts": [
        [
          2021,
          9,
          28
        ]
      ]
    },
    "author": [
      {
        "given": "Evan Zheran",
        "family": "Liu"
      },
      {
        "given": "Behzad",
        "family": "Haghgoo"
      },
      {
        "given": "Annie S.",
        "family": "Chen"
      },
      {
        "given": "Aditi",
        "family": "Raghunathan"
      },
      {
        "given": "Pang Wei",
        "family": "Koh"
      },
      {
        "given": "Shiori",
        "family": "Sagawa"
      },
      {
        "given": "Percy",
        "family": "Liang"
      },
      {
        "given": "Chelsea",
        "family": "Finn"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "Standard training via empirical risk minimization (ERM) can produce models that achieve high accuracy on average but low accuracy on certain groups, especially in the presence of spurious correlations between the input and label. Prior approaches that achieve high worst-group accuracy, like group distributionally robust optimization (group DRO) require expensive group annotations for each training point, whereas approaches that do not use such group annotations typically achieve unsatisfactory worst-group accuracy. In this paper, we propose a simple two-stage approach, JTT, that first trains a standard ERM model for several epochs, and then trains a second model that upweights the training examples that the first model misclassified. Intuitively, this upweights examples from groups on which standard ERM models perform poorly, leading to improved worst-group performance. Averaged over four image classification and natural language processing tasks with spurious correlations, JTT closes 75% of the gap in worst-group accuracy between standard ERM and group DRO, while only requiring group annotations on a small validation set in order to tune hyperparameters.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2107.09044"
  },
  {
    "id": "cHqJJQHt",
    "URL": "https://arxiv.org/abs/2212.03471",
    "number": "2212.03471",
    "title": "Bayesian Forecasting in Economics and Finance: A Modern Review",
    "issued": {
      "date-parts": [
        [
          2023,
          8,
          1
        ]
      ]
    },
    "author": [
      {
        "given": "Gael M.",
        "family": "Martin"
      },
      {
        "given": "David T.",
        "family": "Frazier"
      },
      {
        "given": "Worapree",
        "family": "Maneesoonthorn"
      },
      {
        "given": "Ruben",
        "family": "Loaiza-Maya"
      },
      {
        "given": "Florian",
        "family": "Huber"
      },
      {
        "given": "Gary",
        "family": "Koop"
      },
      {
        "given": "John",
        "family": "Maheu"
      },
      {
        "given": "Didier",
        "family": "Nibbering"
      },
      {
        "given": "Anastasios",
        "family": "Panagiotelis"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "The Bayesian statistical paradigm provides a principled and coherent approach to probabilistic forecasting. Uncertainty about all unknowns that characterize any forecasting problem -- model, parameters, latent states -- is able to be quantified explicitly, and factored into the forecast distribution via the process of integration or averaging. Allied with the elegance of the method, Bayesian forecasting is now underpinned by the burgeoning field of Bayesian computation, which enables Bayesian forecasts to be produced for virtually any problem, no matter how large, or complex. The current state of play in Bayesian forecasting in economics and finance is the subject of this review. The aim is to provide the reader with an overview of modern approaches to the field, set in some historical context; and with sufficient computational detail given to assist the reader with implementation.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2212.03471"
  },
  {
    "id": "xoOsuDeH",
    "URL": "https://arxiv.org/abs/2303.02781v1",
    "number": "2303.02781v1",
    "version": "v1",
    "title": "Robustness, Evaluation and Adaptation of Machine Learning Models in the\n  Wild",
    "issued": {
      "date-parts": [
        [
          2023,
          3,
          5
        ]
      ]
    },
    "author": [
      {
        "literal": "Vihari Piratla"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "Our goal is to improve reliability of Machine Learning (ML) systems deployed in the wild. ML models perform exceedingly well when test examples are similar to train examples. However, real-world applications are required to perform on any distribution of test examples. Current ML systems can fail silently on test examples with distribution shifts. In order to improve reliability of ML models due to covariate or domain shift, we propose algorithms that enable models to: (a) generalize to a larger family of test distributions, (b) evaluate accuracy under distribution shifts, (c) adapt to a target distribution. We study causes of impaired robustness to domain shifts and present algorithms for training domain robust models. A key source of model brittleness is due to domain overfitting, which our new training algorithms suppress and instead encourage domain-general hypotheses. While we improve robustness over standard training methods for certain problem settings, performance of ML systems can still vary drastically with domain shifts. It is crucial for developers and stakeholders to understand model vulnerabilities and operational ranges of input, which could be assessed on the fly during the deployment, albeit at a great cost. Instead, we advocate for proactively estimating accuracy surfaces over any combination of prespecified and interpretable domain shifts for performance forecasting. We present a label-efficient estimation to address estimation over a combinatorial space of domain shifts. Further, when a model's performance on a target domain is found to be poor, traditional approaches adapt the model using the target domain's resources. Standard adaptation methods assume access to sufficient labeled resources, which may be impractical for deployed models. We initiate a study of lightweight adaptation techniques with only unlabeled data resources with a focus on language applications.",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2303.02781v1"
  },
  {
    "id": "PF464FGD",
    "URL": "https://arxiv.org/abs/2309.06180",
    "number": "2309.06180",
    "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention",
    "issued": {
      "date-parts": [
        [
          2023,
          9,
          13
        ]
      ]
    },
    "author": [
      {
        "given": "Woosuk",
        "family": "Kwon"
      },
      {
        "given": "Zhuohan",
        "family": "Li"
      },
      {
        "given": "Siyuan",
        "family": "Zhuang"
      },
      {
        "given": "Ying",
        "family": "Sheng"
      },
      {
        "given": "Lianmin",
        "family": "Zheng"
      },
      {
        "given": "Cody Hao",
        "family": "Yu"
      },
      {
        "given": "Joseph E.",
        "family": "Gonzalez"
      },
      {
        "given": "Hao",
        "family": "Zhang"
      },
      {
        "given": "Ion",
        "family": "Stoica"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4$\\times$ with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm",
    "note": "license: http://creativecommons.org/licenses/by/4.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2309.06180"
  },
  {
    "id": "CYfSdHYg",
    "URL": "https://arxiv.org/abs/2309.07864",
    "number": "2309.07864",
    "title": "The Rise and Potential of Large Language Model Based Agents: A Survey",
    "issued": {
      "date-parts": [
        [
          2023,
          9,
          20
        ]
      ]
    },
    "author": [
      {
        "given": "Zhiheng",
        "family": "Xi"
      },
      {
        "given": "Wenxiang",
        "family": "Chen"
      },
      {
        "given": "Xin",
        "family": "Guo"
      },
      {
        "given": "Wei",
        "family": "He"
      },
      {
        "given": "Yiwen",
        "family": "Ding"
      },
      {
        "given": "Boyang",
        "family": "Hong"
      },
      {
        "given": "Ming",
        "family": "Zhang"
      },
      {
        "given": "Junzhe",
        "family": "Wang"
      },
      {
        "given": "Senjie",
        "family": "Jin"
      },
      {
        "given": "Enyu",
        "family": "Zhou"
      },
      {
        "given": "Rui",
        "family": "Zheng"
      },
      {
        "given": "Xiaoran",
        "family": "Fan"
      },
      {
        "given": "Xiao",
        "family": "Wang"
      },
      {
        "given": "Limao",
        "family": "Xiong"
      },
      {
        "given": "Yuhao",
        "family": "Zhou"
      },
      {
        "given": "Weiran",
        "family": "Wang"
      },
      {
        "given": "Changhao",
        "family": "Jiang"
      },
      {
        "given": "Yicheng",
        "family": "Zou"
      },
      {
        "given": "Xiangyang",
        "family": "Liu"
      },
      {
        "given": "Zhangyue",
        "family": "Yin"
      },
      {
        "given": "Shihan",
        "family": "Dou"
      },
      {
        "given": "Rongxiang",
        "family": "Weng"
      },
      {
        "given": "Wensen",
        "family": "Cheng"
      },
      {
        "given": "Qi",
        "family": "Zhang"
      },
      {
        "given": "Wenjuan",
        "family": "Qin"
      },
      {
        "given": "Yongyan",
        "family": "Zheng"
      },
      {
        "given": "Xipeng",
        "family": "Qiu"
      },
      {
        "given": "Xuanjing",
        "family": "Huang"
      },
      {
        "given": "Tao",
        "family": "Gui"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level, with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. Many efforts have been made to develop intelligent agents, but they mainly focus on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually, what the community lacks is a general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios. Due to the versatile capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many researchers have leveraged LLMs as the foundation to build AI agents and have achieved significant progress. In this paper, we perform a comprehensive survey on LLM-based agents. We start by tracing the concept of agents from its philosophical origins to its development in AI, and explain why LLMs are suitable foundations for agents. Building upon this, we present a general framework for LLM-based agents, comprising three main components: brain, perception, and action, and the framework can be tailored for different applications. Subsequently, we explore the extensive applications of LLM-based agents in three aspects: single-agent scenarios, multi-agent scenarios, and human-agent cooperation. Following this, we delve into agent societies, exploring the behavior and personality of LLM-based agents, the social phenomena that emerge from an agent society, and the insights they offer for human society. Finally, we discuss several key topics and open problems within the field. A repository for the related papers at https://github.com/WooooDyy/LLM-Agent-Paper-List.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2309.07864"
  },
  {
    "id": "D4deReQm",
    "URL": "https://arxiv.org/abs/2309.17453",
    "number": "2309.17453",
    "title": "Efficient Streaming Language Models with Attention Sinks",
    "issued": {
      "date-parts": [
        [
          2024,
          4,
          9
        ]
      ]
    },
    "author": [
      {
        "given": "Guangxuan",
        "family": "Xiao"
      },
      {
        "given": "Yuandong",
        "family": "Tian"
      },
      {
        "given": "Beidi",
        "family": "Chen"
      },
      {
        "given": "Song",
        "family": "Han"
      },
      {
        "given": "Mike",
        "family": "Lewis"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a \"sink\" even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.",
    "note": "license: http://creativecommons.org/licenses/by/4.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2309.17453"
  },
  {
    "id": "1BPbcGQGq",
    "URL": "https://arxiv.org/abs/2402.19473",
    "number": "2402.19473",
    "title": "Retrieval-Augmented Generation for AI-Generated Content: A Survey",
    "issued": {
      "date-parts": [
        [
          2024,
          6,
          24
        ]
      ]
    },
    "author": [
      {
        "given": "Penghao",
        "family": "Zhao"
      },
      {
        "given": "Hailin",
        "family": "Zhang"
      },
      {
        "given": "Qinhan",
        "family": "Yu"
      },
      {
        "given": "Zhengren",
        "family": "Wang"
      },
      {
        "given": "Yunteng",
        "family": "Geng"
      },
      {
        "given": "Fangcheng",
        "family": "Fu"
      },
      {
        "given": "Ling",
        "family": "Yang"
      },
      {
        "given": "Wentao",
        "family": "Zhang"
      },
      {
        "given": "Jie",
        "family": "Jiang"
      },
      {
        "given": "Bin",
        "family": "Cui"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "Advancements in model algorithms, the growth of foundational models, and access to high-quality datasets have propelled the evolution of Artificial Intelligence Generated Content (AIGC). Despite its notable successes, AIGC still faces hurdles such as updating knowledge, handling long-tail data, mitigating data leakage, and managing high training and inference costs. Retrieval-Augmented Generation (RAG) has recently emerged as a paradigm to address such challenges. In particular, RAG introduces the information retrieval process, which enhances the generation process by retrieving relevant objects from available data stores, leading to higher accuracy and better robustness. In this paper, we comprehensively review existing efforts that integrate RAG technique into AIGC scenarios. We first classify RAG foundations according to how the retriever augments the generator, distilling the fundamental abstractions of the augmentation methodologies for various retrievers and generators. This unified perspective encompasses all RAG scenarios, illuminating advancements and pivotal technologies that help with potential future progress. We also summarize additional enhancements methods for RAG, facilitating effective engineering and implementation of RAG systems. Then from another view, we survey on practical applications of RAG across different modalities and tasks, offering valuable references for researchers and practitioners. Furthermore, we introduce the benchmarks for RAG, discuss the limitations of current RAG systems, and suggest potential directions for future research. Github: https://github.com/PKU-DAIR/RAG-Survey.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2402.19473"
  },
  {
    "id": "gfPki3PM",
    "URL": "https://arxiv.org/abs/2406.12274",
    "number": "2406.12274",
    "title": "SafeInfer: Context Adaptive Decoding Time Safety Alignment for Large Language Models",
    "issued": {
      "date-parts": [
        [
          2024,
          12,
          17
        ]
      ]
    },
    "author": [
      {
        "given": "Somnath",
        "family": "Banerjee"
      },
      {
        "given": "Sayan",
        "family": "Layek"
      },
      {
        "given": "Soham",
        "family": "Tripathy"
      },
      {
        "given": "Shanu",
        "family": "Kumar"
      },
      {
        "given": "Animesh",
        "family": "Mukherjee"
      },
      {
        "given": "Rima",
        "family": "Hazra"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "Safety-aligned language models often exhibit fragile and imbalanced safety mechanisms, increasing the likelihood of generating unsafe content. In addition, incorporating new knowledge through editing techniques to language models can further compromise safety. To address these issues, we propose SafeInfer, a context-adaptive, decoding-time safety alignment strategy for generating safe responses to user queries. SafeInfer comprises two phases: the safety amplification phase, which employs safe demonstration examples to adjust the model's hidden states and increase the likelihood of safer outputs, and the safety-guided decoding phase, which influences token selection based on safety-optimized distributions, ensuring the generated content complies with ethical guidelines. Further, we present HarmEval, a novel benchmark for extensive safety evaluations, designed to address potential misuse scenarios in accordance with the policies of leading AI tech giants.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2406.12274"
  },
  {
    "id": "RkqVE8TP",
    "URL": "https://arxiv.org/abs/2409.08354",
    "number": "2409.08354",
    "title": "Bayesian Dynamic Factor Models for High-dimensional Matrix-valued Time Series",
    "issued": {
      "date-parts": [
        [
          2025,
          8,
          11
        ]
      ]
    },
    "author": [
      {
        "given": "Wei",
        "family": "Zhang"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "We introduce a class of Bayesian matrix dynamic factor models that accommodates time-varying volatility, outliers, and cross-sectional correlation in the idiosyncratic components. For model comparison, we employ an importance-sampling estimator of the marginal likelihood based on the cross-entropy method to determine: (1) the optimal dimension of the factor matrix; (2) whether a vector- or matrix-valued structure is more suitable; and (3) whether an approximate or exact factor model is favored by the data. Through a series of Monte Carlo experiments, we demonstrate the accuracy of the factor estimates and the effectiveness of the marginal likelihood estimator in correctly identifying the true model. Applications to macroeconomic and financial datasets illustrate the model's ability to capture key features in matrix-valued time series.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2409.08354"
  },
  {
    "id": "yApXu0Vp",
    "URL": "https://arxiv.org/abs/2410.12837",
    "number": "2410.12837",
    "title": "A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions",
    "issued": {
      "date-parts": [
        [
          2024,
          10,
          18
        ]
      ]
    },
    "author": [
      {
        "given": "Shailja",
        "family": "Gupta"
      },
      {
        "given": "Rajesh",
        "family": "Ranjan"
      },
      {
        "given": "Surya Narayan",
        "family": "Singh"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "This paper presents a comprehensive study of Retrieval-Augmented Generation (RAG), tracing its evolution from foundational concepts to the current state of the art. RAG combines retrieval mechanisms with generative language models to enhance the accuracy of outputs, addressing key limitations of LLMs. The study explores the basic architecture of RAG, focusing on how retrieval and generation are integrated to handle knowledge-intensive tasks. A detailed review of the significant technological advancements in RAG is provided, including key innovations in retrieval-augmented language models and applications across various domains such as question-answering, summarization, and knowledge-based tasks. Recent research breakthroughs are discussed, highlighting novel methods for improving retrieval efficiency. Furthermore, the paper examines ongoing challenges such as scalability, bias, and ethical concerns in deployment. Future research directions are proposed, focusing on improving the robustness of RAG models, expanding the scope of application of RAG models, and addressing societal implications. This survey aims to serve as a foundational resource for researchers and practitioners in understanding the potential of RAG and its trajectory in natural language processing.",
    "note": "license: http://creativecommons.org/licenses/by-nc-nd/4.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2410.12837"
  },
  {
    "id": "EsknayJ2",
    "URL": "https://arxiv.org/abs/2504.15965",
    "number": "2504.15965",
    "title": "From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs",
    "issued": {
      "date-parts": [
        [
          2025,
          4,
          24
        ]
      ]
    },
    "author": [
      {
        "given": "Yaxiong",
        "family": "Wu"
      },
      {
        "given": "Sheng",
        "family": "Liang"
      },
      {
        "given": "Chen",
        "family": "Zhang"
      },
      {
        "given": "Yichao",
        "family": "Wang"
      },
      {
        "given": "Yongyue",
        "family": "Zhang"
      },
      {
        "given": "Huifeng",
        "family": "Guo"
      },
      {
        "given": "Ruiming",
        "family": "Tang"
      },
      {
        "given": "Yong",
        "family": "Liu"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "Memory is the process of encoding, storing, and retrieving information, allowing humans to retain experiences, knowledge, skills, and facts over time, and serving as the foundation for growth and effective interaction with the world. It plays a crucial role in shaping our identity, making decisions, learning from past experiences, building relationships, and adapting to changes. In the era of large language models (LLMs), memory refers to the ability of an AI system to retain, recall, and use information from past interactions to improve future responses and interactions. Although previous research and reviews have provided detailed descriptions of memory mechanisms, there is still a lack of a systematic review that summarizes and analyzes the relationship between the memory of LLM-driven AI systems and human memory, as well as how we can be inspired by human memory to construct more powerful memory systems. To achieve this, in this paper, we propose a comprehensive survey on the memory of LLM-driven AI systems. In particular, we first conduct a detailed analysis of the categories of human memory and relate them to the memory of AI systems. Second, we systematically organize existing memory-related work and propose a categorization method based on three dimensions (object, form, and time) and eight quadrants. Finally, we illustrate some open problems regarding the memory of current AI systems and outline possible future directions for memory in the era of large language models.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2504.15965"
  },
  {
    "id": "QciOeUHY",
    "URL": "https://arxiv.org/abs/2506.06326",
    "number": "2506.06326",
    "title": "Memory OS of AI Agent",
    "issued": {
      "date-parts": [
        [
          2025,
          6,
          10
        ]
      ]
    },
    "author": [
      {
        "given": "Jiazheng",
        "family": "Kang"
      },
      {
        "given": "Mingming",
        "family": "Ji"
      },
      {
        "given": "Zhe",
        "family": "Zhao"
      },
      {
        "given": "Ting",
        "family": "Bai"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "Large Language Models (LLMs) face a crucial challenge from fixed context windows and inadequate memory management, leading to a severe shortage of long-term memory capabilities and limited personalization in the interactive experience with AI agents. To overcome this challenge, we innovatively propose a Memory Operating System, i.e., MemoryOS, to achieve comprehensive and efficient memory management for AI agents. Inspired by the memory management principles in operating systems, MemoryOS designs a hierarchical storage architecture and consists of four key modules: Memory Storage, Updating, Retrieval, and Generation. Specifically, the architecture comprises three levels of storage units: short-term memory, mid-term memory, and long-term personal memory. Key operations within MemoryOS include dynamic updates between storage units: short-term to mid-term updates follow a dialogue-chain-based FIFO principle, while mid-term to long-term updates use a segmented page organization strategy. Our pioneering MemoryOS enables hierarchical memory integration and dynamic updating. Extensive experiments on the LoCoMo benchmark show an average improvement of 49.11% on F1 and 46.18% on BLEU-1 over the baselines on GPT-4o-mini, showing contextual coherence and personalized memory retention in long conversations. The implementation code is open-sourced at https://github.com/BAI-LAB/MemoryOS.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2506.06326"
  },
  {
    "id": "nTk9nk8h",
    "URL": "https://arxiv.org/abs/2508.10055",
    "number": "2508.10055",
    "title": "Bayesian Models for Joint Selection of Features and Auto-Regressive Lags: Theory and Applications in Environmental and Financial Forecasting",
    "issued": {
      "date-parts": [
        [
          2025,
          8,
          18
        ]
      ]
    },
    "author": [
      {
        "given": "Alokesh",
        "family": "Manna"
      },
      {
        "given": "Sujit K.",
        "family": "Ghosh"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "We develop a Bayesian framework for variable selection in linear regression with autocorrelated errors, accommodating lagged covariates and autoregressive structures. This setting occurs in time series applications where responses depend on contemporaneous or past explanatory variables and persistent stochastic shocks, including financial modeling, hydrological forecasting, and meteorological applications requiring temporal dependency capture. Our methodology uses hierarchical Bayesian models with spike-and-slab priors to simultaneously select relevant covariates and lagged error terms. We propose an efficient two-stage MCMC algorithm separating sampling of variable inclusion indicators and model parameters to address high-dimensional computational challenges. Theoretical analysis establishes posterior selection consistency under mild conditions, even when candidate predictors grow exponentially with sample size, common in modern time series with many potential lagged variables. Through simulations and real applications (groundwater depth prediction, S&P 500 log returns modeling), we demonstrate substantial gains in variable selection accuracy and predictive performance. Compared to existing methods, our framework achieves lower MSPE, improved true model component identification, and greater robustness with autocorrelated noise, underscoring practical utility for model interpretation and forecasting in autoregressive settings.",
    "note": "license: http://creativecommons.org/publicdomain/zero/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2508.10055"
  },
  {
    "id": "1C0w0XTvj",
    "URL": "https://arxiv.org/abs/2509.01794",
    "number": "2509.01794",
    "title": "A Multi-target Bayesian Transformer Framework for Predicting Cardiovascular Disease Biomarkers during Pandemics",
    "issued": {
      "date-parts": [
        [
          2025,
          9,
          3
        ]
      ]
    },
    "author": [
      {
        "given": "Trusting",
        "family": "Inekwe"
      },
      {
        "given": "Emmanuel",
        "family": "Agu"
      },
      {
        "given": "Winnie",
        "family": "Mkandawire"
      },
      {
        "given": "Andres",
        "family": "Colubri"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "The COVID-19 pandemic disrupted healthcare systems worldwide, disproportionately impacting individuals with chronic conditions such as cardiovascular disease (CVD). These disruptions -- through delayed care and behavioral changes, affected key CVD biomarkers, including LDL cholesterol (LDL-C), HbA1c, BMI, and systolic blood pressure (SysBP). Accurate modeling of these changes is crucial for predicting disease progression and guiding preventive care. However, prior work has not addressed multi-target prediction of CVD biomarker from Electronic Health Records (EHRs) using machine learning (ML), while jointly capturing biomarker interdependencies, temporal patterns, and predictive uncertainty. In this paper, we propose MBT-CB, a Multi-target Bayesian Transformer (MBT) with pre-trained BERT-based transformer framework to jointly predict LDL-C, HbA1c, BMI and SysBP CVD biomarkers from EHR data. The model leverages Bayesian Variational Inference to estimate uncertainties, embeddings to capture temporal relationships and a DeepMTR model to capture biomarker inter-relationships. We evaluate MBT-CT on retrospective EHR data from 3,390 CVD patient records (304 unique patients) in Central Massachusetts during the Covid-19 pandemic. MBT-CB outperformed a comprehensive set of baselines including other BERT-based ML models, achieving an MAE of 0.00887, RMSE of 0.0135 and MSE of 0.00027, while effectively capturing data and model uncertainty, patient biomarker inter-relationships, and temporal dynamics via its attention and embedding mechanisms. MBT-CB's superior performance highlights its potential to improve CVD biomarker prediction and support clinical decision-making during pandemics.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2509.01794"
  },
  {
    "id": "138XfoxXm",
    "URL": "https://arxiv.org/abs/2509.08183",
    "number": "2509.08183",
    "title": "Chaotic Bayesian Inference: Strange Attractors as Risk Models for Black Swan Events",
    "issued": {
      "date-parts": [
        [
          2025,
          9,
          11
        ]
      ]
    },
    "author": [
      {
        "given": "Crystal",
        "family": "Rust"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "We introduce a new risk modeling framework where chaotic attractors shape the geometry of Bayesian inference. By combining heavy-tailed priors with Lorenz and Rossler dynamics, the models naturally generate volatility clustering, fat tails, and extreme events. We compare two complementary approaches: Model A, which emphasizes geometric stability, and Model B, which highlights rare bursts using Fibonacci diagnostics. Together, they provide a dual perspective for systemic risk analysis, linking Black Swan theory to practical tools for stress testing and volatility monitoring.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2509.08183"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "2",
    "DOI": "10.1007/s10462-012-9338-y",
    "type": "article-journal",
    "page": "275-293",
    "source": "Crossref",
    "title": "Mixture of experts: a literature survey",
    "volume": "42",
    "author": [
      {
        "given": "Saeed",
        "family": "Masoudnia"
      },
      {
        "given": "Reza",
        "family": "Ebrahimpour"
      }
    ],
    "container-title": "Artificial Intelligence Review",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2012,
          5,
          12
        ]
      ]
    },
    "URL": "https://doi.org/f59sxs",
    "container-title-short": "Artif Intell Rev",
    "id": "g7RE7G0h",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1007/s10462-012-9338-y"
  },
  {
    "publisher": "Elsevier BV",
    "issue": "4",
    "DOI": "10.1016/j.chaos.2008.07.022",
    "type": "article-journal",
    "page": "1764-1772",
    "source": "Crossref",
    "title": "Dynamic effects of increasing heterogeneity in financial markets",
    "volume": "41",
    "author": [
      {
        "given": "Ahmad K.",
        "family": "Naimzada"
      },
      {
        "given": "Giorgio",
        "family": "Ricchiuti"
      }
    ],
    "container-title": "Chaos, Solitons &amp; Fractals",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2009,
          8
        ]
      ]
    },
    "URL": "https://doi.org/bfbqxn",
    "container-title-short": "Chaos, Solitons &amp; Fractals",
    "id": "1HdkTgLul",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1016/j.chaos.2008.07.022"
  },
  {
    "publisher": "Elsevier BV",
    "DOI": "10.1016/j.jbi.2022.104086",
    "type": "article-journal",
    "page": "104086",
    "source": "Crossref",
    "title": "Automated interpretable discovery of heterogeneous treatment effectiveness: A COVID-19 case study",
    "volume": "130",
    "author": [
      {
        "given": "Benjamin J.",
        "family": "Lengerich"
      },
      {
        "given": "Mark E.",
        "family": "Nunnally"
      },
      {
        "given": "Yin",
        "family": "Aphinyanaphongs"
      },
      {
        "given": "Caleb",
        "family": "Ellington"
      },
      {
        "given": "Rich",
        "family": "Caruana"
      }
    ],
    "container-title": "Journal of Biomedical Informatics",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2022,
          6
        ]
      ]
    },
    "URL": "https://doi.org/gt68h5",
    "container-title-short": "Journal of Biomedical Informatics",
    "PMCID": "PMC9055753",
    "PMID": "35504543",
    "id": "esxxcr9l",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1016/j.jbi.2022.104086"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "1",
    "abstract": "<jats:title>Abstract</jats:title><jats:p>Recent large language models (LLMs), such as ChatGPT, have demonstrated remarkable prediction performance for a growing array of tasks. However, their proliferation into high-stakes domains and compute-limited settings has created a burgeoning need for interpretability and efficiency. We address this need by proposing Aug-imodels, a framework for leveraging the knowledge learned by LLMs to build extremely efficient and interpretable prediction models. Aug-imodels use LLMs during fitting but not during inference, allowing complete transparency and often a speed/memory improvement of greater than 1000x for inference compared to LLMs. We explore two instantiations of Aug-imodels in natural-language processing: Aug-Linear, which augments a linear model with decoupled embeddings from an LLM and Aug-Tree, which augments a decision tree with LLM feature expansions. Across a variety of text-classification datasets, both outperform their non-augmented, interpretable counterparts. Aug-Linear can even outperform much larger models, e.g. a 6-billion parameter GPT-J model, despite having 10,000x fewer parameters and being fully transparent. We further explore Aug-imodels in a natural-language fMRI study, where they generate interesting interpretations from scientific data.</jats:p>",
    "DOI": "10.1038/s41467-023-43713-1",
    "type": "article-journal",
    "source": "Crossref",
    "title": "Augmenting interpretable models with large language models during training",
    "volume": "14",
    "author": [
      {
        "given": "Chandan",
        "family": "Singh"
      },
      {
        "given": "Armin",
        "family": "Askari"
      },
      {
        "given": "Rich",
        "family": "Caruana"
      },
      {
        "given": "Jianfeng",
        "family": "Gao"
      }
    ],
    "container-title": "Nature Communications",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2023,
          11,
          30
        ]
      ]
    },
    "URL": "https://doi.org/g9t2z9",
    "container-title-short": "Nat Commun",
    "PMCID": "PMC10689442",
    "PMID": "38036543",
    "id": "HQXzkG4Q",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1038/s41467-023-43713-1"
  },
  {
    "publisher": "Informa UK Limited",
    "issue": "509",
    "DOI": "10.1080/01621459.2014.896806",
    "type": "article-journal",
    "page": "159-174",
    "source": "Crossref",
    "title": "Bayesian Inference of Multiple Gaussian Graphical Models",
    "volume": "110",
    "author": [
      {
        "given": "Christine",
        "family": "Peterson"
      },
      {
        "given": "Francesco C.",
        "family": "Stingo"
      },
      {
        "given": "Marina",
        "family": "Vannucci"
      }
    ],
    "container-title": "Journal of the American Statistical Association",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2015,
          1,
          2
        ]
      ]
    },
    "URL": "https://doi.org/f69dnj",
    "container-title-short": "Journal of the American Statistical Association",
    "PMCID": "PMC4465207",
    "PMID": "26078481",
    "id": "1Da8QJneg",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1080/01621459.2014.896806"
  },
  {
    "publisher": "Informa UK Limited",
    "issue": "538",
    "DOI": "10.1080/01621459.2021.2000866",
    "type": "article-journal",
    "page": "533-546",
    "source": "Crossref",
    "title": "Bayesian Edge Regression in Undirected Graphical Models to Characterize Interpatient Heterogeneity in Cancer",
    "volume": "117",
    "author": [
      {
        "given": "Zeya",
        "family": "Wang"
      },
      {
        "given": "Veerabhadran",
        "family": "Baladandayuthapani"
      },
      {
        "given": "Ahmed O.",
        "family": "Kaseb"
      },
      {
        "given": "Hesham M.",
        "family": "Amin"
      },
      {
        "given": "Manal M.",
        "family": "Hassan"
      },
      {
        "given": "Wenyi",
        "family": "Wang"
      },
      {
        "given": "Jeffrey S.",
        "family": "Morris"
      }
    ],
    "container-title": "Journal of the American Statistical Association",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2022,
          1,
          5
        ]
      ]
    },
    "URL": "https://doi.org/gt68hr",
    "container-title-short": "Journal of the American Statistical Association",
    "PMCID": "PMC9454401",
    "PMID": "36090952",
    "id": "TULLRYDp",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1080/01621459.2021.2000866"
  },
  {
    "publisher": "Oxford University Press (OUP)",
    "issue": "1",
    "DOI": "10.1093/biomet/asq060",
    "type": "article-journal",
    "page": "1-15",
    "source": "Crossref",
    "title": "Joint estimation of multiple graphical models",
    "volume": "98",
    "author": [
      {
        "given": "J.",
        "family": "Guo"
      },
      {
        "given": "E.",
        "family": "Levina"
      },
      {
        "given": "G.",
        "family": "Michailidis"
      },
      {
        "given": "J.",
        "family": "Zhu"
      }
    ],
    "container-title": "Biometrika",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2011,
          2,
          9
        ]
      ]
    },
    "URL": "https://doi.org/fqvbh2",
    "container-title-short": "Biometrika",
    "PMCID": "PMC3412604",
    "PMID": "23049124",
    "id": "hxZIBmjM",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1093/biomet/asq060"
  },
  {
    "publisher": "Oxford University Press (OUP)",
    "issue": "2",
    "abstract": "<jats:title>ABSTRACT</jats:title>\n               <jats:p>Covariate-dependent graph learning has gained increasing interest in the graphical modeling literature for the analysis of heterogeneous data. This task, however, poses challenges to modeling, computational efficiency, and interpretability. The parameter of interest can be naturally represented as a 3-dimensional array with elements that can be grouped according to 2 directions, corresponding to node level and covariate level, respectively. In this article, we propose a novel dual group spike-and-slab prior that enables multi-level selection at covariate-level and node-level, as well as individual (local) level sparsity. We introduce a nested strategy with specific choices to address distinct challenges posed by the various grouping directions. For posterior inference, we develop a full Gibbs sampler for all parameters, which mitigates the difficulties of parameter tuning often encountered in high-dimensional graphical models and facilitates routine implementation. Through simulation studies, we demonstrate that the proposed model outperforms existing methods in its accuracy of graph recovery. We show the practical utility of our model via an application to microbiome data where we seek to better understand the interactions among microbes as well as how these are affected by relevant covariates.</jats:p>",
    "DOI": "10.1093/biomtc/ujaf053",
    "type": "article-journal",
    "source": "Crossref",
    "title": "Bayesian covariate-dependent graph learning with a dual group spike-and-slab prior",
    "volume": "81",
    "author": [
      {
        "given": "Zijian",
        "family": "Zeng"
      },
      {
        "given": "Meng",
        "family": "Li"
      },
      {
        "given": "Marina",
        "family": "Vannucci"
      }
    ],
    "container-title": "Biometrics",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2025,
          4,
          2
        ]
      ]
    },
    "URL": "https://doi.org/g95bkg",
    "PMID": "40322851",
    "id": "x5cbX2Cv",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1093/biomtc/ujaf053"
  },
  {
    "publisher": "Oxford University Press (OUP)",
    "issue": "3",
    "abstract": "<jats:title>Abstract</jats:title>\n               <jats:p>We consider the problem of estimating sparse graphs by a lasso penalty applied to the inverse covariance matrix. Using a coordinate descent procedure for the lasso, we develop a simple algorithmâthe graphical lassoâthat is remarkably fast: It solves a 1000-node problem (â¼500000 parameters) in at most a minute and is 30â4000 times faster than competing methods. It also provides a conceptual link between the exact problem and the approximation suggested by Meinshausen and BÃ¼hlmann (2006). We illustrate the method on some cell-signaling data from proteomics.</jats:p>",
    "DOI": "10.1093/biostatistics/kxm045",
    "type": "article-journal",
    "page": "432-441",
    "source": "Crossref",
    "title": "Sparse inverse covariance estimation with the graphical lasso",
    "volume": "9",
    "author": [
      {
        "given": "Jerome",
        "family": "Friedman"
      },
      {
        "given": "Trevor",
        "family": "Hastie"
      },
      {
        "given": "Robert",
        "family": "Tibshirani"
      }
    ],
    "container-title": "Biostatistics",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2007,
          12,
          12
        ]
      ]
    },
    "URL": "https://doi.org/db7svr",
    "PMCID": "PMC3019769",
    "PMID": "18079126",
    "id": "m4KsbXUW",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1093/biostatistics/kxm045"
  },
  {
    "publisher": "Cold Spring Harbor Laboratory",
    "abstract": "<jats:title>Abstract</jats:title><jats:p>Summarizing multiple data modalities into a parsimonious cancer âsubtypeâ is difficult because the most informative representation of each patientâs disease is not observed. We propose to model these latent summaries as<jats:italic>discriminative subtypes</jats:italic>: sample representations which induce accurate and interpretable sample-specific models for downstream predictions. In this way, discriminative subtypes, which are shared between data modalities, can be estimated from one data modality and optimized according to the predictions induced in another modality. We apply this approach to lung cancer by training a deep neural network to predict discriminative subtypes from histopathology images, and use these predicted subtypes to generate models which classify adenocarcinoma, squamous cell carcinoma, and healthy tissue based on transcriptomic signatures. In this way, we optimize the latent discriminative subtypes through induced prediction loss, and the discriminative subtypes are interpreted with standard interpretation of transcriptomic predictive models. Our framework achieves state-of-the-art classification accuracy (F1-score of 0.97) and identifies discriminative subtypes which link histopathology images to transcriptomic explanations without requiring pre-specification of morphological patterns or transcriptomic processes.</jats:p>",
    "DOI": "10.1101/2020.06.25.20140053",
    "type": "manuscript",
    "source": "Crossref",
    "title": "Discriminative Subtyping of Lung Cancers from Histopathology Images via Contextual Deep Learning",
    "author": [
      {
        "given": "Benjamin J.",
        "family": "Lengerich"
      },
      {
        "given": "Maruan",
        "family": "Al-Shedivat"
      },
      {
        "given": "Amir",
        "family": "Alavi"
      },
      {
        "given": "Jennifer",
        "family": "Williams"
      },
      {
        "given": "Sami",
        "family": "Labbaki"
      },
      {
        "given": "Eric P.",
        "family": "Xing"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2020,
          6,
          26
        ]
      ]
    },
    "URL": "https://doi.org/gt68h6",
    "id": "O1UU4a5P",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1101/2020.06.25.20140053"
  },
  {
    "publisher": "Cold Spring Harbor Laboratory",
    "abstract": "<jats:p>Cancers are shaped by somatic mutations, microenvironment, and patient background, each altering gene expression and regulation in complex ways, resulting in heterogeneous cellular states and dynamics. Inferring gene regulatory networks (GRNs) from expression data can help characterize this regulation-driven heterogeneity, but network inference requires many statistical samples, limiting GRNs to cluster-level analyses that ignore intra-cluster heterogeneity. We propose to move beyond coarse analyses of pre-defined subgroups by using<jats:italic>contextualized</jats:italic>learning, a multi-task learning paradigm that uses multi-view contexts including phenotypic, molecular, and environmental information to infer personalized models. With sample-specific contexts, contextualization enables sample-specific models and even generalizes at test time to predict network models for entirely unseen contexts. We unify three network model classes (Correlation, Markov, Neighborhood Selection) and estimate context-specific GRNs for 7997 tumors across 25 tumor types, using copy number and driver mutation profiles, tumor microenvironment, and patient demographics as model context. Our generative modeling approach allows us to predict GRNs for unseen tumor types based on a pan-cancer model of how somatic mutations affect gene regulation. Finally, contextualized networks enable GRN-based precision oncology by providing a structured view of expression dynamics at sample-specific resolution, explaining known biomarkers in terms of network-mediated effects and leading to novel subtypings that improve survival prognosis. We provide a SKLearn-style Python package<jats:ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://contextualized.ml\">https://contextualized.ml</jats:ext-link>for learning and analyzing contextualized models, as well as interactive plotting tools for pan-cancer data exploration at<jats:ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://github.com/cnellington/CancerContextualized\">https://github.com/cnellington/CancerContextualized</jats:ext-link>.</jats:p><jats:sec><jats:title>Significance Statement</jats:title><jats:p>Network estimation is essential for understanding the structure and function of biological systems, but current statistical approaches fail to capture inter-subject heterogeneity or cross-modality information flow, both of which are needed for understanding complex phenotypes and pathologies. We introduce contextualized network inference, leveraging multi-view contextual metadata to capture similarities and differences among heterogeneous observations during network estimation. Sharing information across contexts enables inference at sample-specific resolution, thus quantifying variation between subjects and revealing context-specific network rewiring. Applied to tumor-specific transcriptional network inference using clinical, molecular, and multi-omic data, contextualized networks improve accuracy, generalize to unseen cancer types, and discover novel prognostic tumor subtypes. By tailoring disease models to each sample, contextualized networks promise to enable precision medicine at unprecedented resolution.</jats:p></jats:sec>",
    "DOI": "10.1101/2023.12.01.569658",
    "type": "manuscript",
    "source": "Crossref",
    "title": "Learning to Estimate Sample-specific Transcriptional Networks for 7000 Tumors",
    "author": [
      {
        "given": "Caleb N.",
        "family": "Ellington"
      },
      {
        "given": "Benjamin J.",
        "family": "Lengerich"
      },
      {
        "given": "Thomas B.K.",
        "family": "Watkins"
      },
      {
        "given": "Jiekun",
        "family": "Yang"
      },
      {
        "given": "Abhinav",
        "family": "Adduri"
      },
      {
        "given": "Sazan",
        "family": "Mahbub"
      },
      {
        "given": "Hanxi",
        "family": "Xiao"
      },
      {
        "given": "Manolis",
        "family": "Kellis"
      },
      {
        "given": "Eric P.",
        "family": "Xing"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023,
          12,
          4
        ]
      ]
    },
    "URL": "https://doi.org/gt68h7",
    "id": "Rt6voTFN",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1101/2023.12.01.569658"
  },
  {
    "publisher": "Oxford University Press (OUP)",
    "issue": "4",
    "abstract": "<jats:title>SUMMARY</jats:title>\n               <jats:p>We explore a class of regression and generalized regression models in which the coefficients are allowed to vary as smooth functions of other variables. General algorithms are presented for estimating the models flexibly and some examples are given. This class of models ties together generalized additive models and dynamic generalized linear models into one common framework. When applied to the proportional hazards model for survival data, this approach provides a new way of modelling departures from the proportional hazards assumption.</jats:p>",
    "DOI": "10.1111/j.2517-6161.1993.tb01939.x",
    "type": "article-journal",
    "page": "757-779",
    "source": "Crossref",
    "title": "Varying-Coefficient Models",
    "volume": "55",
    "author": [
      {
        "given": "Trevor",
        "family": "Hastie"
      },
      {
        "given": "Robert",
        "family": "Tibshirani"
      }
    ],
    "container-title": "Journal of the Royal Statistical Society Series B: Statistical Methodology",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          1993,
          9,
          1
        ]
      ]
    },
    "URL": "https://doi.org/gmfvmb",
    "id": "ugXwusl0",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1111/j.2517-6161.1993.tb01939.x"
  },
  {
    "publisher": "Oxford University Press (OUP)",
    "issue": "2",
    "abstract": "<jats:title>Summary</jats:title><jats:p>We consider the problem of estimating multiple related Gaussian graphical models from a high dimensional data set with observations belonging to distinct classes. We propose the joint graphical lasso, which borrows strength across the classes to estimate multiple graphical models that share certain characteristics, such as the locations or weights of non-zero edges. Our approach is based on maximizing a penalized log-likelihood. We employ generalized fused lasso or group lasso penalties and implement a fast alternating directions method of multipliers algorithm to solve the corresponding convex optimization problems. The performance of the method proposed is illustrated through simulated and real data examples.</jats:p>",
    "DOI": "10.1111/rssb.12033",
    "type": "article-journal",
    "page": "373-397",
    "source": "Crossref",
    "title": "The Joint Graphical Lasso for Inverse Covariance Estimation Across Multiple Classes",
    "volume": "76",
    "author": [
      {
        "given": "Patrick",
        "family": "Danaher"
      },
      {
        "given": "Pei",
        "family": "Wang"
      },
      {
        "given": "Daniela M.",
        "family": "Witten"
      }
    ],
    "container-title": "Journal of the Royal Statistical Society Series B: Statistical Methodology",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2013,
          8,
          12
        ]
      ]
    },
    "URL": "https://doi.org/f5sj9g",
    "PMCID": "PMC4012833",
    "PMID": "24817823",
    "id": "JDoK9thg",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1111/rssb.12033"
  },
  {
    "publisher": "ACM",
    "DOI": "10.1145/3097983.3098066",
    "type": "paper-conference",
    "page": "275-284",
    "source": "Crossref",
    "title": "The Selective Labels Problem",
    "author": [
      {
        "given": "Himabindu",
        "family": "Lakkaraju"
      },
      {
        "given": "Jon",
        "family": "Kleinberg"
      },
      {
        "given": "Jure",
        "family": "Leskovec"
      },
      {
        "given": "Jens",
        "family": "Ludwig"
      },
      {
        "given": "Sendhil",
        "family": "Mullainathan"
      }
    ],
    "event": "KDD '17: The 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
    "container-title": "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
    "issued": {
      "date-parts": [
        [
          2017,
          8,
          4
        ]
      ]
    },
    "URL": "https://doi.org/ggd7hz",
    "PMCID": "PMC5958915",
    "PMID": "29780658",
    "id": "MGkiKe9y",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1145/3097983.3098066"
  },
  {
    "publisher": "Association for Computing Machinery (ACM)",
    "issue": "9",
    "abstract": "<jats:p>\n            This article surveys and organizes research works in a new paradigm in natural language processing, which we dub âprompt-based learning.â Unlike traditional supervised learning, which trains a model to take in an input\n            <jats:bold>\n              <jats:italic>x</jats:italic>\n            </jats:bold>\n            and predict an output\n            <jats:bold>\n              <jats:italic>y</jats:italic>\n            </jats:bold>\n            as\n            <jats:italic>P</jats:italic>\n            (\n            <jats:bold>\n              <jats:italic>y|x</jats:italic>\n            </jats:bold>\n            ), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input\n            <jats:bold>\n              <jats:italic>x</jats:italic>\n            </jats:bold>\n            is modified using a\n            <jats:italic>template</jats:italic>\n            into a textual string\n            <jats:italic>prompt</jats:italic>\n            <jats:bold>\n              <jats:italic>xâ²</jats:italic>\n            </jats:bold>\n            that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string\n            <jats:bold>\n              <jats:italic>xÌ</jats:italic>\n            </jats:bold>\n            , from which the final output\n            <jats:bold>\n              <jats:italic>y</jats:italic>\n            </jats:bold>\n            can be derived. This framework is powerful and attractive for a number of reasons: It allows the language model to be\n            <jats:italic>pre-trained</jats:italic>\n            on massive amounts of raw text, and by defining a new prompting function the model is able to perform\n            <jats:italic>few-shot</jats:italic>\n            or even\n            <jats:italic>zero-shot</jats:italic>\n            learning, adapting to new scenarios with few or no labeled data. In this article, we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.,Â the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources, e.g., a website\n            <jats:ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"url\" xlink:href=\"http://pretrain.nlpedia.ai/\">NLPediaâPretrain</jats:ext-link>\n            including constantly updated survey and paperlist.\n          </jats:p>",
    "DOI": "10.1145/3560815",
    "type": "article-journal",
    "page": "1-35",
    "source": "Crossref",
    "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",
    "volume": "55",
    "author": [
      {
        "given": "Pengfei",
        "family": "Liu"
      },
      {
        "given": "Weizhe",
        "family": "Yuan"
      },
      {
        "given": "Jinlan",
        "family": "Fu"
      },
      {
        "given": "Zhengbao",
        "family": "Jiang"
      },
      {
        "given": "Hiroaki",
        "family": "Hayashi"
      },
      {
        "given": "Graham",
        "family": "Neubig"
      }
    ],
    "container-title": "ACM Computing Surveys",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2023,
          1,
          16
        ]
      ]
    },
    "URL": "https://doi.org/gq5fh2",
    "container-title-short": "ACM Comput. Surv.",
    "id": "12nAa0T4v",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1145/3560815"
  },
  {
    "publisher": "Informa UK Limited",
    "issue": "496",
    "DOI": "10.1198/jasa.2011.tm10465",
    "type": "article-journal",
    "page": "1418-1433",
    "source": "Crossref",
    "title": "Bayesian Inference for General Gaussian Graphical Models With Application to Multivariate Lattice Data",
    "volume": "106",
    "author": [
      {
        "given": "Adrian",
        "family": "Dobra"
      },
      {
        "given": "Alex",
        "family": "Lenkoski"
      },
      {
        "given": "Abel",
        "family": "Rodriguez"
      }
    ],
    "container-title": "Journal of the American Statistical Association",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2011,
          12
        ]
      ]
    },
    "URL": "https://doi.org/fxq5wh",
    "container-title-short": "Journal of the American Statistical Association",
    "PMCID": "PMC4767185",
    "PMID": "26924867",
    "id": "721WoKJr",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1198/jasa.2011.tm10465"
  },
  {
    "publisher": "Institute of Mathematical Statistics",
    "issue": "3",
    "DOI": "10.1214/009053606000000281",
    "type": "article-journal",
    "source": "Crossref",
    "title": "High-dimensional graphs and variable selection with the Lasso",
    "volume": "34",
    "author": [
      {
        "given": "Nicolai",
        "family": "Meinshausen"
      },
      {
        "given": "Peter",
        "family": "BÃ¼hlmann"
      }
    ],
    "container-title": "The Annals of Statistics",
    "issued": {
      "date-parts": [
        [
          2006,
          6,
          1
        ]
      ]
    },
    "URL": "https://doi.org/fwt5kt",
    "container-title-short": "Ann. Statist.",
    "id": "eFgbj4Kw",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1214/009053606000000281"
  },
  {
    "publisher": "Institute of Mathematical Statistics",
    "issue": "1",
    "DOI": "10.1214/09-aoas308",
    "type": "article-journal",
    "source": "Crossref",
    "title": "Estimating time-varying networks",
    "volume": "4",
    "author": [
      {
        "given": "Mladen",
        "family": "Kolar"
      },
      {
        "given": "Le",
        "family": "Song"
      },
      {
        "given": "Amr",
        "family": "Ahmed"
      },
      {
        "given": "Eric P.",
        "family": "Xing"
      }
    ],
    "container-title": "The Annals of Applied Statistics",
    "issued": {
      "date-parts": [
        [
          2010,
          3,
          1
        ]
      ]
    },
    "URL": "https://doi.org/b3rn6q",
    "container-title-short": "Ann. Appl. Stat.",
    "id": "lAsTg3IH",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1214/09-aoas308"
  },
  {
    "publisher": "Institute of Mathematical Statistics",
    "issue": "5",
    "DOI": "10.1214/aos/1017939139",
    "type": "article-journal",
    "source": "Crossref",
    "title": "Statistical estimation in varying coefficient models",
    "volume": "27",
    "author": [
      {
        "given": "Jianqing",
        "family": "Fan"
      },
      {
        "given": "Wenyang",
        "family": "Zhang"
      }
    ],
    "container-title": "The Annals of Statistics",
    "issued": {
      "date-parts": [
        [
          1999,
          10,
          1
        ]
      ]
    },
    "URL": "https://doi.org/dsxd4s",
    "container-title-short": "Ann. Statist.",
    "id": "l6vMkIsa",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1214/aos/1017939139"
  },
  {
    "publisher": "Association for Computational Linguistics",
    "DOI": "10.18653/v1/2023.emnlp-main.384",
    "type": "paper-conference",
    "page": "6253-6267",
    "source": "Crossref",
    "title": "Tree Prompting: Efficient Task Adaptation without Fine-Tuning",
    "author": [
      {
        "given": "Chandan",
        "family": "Singh"
      },
      {
        "given": "John",
        "family": "Morris"
      },
      {
        "given": "Alexander",
        "family": "Rush"
      },
      {
        "given": "Jianfeng",
        "family": "Gao"
      },
      {
        "given": "Yuntian",
        "family": "Deng"
      }
    ],
    "event": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    "container-title": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "URL": "https://doi.org/gtgrkq",
    "id": "kAJDlMwy",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.18653/v1/2023.emnlp-main.384"
  },
  {
    "publisher": "The Open Journal",
    "issue": "97",
    "DOI": "10.21105/joss.06469",
    "type": "article-journal",
    "page": "6469",
    "source": "Crossref",
    "title": "Contextualized: Heterogeneous Modeling Toolbox",
    "volume": "9",
    "author": [
      {
        "given": "Caleb N.",
        "family": "Ellington"
      },
      {
        "given": "Benjamin J.",
        "family": "Lengerich"
      },
      {
        "given": "Wesley",
        "family": "Lo"
      },
      {
        "given": "Aaron",
        "family": "Alvarez"
      },
      {
        "given": "Andrea",
        "family": "Rubbi"
      },
      {
        "given": "Manolis",
        "family": "Kellis"
      },
      {
        "given": "Eric P.",
        "family": "Xing"
      }
    ],
    "container-title": "Journal of Open Source Software",
    "issued": {
      "date-parts": [
        [
          2024,
          5,
          8
        ]
      ]
    },
    "URL": "https://doi.org/gt68h8",
    "container-title-short": "JOSS",
    "id": "4cK1tiec",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.21105/joss.06469"
  },
  {
    "publisher": "JSTOR",
    "issue": "1",
    "DOI": "10.2307/2528966",
    "type": "article-journal",
    "page": "157",
    "source": "Crossref",
    "title": "Covariance Selection",
    "volume": "28",
    "author": [
      {
        "given": "A. P.",
        "family": "Dempster"
      }
    ],
    "container-title": "Biometrics",
    "issued": {
      "date-parts": [
        [
          1972,
          3
        ]
      ]
    },
    "URL": "https://doi.org/d5t49s",
    "container-title-short": "Biometrics",
    "id": "1AAscRzye",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.2307/2528966"
  },
  {
    "publisher": "MDPI AG",
    "issue": "2",
    "abstract": "<jats:p>Numerous papers have demonstrated that by using a varying coefficients model (VCM), researchers can unveil patterns of interactions between variables that could otherwise remain hidden if using the more popular regression model with an interaction term. Hence, one would expect high acceptance of the VCM as a tool for studying statistical interactions in datasets. Yet, the current paper shows that the VCM is still struggling to migrate from journals in which methods are presented to journals in which methods are utilized. First, a search in Google Scholar with the phrase âvarying coefficientsâ returned ~79,200 results in comparison to returning ~2,710,000 results with the phrase âinteraction termâ. Second, a bibliometric analysis of publications with the VCM showed that in many research domains, there were more publications with the VCM in journals on methods than publications with the VCM in journals for empirical investigations. Economics and environmental studies stood out with many more publications with the VCM in empirical journals than in journals on statistical methods. The gap between the high acclaims of the VCM in the statistical literature and its low utilization rate in practice should be of concern to the research community. The possible reasons for this gap and its potential remedies are discussed.</jats:p>",
    "DOI": "10.3390/publications13020019",
    "type": "article-journal",
    "page": "19",
    "source": "Crossref",
    "title": "Publication Trends on the Varying Coefficients Model: Estimating the Actual (Under)Utilization of a Highly Acclaimed Method for Studying Statistical Interactions",
    "volume": "13",
    "author": [
      {
        "given": "Assaf",
        "family": "Botzer"
      }
    ],
    "container-title": "Publications",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2025,
          4,
          7
        ]
      ]
    },
    "URL": "https://doi.org/g9t2rq",
    "container-title-short": "Publications",
    "id": "hRp04fhf",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.3390/publications13020019"
  },
  {
    "type": "article",
    "id": "SfCo6pSp",
    "categories": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Al-Shedivat",
        "given": "Maruan"
      },
      {
        "family": "Dubey",
        "given": "Avinava"
      },
      {
        "family": "Xing",
        "given": "Eric P."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "abstract": "Modern learning algorithms excel at producing accurate but complex models of the data. However, deploying such models in the real-world requires extra care: we must ensure their reliability, robustness, and absence of undesired biases. This motivates the development of models that are equally accurate but can be also easily inspected and assessed beyond their predictive performance. To this end, we introduce contextual explanation networks (CEN)---a class of architectures that learn to predict by generating and utilizing intermediate, simplified probabilistic models. Specifically, CENs generate parameters for intermediate graphical models which are further used for prediction and play the role of explanations. Contrary to the existing post-hoc model-explanation tools, CENs learn to predict and to explain simultaneously. Our approach offers two major advantages: (i) for each prediction valid, instance-specific explanation is generated with no computational overhead and (ii) prediction via explanation acts as a regularizer and boosts performance in data-scarce settings. We analyze the proposed framework theoretically and experimentally. Our results on image and text classification and survival analysis tasks demonstrate that CENs are not only competitive with the state-of-the-art methods but also offer additional insights behind each prediction, that can be valuable for decision support. We also show that while post-hoc methods may produce misleading explanations in certain cases, CENs are consistent and allow to detect such cases systematically.",
    "DOI": "10.48550/arxiv.1705.10301",
    "publisher": "arXiv",
    "title": "Contextual Explanation Networks",
    "URL": "https://doi.org/gt68h9",
    "version": "4",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1705.10301"
  },
  {
    "type": "article",
    "id": "J9eXUymQ",
    "categories": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Ahn",
        "given": "Ho Seok"
      },
      {
        "family": "Dayoub",
        "given": "Feras"
      },
      {
        "family": "Popovic",
        "given": "Marija"
      },
      {
        "family": "MacDonald",
        "given": "Bruce"
      },
      {
        "family": "Siegwart",
        "given": "Roland"
      },
      {
        "family": "Sa",
        "given": "Inkyu"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "abstract": "Horticultural enterprises are becoming more sophisticated as the range of the crops they target expands. Requirements for enhanced efficiency and productivity have driven the demand for automating on-field operations. However, various problems remain yet to be solved for their reliable, safe deployment in real-world scenarios. This paper examines major research trends and current challenges in horticultural robotics. Specifically, our work focuses on sensing and perception in the three main horticultural procedures: pollination, yield estimation, and harvesting. For each task, we expose major issues arising from the unstructured, cluttered, and rugged nature of field environments, including variable lighting conditions and difficulties in fruit-specific detection, and highlight promising contemporary studies.",
    "DOI": "10.48550/arxiv.1807.03124",
    "publisher": "arXiv",
    "title": "An Overview of Perception Methods for Horticultural Robots: From Pollination to Harvest",
    "URL": "https://doi.org/g93rnk",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1807.03124"
  },
  {
    "type": "article",
    "id": "urJgpE6q",
    "categories": [
      "Computation and Language (cs.CL)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Devlin",
        "given": "Jacob"
      },
      {
        "family": "Chang",
        "given": "Ming-Wei"
      },
      {
        "family": "Lee",
        "given": "Kenton"
      },
      {
        "family": "Toutanova",
        "given": "Kristina"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
    "DOI": "10.48550/arxiv.1810.04805",
    "publisher": "arXiv",
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "URL": "https://doi.org/hm65",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1810.04805"
  },
  {
    "type": "article",
    "id": "WlwUpYp",
    "categories": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Lengerich",
        "given": "Benjamin"
      },
      {
        "family": "Aragam",
        "given": "Bryon"
      },
      {
        "family": "Xing",
        "given": "Eric P."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "abstract": "Modern applications of machine learning (ML) deal with increasingly heterogeneous datasets comprised of data collected from overlapping latent subpopulations. As a result, traditional models trained over large datasets may fail to recognize highly predictive localized effects in favour of weakly predictive global patterns. This is a problem because localized effects are critical to developing individualized policies and treatment plans in applications ranging from precision medicine to advertising. To address this challenge, we propose to estimate sample-specific models that tailor inference and prediction at the individual level. In contrast to classical ML models that estimate a single, complex model (or only a few complex models), our approach produces a model personalized to each sample. These sample-specific models can be studied to understand subgroup dynamics that go beyond coarse-grained class labels. Crucially, our approach does not assume that relationships between samples (e.g. a similarity network) are known a priori. Instead, we use unmodeled covariates to learn a latent distance metric over the samples. We apply this approach to financial, biomedical, and electoral data as well as simulated data and show that sample-specific models provide fine-grained interpretations of complicated phenomena without sacrificing predictive accuracy compared to state-of-the-art models such as deep neural networks.",
    "DOI": "10.48550/arxiv.1910.06939",
    "publisher": "arXiv",
    "title": "Learning Sample-Specific Models with Low-Rank Personalized Regression",
    "URL": "https://doi.org/gt68jb",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1910.06939"
  },
  {
    "type": "article",
    "id": "11l8svMmM",
    "categories": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Sagawa",
        "given": "Shiori"
      },
      {
        "family": "Koh",
        "given": "Pang Wei"
      },
      {
        "family": "Hashimoto",
        "given": "Tatsunori B."
      },
      {
        "family": "Liang",
        "given": "Percy"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "abstract": "Overparameterized neural networks can be highly accurate on average on an i.i.d. test set yet consistently fail on atypical groups of the data (e.g., by learning spurious correlations that hold on average but not in such groups). Distributionally robust optimization (DRO) allows us to learn models that instead minimize the worst-case training loss over a set of pre-defined groups. However, we find that naively applying group DRO to overparameterized neural networks fails: these models can perfectly fit the training data, and any model with vanishing average training loss also already has vanishing worst-case training loss. Instead, the poor worst-case performance arises from poor generalization on some groups. By coupling group DRO models with increased regularization---a stronger-than-typical L2 penalty or early stopping---we achieve substantially higher worst-group accuracies, with 10-40 percentage point improvements on a natural language inference task and two image tasks, while maintaining high average accuracies. Our results suggest that regularization is important for worst-group generalization in the overparameterized regime, even if it is not needed for average generalization. Finally, we introduce a stochastic optimization algorithm, with convergence guarantees, to efficiently train group DRO models.",
    "DOI": "10.48550/arxiv.1911.08731",
    "publisher": "arXiv",
    "title": "Distributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Case Generalization",
    "URL": "https://doi.org/g93rnm",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1911.08731"
  },
  {
    "type": "article-journal",
    "id": "4fnOdPts",
    "categories": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences",
      "FOS: Biological sciences",
      "FOS: Biological sciences"
    ],
    "author": [
      {
        "family": "Geirhos",
        "given": "Robert"
      },
      {
        "family": "Jacobsen",
        "given": "JÃ¶rn-Henrik"
      },
      {
        "family": "Michaelis",
        "given": "Claudio"
      },
      {
        "family": "Zemel",
        "given": "Richard"
      },
      {
        "family": "Brendel",
        "given": "Wieland"
      },
      {
        "family": "Bethge",
        "given": "Matthias"
      },
      {
        "family": "Wichmann",
        "given": "Felix A."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "abstract": "Deep learning has triggered the current rise of artificial intelligence and is the workhorse of today's machine intelligence. Numerous success stories have rapidly spread all over science, industry and society, but its limitations have only recently come into focus. In this perspective we seek to distill how many of deep learning's problems can be seen as different symptoms of the same underlying problem: shortcut learning. Shortcuts are decision rules that perform well on standard benchmarks but fail to transfer to more challenging testing conditions, such as real-world scenarios. Related issues are known in Comparative Psychology, Education and Linguistics, suggesting that shortcut learning may be a common characteristic of learning systems, biological and artificial alike. Based on these observations, we develop a set of recommendations for model interpretation and benchmarking, highlighting recent advances in machine learning to improve robustness and transferability from the lab to real-world applications.",
    "container-title": "arXiv",
    "DOI": "10.48550/arxiv.2004.07780",
    "publisher": "arXiv",
    "title": "Shortcut Learning in Deep Neural Networks",
    "URL": "https://doi.org/g93rnn",
    "version": "5",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2004.07780"
  },
  {
    "type": "article",
    "id": "PKjSQOD",
    "categories": [
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Koh",
        "given": "Pang Wei"
      },
      {
        "family": "Sagawa",
        "given": "Shiori"
      },
      {
        "family": "Marklund",
        "given": "Henrik"
      },
      {
        "family": "Xie",
        "given": "Sang Michael"
      },
      {
        "family": "Zhang",
        "given": "Marvin"
      },
      {
        "family": "Balsubramani",
        "given": "Akshay"
      },
      {
        "family": "Hu",
        "given": "Weihua"
      },
      {
        "family": "Yasunaga",
        "given": "Michihiro"
      },
      {
        "family": "Phillips",
        "given": "Richard Lanas"
      },
      {
        "family": "Gao",
        "given": "Irena"
      },
      {
        "family": "Lee",
        "given": "Tony"
      },
      {
        "family": "David",
        "given": "Etienne"
      },
      {
        "family": "Stavness",
        "given": "Ian"
      },
      {
        "family": "Guo",
        "given": "Wei"
      },
      {
        "family": "Earnshaw",
        "given": "Berton A."
      },
      {
        "family": "Haque",
        "given": "Imran S."
      },
      {
        "family": "Beery",
        "given": "Sara"
      },
      {
        "family": "Leskovec",
        "given": "Jure"
      },
      {
        "family": "Kundaje",
        "given": "Anshul"
      },
      {
        "family": "Pierson",
        "given": "Emma"
      },
      {
        "family": "Levine",
        "given": "Sergey"
      },
      {
        "family": "Finn",
        "given": "Chelsea"
      },
      {
        "family": "Liang",
        "given": "Percy"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "abstract": "Distribution shifts -- where the training distribution differs from the test distribution -- can substantially degrade the accuracy of machine learning (ML) systems deployed in the wild. Despite their ubiquity in the real-world deployments, these distribution shifts are under-represented in the datasets widely used in the ML community today. To address this gap, we present WILDS, a curated benchmark of 10 datasets reflecting a diverse range of distribution shifts that naturally arise in real-world applications, such as shifts across hospitals for tumor identification; across camera traps for wildlife monitoring; and across time and location in satellite imaging and poverty mapping. On each dataset, we show that standard training yields substantially lower out-of-distribution than in-distribution performance. This gap remains even with models trained by existing methods for tackling distribution shifts, underscoring the need for new methods for training models that are more robust to the types of distribution shifts that arise in practice. To facilitate method development, we provide an open-source package that automates dataset loading, contains default model architectures and hyperparameters, and standardizes evaluations. Code and leaderboards are available at https://wilds.stanford.edu.",
    "DOI": "10.48550/arxiv.2012.07421",
    "publisher": "arXiv",
    "title": "WILDS: A Benchmark of in-the-Wild Distribution Shifts",
    "URL": "https://doi.org/g93rnp",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2012.07421"
  },
  {
    "type": "article",
    "id": "17tnf46zM",
    "categories": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Radford",
        "given": "Alec"
      },
      {
        "family": "Kim",
        "given": "Jong Wook"
      },
      {
        "family": "Hallacy",
        "given": "Chris"
      },
      {
        "family": "Ramesh",
        "given": "Aditya"
      },
      {
        "family": "Goh",
        "given": "Gabriel"
      },
      {
        "family": "Agarwal",
        "given": "Sandhini"
      },
      {
        "family": "Sastry",
        "given": "Girish"
      },
      {
        "family": "Askell",
        "given": "Amanda"
      },
      {
        "family": "Mishkin",
        "given": "Pamela"
      },
      {
        "family": "Clark",
        "given": "Jack"
      },
      {
        "family": "Krueger",
        "given": "Gretchen"
      },
      {
        "family": "Sutskever",
        "given": "Ilya"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "abstract": "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",
    "DOI": "10.48550/arxiv.2103.00020",
    "publisher": "arXiv",
    "title": "Learning Transferable Visual Models From Natural Language Supervision",
    "URL": "https://doi.org/hs7z",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2103.00020"
  },
  {
    "type": "article",
    "id": "1GbAsSOZV",
    "categories": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Bommasani",
        "given": "Rishi"
      },
      {
        "family": "Hudson",
        "given": "Drew A."
      },
      {
        "family": "Adeli",
        "given": "Ehsan"
      },
      {
        "family": "Altman",
        "given": "Russ"
      },
      {
        "family": "Arora",
        "given": "Simran"
      },
      {
        "family": "von Arx",
        "given": "Sydney"
      },
      {
        "family": "Bernstein",
        "given": "Michael S."
      },
      {
        "family": "Bohg",
        "given": "Jeannette"
      },
      {
        "family": "Bosselut",
        "given": "Antoine"
      },
      {
        "family": "Brunskill",
        "given": "Emma"
      },
      {
        "family": "Brynjolfsson",
        "given": "Erik"
      },
      {
        "family": "Buch",
        "given": "Shyamal"
      },
      {
        "family": "Card",
        "given": "Dallas"
      },
      {
        "family": "Castellon",
        "given": "Rodrigo"
      },
      {
        "family": "Chatterji",
        "given": "Niladri"
      },
      {
        "family": "Chen",
        "given": "Annie"
      },
      {
        "family": "Creel",
        "given": "Kathleen"
      },
      {
        "family": "Davis",
        "given": "Jared Quincy"
      },
      {
        "family": "Demszky",
        "given": "Dora"
      },
      {
        "family": "Donahue",
        "given": "Chris"
      },
      {
        "family": "Doumbouya",
        "given": "Moussa"
      },
      {
        "family": "Durmus",
        "given": "Esin"
      },
      {
        "family": "Ermon",
        "given": "Stefano"
      },
      {
        "family": "Etchemendy",
        "given": "John"
      },
      {
        "family": "Ethayarajh",
        "given": "Kawin"
      },
      {
        "family": "Fei-Fei",
        "given": "Li"
      },
      {
        "family": "Finn",
        "given": "Chelsea"
      },
      {
        "family": "Gale",
        "given": "Trevor"
      },
      {
        "family": "Gillespie",
        "given": "Lauren"
      },
      {
        "family": "Goel",
        "given": "Karan"
      },
      {
        "family": "Goodman",
        "given": "Noah"
      },
      {
        "family": "Grossman",
        "given": "Shelby"
      },
      {
        "family": "Guha",
        "given": "Neel"
      },
      {
        "family": "Hashimoto",
        "given": "Tatsunori"
      },
      {
        "family": "Henderson",
        "given": "Peter"
      },
      {
        "family": "Hewitt",
        "given": "John"
      },
      {
        "family": "Ho",
        "given": "Daniel E."
      },
      {
        "family": "Hong",
        "given": "Jenny"
      },
      {
        "family": "Hsu",
        "given": "Kyle"
      },
      {
        "family": "Huang",
        "given": "Jing"
      },
      {
        "family": "Icard",
        "given": "Thomas"
      },
      {
        "family": "Jain",
        "given": "Saahil"
      },
      {
        "family": "Jurafsky",
        "given": "Dan"
      },
      {
        "family": "Kalluri",
        "given": "Pratyusha"
      },
      {
        "family": "Karamcheti",
        "given": "Siddharth"
      },
      {
        "family": "Keeling",
        "given": "Geoff"
      },
      {
        "family": "Khani",
        "given": "Fereshte"
      },
      {
        "family": "Khattab",
        "given": "Omar"
      },
      {
        "family": "Koh",
        "given": "Pang Wei"
      },
      {
        "family": "Krass",
        "given": "Mark"
      },
      {
        "family": "Krishna",
        "given": "Ranjay"
      },
      {
        "family": "Kuditipudi",
        "given": "Rohith"
      },
      {
        "family": "Kumar",
        "given": "Ananya"
      },
      {
        "family": "Ladhak",
        "given": "Faisal"
      },
      {
        "family": "Lee",
        "given": "Mina"
      },
      {
        "family": "Lee",
        "given": "Tony"
      },
      {
        "family": "Leskovec",
        "given": "Jure"
      },
      {
        "family": "Levent",
        "given": "Isabelle"
      },
      {
        "family": "Li",
        "given": "Xiang Lisa"
      },
      {
        "family": "Li",
        "given": "Xuechen"
      },
      {
        "family": "Ma",
        "given": "Tengyu"
      },
      {
        "family": "Malik",
        "given": "Ali"
      },
      {
        "family": "Manning",
        "given": "Christopher D."
      },
      {
        "family": "Mirchandani",
        "given": "Suvir"
      },
      {
        "family": "Mitchell",
        "given": "Eric"
      },
      {
        "family": "Munyikwa",
        "given": "Zanele"
      },
      {
        "family": "Nair",
        "given": "Suraj"
      },
      {
        "family": "Narayan",
        "given": "Avanika"
      },
      {
        "family": "Narayanan",
        "given": "Deepak"
      },
      {
        "family": "Newman",
        "given": "Ben"
      },
      {
        "family": "Nie",
        "given": "Allen"
      },
      {
        "family": "Niebles",
        "given": "Juan Carlos"
      },
      {
        "family": "Nilforoshan",
        "given": "Hamed"
      },
      {
        "family": "Nyarko",
        "given": "Julian"
      },
      {
        "family": "Ogut",
        "given": "Giray"
      },
      {
        "family": "Orr",
        "given": "Laurel"
      },
      {
        "family": "Papadimitriou",
        "given": "Isabel"
      },
      {
        "family": "Park",
        "given": "Joon Sung"
      },
      {
        "family": "Piech",
        "given": "Chris"
      },
      {
        "family": "Portelance",
        "given": "Eva"
      },
      {
        "family": "Potts",
        "given": "Christopher"
      },
      {
        "family": "Raghunathan",
        "given": "Aditi"
      },
      {
        "family": "Reich",
        "given": "Rob"
      },
      {
        "family": "Ren",
        "given": "Hongyu"
      },
      {
        "family": "Rong",
        "given": "Frieda"
      },
      {
        "family": "Roohani",
        "given": "Yusuf"
      },
      {
        "family": "Ruiz",
        "given": "Camilo"
      },
      {
        "family": "Ryan",
        "given": "Jack"
      },
      {
        "family": "RÃ©",
        "given": "Christopher"
      },
      {
        "family": "Sadigh",
        "given": "Dorsa"
      },
      {
        "family": "Sagawa",
        "given": "Shiori"
      },
      {
        "family": "Santhanam",
        "given": "Keshav"
      },
      {
        "family": "Shih",
        "given": "Andy"
      },
      {
        "family": "Srinivasan",
        "given": "Krishnan"
      },
      {
        "family": "Tamkin",
        "given": "Alex"
      },
      {
        "family": "Taori",
        "given": "Rohan"
      },
      {
        "family": "Thomas",
        "given": "Armin W."
      },
      {
        "family": "TramÃ¨r",
        "given": "Florian"
      },
      {
        "family": "Wang",
        "given": "Rose E."
      },
      {
        "family": "Wang",
        "given": "William"
      },
      {
        "family": "Wu",
        "given": "Bohan"
      },
      {
        "family": "Wu",
        "given": "Jiajun"
      },
      {
        "family": "Wu",
        "given": "Yuhuai"
      },
      {
        "family": "Xie",
        "given": "Sang Michael"
      },
      {
        "family": "Yasunaga",
        "given": "Michihiro"
      },
      {
        "family": "You",
        "given": "Jiaxuan"
      },
      {
        "family": "Zaharia",
        "given": "Matei"
      },
      {
        "family": "Zhang",
        "given": "Michael"
      },
      {
        "family": "Zhang",
        "given": "Tianyi"
      },
      {
        "family": "Zhang",
        "given": "Xikun"
      },
      {
        "family": "Zhang",
        "given": "Yuhui"
      },
      {
        "family": "Zheng",
        "given": "Lucia"
      },
      {
        "family": "Zhou",
        "given": "Kaitlyn"
      },
      {
        "family": "Liang",
        "given": "Percy"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "abstract": "AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.",
    "DOI": "10.48550/arxiv.2108.07258",
    "publisher": "arXiv",
    "title": "On the Opportunities and Risks of Foundation Models",
    "URL": "https://doi.org/hw3v",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2108.07258"
  },
  {
    "type": "article",
    "id": "grNza1Og",
    "categories": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Lengerich",
        "given": "Ben"
      },
      {
        "family": "Ellington",
        "given": "Caleb"
      },
      {
        "family": "Aragam",
        "given": "Bryon"
      },
      {
        "family": "Xing",
        "given": "Eric P."
      },
      {
        "family": "Kellis",
        "given": "Manolis"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "abstract": "Context-specific Bayesian networks (i.e. directed acyclic graphs, DAGs) identify context-dependent relationships between variables, but the non-convexity induced by the acyclicity requirement makes it difficult to share information between context-specific estimators (e.g. with graph generator functions). For this reason, existing methods for inferring context-specific Bayesian networks have favored breaking datasets into subsamples, limiting statistical power and resolution, and preventing the use of multidimensional and latent contexts. To overcome this challenge, we propose NOTEARS-optimized Mixtures of Archetypal DAGs (NOTMAD). NOTMAD models context-specific Bayesian networks as the output of a function which learns to mix archetypal networks according to sample context. The archetypal networks are estimated jointly with the context-specific networks and do not require any prior knowledge. We encode the acyclicity constraint as a smooth regularization loss which is back-propagated to the mixing function; in this way, NOTMAD shares information between context-specific acyclic graphs, enabling the estimation of Bayesian network structures and parameters at even single-sample resolution. We demonstrate the utility of NOTMAD and sample-specific network inference through analysis and experiments, including patient-specific gene expression networks which correspond to morphological variation in cancer.",
    "DOI": "10.48550/arxiv.2111.01104",
    "publisher": "arXiv",
    "title": "NOTMAD: Estimating Bayesian Networks with Sample-Specific Structures and Parameters",
    "URL": "https://doi.org/gt68jc",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2111.01104"
  },
  {
    "type": "article",
    "id": "k6r0UwSv",
    "categories": [
      "Machine Learning (stat.ML)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Suriyakumar",
        "given": "Vinith M."
      },
      {
        "family": "Ghassemi",
        "given": "Marzyeh"
      },
      {
        "family": "Ustun",
        "given": "Berk"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "abstract": "Machine learning models are often personalized with categorical attributes that are protected, sensitive, self-reported, or costly to acquire. In this work, we show models that are personalized with group attributes can reduce performance at a group level. We propose formal conditions to ensure the \"fair use\" of group attributes in prediction tasks by training one additional model -- i.e., collective preference guarantees to ensure that each group who provides personal data will receive a tailored gain in performance in return. We present sufficient conditions to ensure fair use in empirical risk minimization and characterize failure modes that lead to fair use violations due to standard practices in model development and deployment. We present a comprehensive empirical study of fair use in clinical prediction tasks. Our results demonstrate the prevalence of fair use violations in practice and illustrate simple interventions to mitigate their harm.",
    "DOI": "10.48550/arxiv.2206.02058",
    "publisher": "arXiv",
    "title": "When Personalization Harms: Reconsidering the Use of Group Attributes in Prediction",
    "URL": "https://doi.org/gt68jd",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2206.02058"
  },
  {
    "type": "article",
    "id": "rYveVDKJ",
    "categories": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Hollmann",
        "given": "Noah"
      },
      {
        "family": "MÃ¼ller",
        "given": "Samuel"
      },
      {
        "family": "Eggensperger",
        "given": "Katharina"
      },
      {
        "family": "Hutter",
        "given": "Frank"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "abstract": "We present TabPFN, a trained Transformer that can do supervised classification for small tabular datasets in less than a second, needs no hyperparameter tuning and is competitive with state-of-the-art classification methods. TabPFN performs in-context learning (ICL), it learns to make predictions using sequences of labeled examples (x, f(x)) given in the input, without requiring further parameter updates. TabPFN is fully entailed in the weights of our network, which accepts training and test samples as a set-valued input and yields predictions for the entire test set in a single forward pass. TabPFN is a Prior-Data Fitted Network (PFN) and is trained offline once, to approximate Bayesian inference on synthetic datasets drawn from our prior. This prior incorporates ideas from causal reasoning: It entails a large space of structural causal models with a preference for simple structures. On the 18 datasets in the OpenML-CC18 suite that contain up to 1 000 training data points, up to 100 purely numerical features without missing values, and up to 10 classes, we show that our method clearly outperforms boosted trees and performs on par with complex state-of-the-art AutoML systems with up to 230$\\times$ speedup. This increases to a 5 700$\\times$ speedup when using a GPU. We also validate these results on an additional 67 small numerical datasets from OpenML. We provide all our code, the trained TabPFN, an interactive browser demo and a Colab notebook at https://github.com/automl/TabPFN.",
    "DOI": "10.48550/arxiv.2207.01848",
    "publisher": "arXiv",
    "title": "TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second",
    "URL": "https://doi.org/g9t22b",
    "version": "6",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2207.01848"
  },
  {
    "type": "article",
    "id": "R7y5TKp9",
    "categories": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Garg",
        "given": "Shivam"
      },
      {
        "family": "Tsipras",
        "given": "Dimitris"
      },
      {
        "family": "Liang",
        "given": "Percy"
      },
      {
        "family": "Valiant",
        "given": "Gregory"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "abstract": "In-context learning refers to the ability of a model to condition on a prompt sequence consisting of in-context examples (input-output pairs corresponding to some task) along with a new query input, and generate the corresponding output. Crucially, in-context learning happens only at inference time without any parameter updates to the model. While large language models such as GPT-3 exhibit some ability to perform in-context learning, it is unclear what the relationship is between tasks on which this succeeds and what is present in the training data. To make progress towards understanding in-context learning, we consider the well-defined problem of training a model to in-context learn a function class (e.g., linear functions): that is, given data derived from some functions in the class, can we train a model to in-context learn \"most\" functions from this class? We show empirically that standard Transformers can be trained from scratch to perform in-context learning of linear functions -- that is, the trained model is able to learn unseen linear functions from in-context examples with performance comparable to the optimal least squares estimator. In fact, in-context learning is possible even under two forms of distribution shift: (i) between the training data of the model and inference-time prompts, and (ii) between the in-context examples and the query input during inference. We also show that we can train Transformers to in-context learn more complex function classes -- namely sparse linear functions, two-layer neural networks, and decision trees -- with performance that matches or exceeds task-specific learning algorithms. Our code and models are available at https://github.com/dtsip/in-context-learning .",
    "DOI": "10.48550/arxiv.2208.01066",
    "publisher": "arXiv",
    "title": "What Can Transformers Learn In-Context? A Case Study of Simple Function Classes",
    "URL": "https://doi.org/g9t22c",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2208.01066"
  },
  {
    "type": "article",
    "id": "Xtwwrjzy",
    "categories": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Choi",
        "given": "Kristy"
      },
      {
        "family": "Cundy",
        "given": "Chris"
      },
      {
        "family": "Srivastava",
        "given": "Sanjari"
      },
      {
        "family": "Ermon",
        "given": "Stefano"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "abstract": "Particularly in low-data regimes, an outstanding challenge in machine learning is developing principled techniques for augmenting our models with suitable priors. This is to encourage them to learn in ways that are compatible with our understanding of the world. But in contrast to generic priors such as shrinkage or sparsity, we draw inspiration from the recent successes of large-scale language models (LMs) to construct task-specific priors distilled from the rich knowledge of LMs. Our method, Language Model Priors (LMPriors), incorporates auxiliary natural language metadata about the task -- such as variable names and descriptions -- to encourage downstream model outputs to be consistent with the LM's common-sense reasoning based on the metadata. Empirically, we demonstrate that LMPriors improve model performance in settings where such natural language descriptions are available, and perform well on several tasks that benefit from such prior knowledge, such as feature selection, causal inference, and safe reinforcement learning.",
    "DOI": "10.48550/arxiv.2210.12530",
    "publisher": "arXiv",
    "title": "LMPriors: Pre-Trained Language Models as Task-Specific Priors",
    "URL": "https://doi.org/g9t22d",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2210.12530"
  },
  {
    "type": "article",
    "id": "1AazNaZYl",
    "categories": [
      "Computation and Language (cs.CL)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Su",
        "given": "Hongjin"
      },
      {
        "family": "Shi",
        "given": "Weijia"
      },
      {
        "family": "Kasai",
        "given": "Jungo"
      },
      {
        "family": "Wang",
        "given": "Yizhong"
      },
      {
        "family": "Hu",
        "given": "Yushi"
      },
      {
        "family": "Ostendorf",
        "given": "Mari"
      },
      {
        "family": "Yih",
        "given": "Wen-tau"
      },
      {
        "family": "Smith",
        "given": "Noah A."
      },
      {
        "family": "Zettlemoyer",
        "given": "Luke"
      },
      {
        "family": "Yu",
        "given": "Tao"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "abstract": "We introduce INSTRUCTOR, a new method for computing text embeddings given task instructions: every text input is embedded together with instructions explaining the use case (e.g., task and domain descriptions). Unlike encoders from prior work that are more specialized, INSTRUCTOR is a single embedder that can generate text embeddings tailored to different downstream tasks and domains, without any further training. We first annotate instructions for 330 diverse tasks and train INSTRUCTOR on this multitask mixture with a contrastive loss. We evaluate INSTRUCTOR on 70 embedding evaluation tasks (66 of which are unseen during training), ranging from classification and information retrieval to semantic textual similarity and text generation evaluation. INSTRUCTOR, while having an order of magnitude fewer parameters than the previous best model, achieves state-of-the-art performance, with an average improvement of 3.4% compared to the previous best results on the 70 diverse datasets. Our analysis suggests that INSTRUCTOR is robust to changes in instructions, and that instruction finetuning mitigates the challenge of training a single model on diverse datasets. Our model, code, and data are available at https://instructor-embedding.github.io.",
    "DOI": "10.48550/arxiv.2212.09741",
    "publisher": "arXiv",
    "title": "One Embedder, Any Task: Instruction-Finetuned Text Embeddings",
    "URL": "https://doi.org/g9t22f",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2212.09741"
  },
  {
    "type": "article",
    "id": "16Xv40Ngd",
    "categories": [
      "Computation and Language (cs.CL)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Dai",
        "given": "Damai"
      },
      {
        "family": "Sun",
        "given": "Yutao"
      },
      {
        "family": "Dong",
        "given": "Li"
      },
      {
        "family": "Hao",
        "given": "Yaru"
      },
      {
        "family": "Ma",
        "given": "Shuming"
      },
      {
        "family": "Sui",
        "given": "Zhifang"
      },
      {
        "family": "Wei",
        "given": "Furu"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "abstract": "Large pretrained language models have shown surprising in-context learning (ICL) ability. With a few demonstration input-label pairs, they can predict the label for an unseen input without parameter updates. Despite the great success in performance, its working mechanism still remains an open question. In this paper, we explain language models as meta-optimizers and understand in-context learning as implicit finetuning. Theoretically, we figure out that Transformer attention has a dual form of gradient descent. On top of it, we understand ICL as follows: GPT first produces meta-gradients according to the demonstration examples, and then these meta-gradients are applied to the original GPT to build an ICL model. We comprehensively compare the behaviors of in-context learning and explicit finetuning on real tasks to provide empirical evidence that supports our understanding. Experimental results show that in-context learning behaves similarly to explicit finetuning from multiple perspectives. Inspired by the dual form between Transformer attention and gradient descent, we design a momentum-based attention by analogy with gradient descent with momentum. The improved performance over vanilla attention further supports our understanding from another perspective, and more importantly, shows the potential to utilize our understanding for future model design. The code is available at \\url{https://aka.ms/icl}.",
    "DOI": "10.48550/arxiv.2212.10559",
    "publisher": "arXiv",
    "title": "Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers",
    "URL": "https://doi.org/gtkkf9",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2212.10559"
  },
  {
    "type": "article",
    "id": "11RvF4F7q",
    "categories": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "McInerney",
        "given": "Denis Jered"
      },
      {
        "family": "Young",
        "given": "Geoffrey"
      },
      {
        "family": "van de Meent",
        "given": "Jan-Willem"
      },
      {
        "family": "Wallace",
        "given": "Byron C."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "abstract": "We propose CHiLL (Crafting High-Level Latents), an approach for natural-language specification of features for linear models. CHiLL prompts LLMs with expert-crafted queries to generate interpretable features from health records. The resulting noisy labels are then used to train a simple linear classifier. Generating features based on queries to an LLM can empower physicians to use their domain expertise to craft features that are clinically meaningful for a downstream task of interest, without having to manually extract these from raw EHR. We are motivated by a real-world risk prediction task, but as a reproducible proxy, we use MIMIC-III and MIMIC-CXR data and standard predictive tasks (e.g., 30-day readmission) to evaluate this approach. We find that linear models using automatically extracted features are comparably performant to models using reference features, and provide greater interpretability than linear models using \"Bag-of-Words\" features. We verify that learned feature weights align well with clinical expectations.",
    "DOI": "10.48550/arxiv.2302.12343",
    "publisher": "arXiv",
    "title": "CHiLL: Zero-shot Custom Interpretable Feature Extraction from Clinical Notes with Large Language Models",
    "URL": "https://doi.org/g9t22g",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2302.12343"
  },
  {
    "type": "article",
    "id": "17lpGtuH5",
    "categories": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "OpenAI"
      },
      {
        "family": "Achiam",
        "given": "Josh"
      },
      {
        "family": "Adler",
        "given": "Steven"
      },
      {
        "family": "Agarwal",
        "given": "Sandhini"
      },
      {
        "family": "Ahmad",
        "given": "Lama"
      },
      {
        "family": "Akkaya",
        "given": "Ilge"
      },
      {
        "family": "Aleman",
        "given": "Florencia Leoni"
      },
      {
        "family": "Almeida",
        "given": "Diogo"
      },
      {
        "family": "Altenschmidt",
        "given": "Janko"
      },
      {
        "family": "Altman",
        "given": "Sam"
      },
      {
        "family": "Anadkat",
        "given": "Shyamal"
      },
      {
        "family": "Avila",
        "given": "Red"
      },
      {
        "family": "Babuschkin",
        "given": "Igor"
      },
      {
        "family": "Balaji",
        "given": "Suchir"
      },
      {
        "family": "Balcom",
        "given": "Valerie"
      },
      {
        "family": "Baltescu",
        "given": "Paul"
      },
      {
        "family": "Bao",
        "given": "Haiming"
      },
      {
        "family": "Bavarian",
        "given": "Mohammad"
      },
      {
        "family": "Belgum",
        "given": "Jeff"
      },
      {
        "family": "Bello",
        "given": "Irwan"
      },
      {
        "family": "Berdine",
        "given": "Jake"
      },
      {
        "family": "Bernadett-Shapiro",
        "given": "Gabriel"
      },
      {
        "family": "Berner",
        "given": "Christopher"
      },
      {
        "family": "Bogdonoff",
        "given": "Lenny"
      },
      {
        "family": "Boiko",
        "given": "Oleg"
      },
      {
        "family": "Boyd",
        "given": "Madelaine"
      },
      {
        "family": "Brakman",
        "given": "Anna-Luisa"
      },
      {
        "family": "Brockman",
        "given": "Greg"
      },
      {
        "family": "Brooks",
        "given": "Tim"
      },
      {
        "family": "Brundage",
        "given": "Miles"
      },
      {
        "family": "Button",
        "given": "Kevin"
      },
      {
        "family": "Cai",
        "given": "Trevor"
      },
      {
        "family": "Campbell",
        "given": "Rosie"
      },
      {
        "family": "Cann",
        "given": "Andrew"
      },
      {
        "family": "Carey",
        "given": "Brittany"
      },
      {
        "family": "Carlson",
        "given": "Chelsea"
      },
      {
        "family": "Carmichael",
        "given": "Rory"
      },
      {
        "family": "Chan",
        "given": "Brooke"
      },
      {
        "family": "Chang",
        "given": "Che"
      },
      {
        "family": "Chantzis",
        "given": "Fotis"
      },
      {
        "family": "Chen",
        "given": "Derek"
      },
      {
        "family": "Chen",
        "given": "Sully"
      },
      {
        "family": "Chen",
        "given": "Ruby"
      },
      {
        "family": "Chen",
        "given": "Jason"
      },
      {
        "family": "Chen",
        "given": "Mark"
      },
      {
        "family": "Chess",
        "given": "Ben"
      },
      {
        "family": "Cho",
        "given": "Chester"
      },
      {
        "family": "Chu",
        "given": "Casey"
      },
      {
        "family": "Chung",
        "given": "Hyung Won"
      },
      {
        "family": "Cummings",
        "given": "Dave"
      },
      {
        "family": "Currier",
        "given": "Jeremiah"
      },
      {
        "family": "Dai",
        "given": "Yunxing"
      },
      {
        "family": "Decareaux",
        "given": "Cory"
      },
      {
        "family": "Degry",
        "given": "Thomas"
      },
      {
        "family": "Deutsch",
        "given": "Noah"
      },
      {
        "family": "Deville",
        "given": "Damien"
      },
      {
        "family": "Dhar",
        "given": "Arka"
      },
      {
        "family": "Dohan",
        "given": "David"
      },
      {
        "family": "Dowling",
        "given": "Steve"
      },
      {
        "family": "Dunning",
        "given": "Sheila"
      },
      {
        "family": "Ecoffet",
        "given": "Adrien"
      },
      {
        "family": "Eleti",
        "given": "Atty"
      },
      {
        "family": "Eloundou",
        "given": "Tyna"
      },
      {
        "family": "Farhi",
        "given": "David"
      },
      {
        "family": "Fedus",
        "given": "Liam"
      },
      {
        "family": "Felix",
        "given": "Niko"
      },
      {
        "family": "Fishman",
        "given": "SimÃ³n Posada"
      },
      {
        "family": "Forte",
        "given": "Juston"
      },
      {
        "family": "Fulford",
        "given": "Isabella"
      },
      {
        "family": "Gao",
        "given": "Leo"
      },
      {
        "family": "Georges",
        "given": "Elie"
      },
      {
        "family": "Gibson",
        "given": "Christian"
      },
      {
        "family": "Goel",
        "given": "Vik"
      },
      {
        "family": "Gogineni",
        "given": "Tarun"
      },
      {
        "family": "Goh",
        "given": "Gabriel"
      },
      {
        "family": "Gontijo-Lopes",
        "given": "Rapha"
      },
      {
        "family": "Gordon",
        "given": "Jonathan"
      },
      {
        "family": "Grafstein",
        "given": "Morgan"
      },
      {
        "family": "Gray",
        "given": "Scott"
      },
      {
        "family": "Greene",
        "given": "Ryan"
      },
      {
        "family": "Gross",
        "given": "Joshua"
      },
      {
        "family": "Gu",
        "given": "Shixiang Shane"
      },
      {
        "family": "Guo",
        "given": "Yufei"
      },
      {
        "family": "Hallacy",
        "given": "Chris"
      },
      {
        "family": "Han",
        "given": "Jesse"
      },
      {
        "family": "Harris",
        "given": "Jeff"
      },
      {
        "family": "He",
        "given": "Yuchen"
      },
      {
        "family": "Heaton",
        "given": "Mike"
      },
      {
        "family": "Heidecke",
        "given": "Johannes"
      },
      {
        "family": "Hesse",
        "given": "Chris"
      },
      {
        "family": "Hickey",
        "given": "Alan"
      },
      {
        "family": "Hickey",
        "given": "Wade"
      },
      {
        "family": "Hoeschele",
        "given": "Peter"
      },
      {
        "family": "Houghton",
        "given": "Brandon"
      },
      {
        "family": "Hsu",
        "given": "Kenny"
      },
      {
        "family": "Hu",
        "given": "Shengli"
      },
      {
        "family": "Hu",
        "given": "Xin"
      },
      {
        "family": "Huizinga",
        "given": "Joost"
      },
      {
        "family": "Jain",
        "given": "Shantanu"
      },
      {
        "family": "Jain",
        "given": "Shawn"
      },
      {
        "family": "Jang",
        "given": "Joanne"
      },
      {
        "family": "Jiang",
        "given": "Angela"
      },
      {
        "family": "Jiang",
        "given": "Roger"
      },
      {
        "family": "Jin",
        "given": "Haozhun"
      },
      {
        "family": "Jin",
        "given": "Denny"
      },
      {
        "family": "Jomoto",
        "given": "Shino"
      },
      {
        "family": "Jonn",
        "given": "Billie"
      },
      {
        "family": "Jun",
        "given": "Heewoo"
      },
      {
        "family": "Kaftan",
        "given": "Tomer"
      },
      {
        "family": "Kaiser",
        "given": "Åukasz"
      },
      {
        "family": "Kamali",
        "given": "Ali"
      },
      {
        "family": "Kanitscheider",
        "given": "Ingmar"
      },
      {
        "family": "Keskar",
        "given": "Nitish Shirish"
      },
      {
        "family": "Khan",
        "given": "Tabarak"
      },
      {
        "family": "Kilpatrick",
        "given": "Logan"
      },
      {
        "family": "Kim",
        "given": "Jong Wook"
      },
      {
        "family": "Kim",
        "given": "Christina"
      },
      {
        "family": "Kim",
        "given": "Yongjik"
      },
      {
        "family": "Kirchner",
        "given": "Jan Hendrik"
      },
      {
        "family": "Kiros",
        "given": "Jamie"
      },
      {
        "family": "Knight",
        "given": "Matt"
      },
      {
        "family": "Kokotajlo",
        "given": "Daniel"
      },
      {
        "family": "Kondraciuk",
        "given": "Åukasz"
      },
      {
        "family": "Kondrich",
        "given": "Andrew"
      },
      {
        "family": "Konstantinidis",
        "given": "Aris"
      },
      {
        "family": "Kosic",
        "given": "Kyle"
      },
      {
        "family": "Krueger",
        "given": "Gretchen"
      },
      {
        "family": "Kuo",
        "given": "Vishal"
      },
      {
        "family": "Lampe",
        "given": "Michael"
      },
      {
        "family": "Lan",
        "given": "Ikai"
      },
      {
        "family": "Lee",
        "given": "Teddy"
      },
      {
        "family": "Leike",
        "given": "Jan"
      },
      {
        "family": "Leung",
        "given": "Jade"
      },
      {
        "family": "Levy",
        "given": "Daniel"
      },
      {
        "family": "Li",
        "given": "Chak Ming"
      },
      {
        "family": "Lim",
        "given": "Rachel"
      },
      {
        "family": "Lin",
        "given": "Molly"
      },
      {
        "family": "Lin",
        "given": "Stephanie"
      },
      {
        "family": "Litwin",
        "given": "Mateusz"
      },
      {
        "family": "Lopez",
        "given": "Theresa"
      },
      {
        "family": "Lowe",
        "given": "Ryan"
      },
      {
        "family": "Lue",
        "given": "Patricia"
      },
      {
        "family": "Makanju",
        "given": "Anna"
      },
      {
        "family": "Malfacini",
        "given": "Kim"
      },
      {
        "family": "Manning",
        "given": "Sam"
      },
      {
        "family": "Markov",
        "given": "Todor"
      },
      {
        "family": "Markovski",
        "given": "Yaniv"
      },
      {
        "family": "Martin",
        "given": "Bianca"
      },
      {
        "family": "Mayer",
        "given": "Katie"
      },
      {
        "family": "Mayne",
        "given": "Andrew"
      },
      {
        "family": "McGrew",
        "given": "Bob"
      },
      {
        "family": "McKinney",
        "given": "Scott Mayer"
      },
      {
        "family": "McLeavey",
        "given": "Christine"
      },
      {
        "family": "McMillan",
        "given": "Paul"
      },
      {
        "family": "McNeil",
        "given": "Jake"
      },
      {
        "family": "Medina",
        "given": "David"
      },
      {
        "family": "Mehta",
        "given": "Aalok"
      },
      {
        "family": "Menick",
        "given": "Jacob"
      },
      {
        "family": "Metz",
        "given": "Luke"
      },
      {
        "family": "Mishchenko",
        "given": "Andrey"
      },
      {
        "family": "Mishkin",
        "given": "Pamela"
      },
      {
        "family": "Monaco",
        "given": "Vinnie"
      },
      {
        "family": "Morikawa",
        "given": "Evan"
      },
      {
        "family": "Mossing",
        "given": "Daniel"
      },
      {
        "family": "Mu",
        "given": "Tong"
      },
      {
        "family": "Murati",
        "given": "Mira"
      },
      {
        "family": "Murk",
        "given": "Oleg"
      },
      {
        "family": "MÃ©ly",
        "given": "David"
      },
      {
        "family": "Nair",
        "given": "Ashvin"
      },
      {
        "family": "Nakano",
        "given": "Reiichiro"
      },
      {
        "family": "Nayak",
        "given": "Rajeev"
      },
      {
        "family": "Neelakantan",
        "given": "Arvind"
      },
      {
        "family": "Ngo",
        "given": "Richard"
      },
      {
        "family": "Noh",
        "given": "Hyeonwoo"
      },
      {
        "family": "Ouyang",
        "given": "Long"
      },
      {
        "family": "O'Keefe",
        "given": "Cullen"
      },
      {
        "family": "Pachocki",
        "given": "Jakub"
      },
      {
        "family": "Paino",
        "given": "Alex"
      },
      {
        "family": "Palermo",
        "given": "Joe"
      },
      {
        "family": "Pantuliano",
        "given": "Ashley"
      },
      {
        "family": "Parascandolo",
        "given": "Giambattista"
      },
      {
        "family": "Parish",
        "given": "Joel"
      },
      {
        "family": "Parparita",
        "given": "Emy"
      },
      {
        "family": "Passos",
        "given": "Alex"
      },
      {
        "family": "Pavlov",
        "given": "Mikhail"
      },
      {
        "family": "Peng",
        "given": "Andrew"
      },
      {
        "family": "Perelman",
        "given": "Adam"
      },
      {
        "family": "Peres",
        "given": "Filipe de Avila Belbute"
      },
      {
        "family": "Petrov",
        "given": "Michael"
      },
      {
        "family": "Pinto",
        "given": "Henrique Ponde de Oliveira"
      },
      {
        "family": "Michael"
      },
      {
        "literal": "Pokorny"
      },
      {
        "family": "Pokrass",
        "given": "Michelle"
      },
      {
        "family": "Pong",
        "given": "Vitchyr H."
      },
      {
        "family": "Powell",
        "given": "Tolly"
      },
      {
        "family": "Power",
        "given": "Alethea"
      },
      {
        "family": "Power",
        "given": "Boris"
      },
      {
        "family": "Proehl",
        "given": "Elizabeth"
      },
      {
        "family": "Puri",
        "given": "Raul"
      },
      {
        "family": "Radford",
        "given": "Alec"
      },
      {
        "family": "Rae",
        "given": "Jack"
      },
      {
        "family": "Ramesh",
        "given": "Aditya"
      },
      {
        "family": "Raymond",
        "given": "Cameron"
      },
      {
        "family": "Real",
        "given": "Francis"
      },
      {
        "family": "Rimbach",
        "given": "Kendra"
      },
      {
        "family": "Ross",
        "given": "Carl"
      },
      {
        "family": "Rotsted",
        "given": "Bob"
      },
      {
        "family": "Roussez",
        "given": "Henri"
      },
      {
        "family": "Ryder",
        "given": "Nick"
      },
      {
        "family": "Saltarelli",
        "given": "Mario"
      },
      {
        "family": "Sanders",
        "given": "Ted"
      },
      {
        "family": "Santurkar",
        "given": "Shibani"
      },
      {
        "family": "Sastry",
        "given": "Girish"
      },
      {
        "family": "Schmidt",
        "given": "Heather"
      },
      {
        "family": "Schnurr",
        "given": "David"
      },
      {
        "family": "Schulman",
        "given": "John"
      },
      {
        "family": "Selsam",
        "given": "Daniel"
      },
      {
        "family": "Sheppard",
        "given": "Kyla"
      },
      {
        "family": "Sherbakov",
        "given": "Toki"
      },
      {
        "family": "Shieh",
        "given": "Jessica"
      },
      {
        "family": "Shoker",
        "given": "Sarah"
      },
      {
        "family": "Shyam",
        "given": "Pranav"
      },
      {
        "family": "Sidor",
        "given": "Szymon"
      },
      {
        "family": "Sigler",
        "given": "Eric"
      },
      {
        "family": "Simens",
        "given": "Maddie"
      },
      {
        "family": "Sitkin",
        "given": "Jordan"
      },
      {
        "family": "Slama",
        "given": "Katarina"
      },
      {
        "family": "Sohl",
        "given": "Ian"
      },
      {
        "family": "Sokolowsky",
        "given": "Benjamin"
      },
      {
        "family": "Song",
        "given": "Yang"
      },
      {
        "family": "Staudacher",
        "given": "Natalie"
      },
      {
        "family": "Such",
        "given": "Felipe Petroski"
      },
      {
        "family": "Summers",
        "given": "Natalie"
      },
      {
        "family": "Sutskever",
        "given": "Ilya"
      },
      {
        "family": "Tang",
        "given": "Jie"
      },
      {
        "family": "Tezak",
        "given": "Nikolas"
      },
      {
        "family": "Thompson",
        "given": "Madeleine B."
      },
      {
        "family": "Tillet",
        "given": "Phil"
      },
      {
        "family": "Tootoonchian",
        "given": "Amin"
      },
      {
        "family": "Tseng",
        "given": "Elizabeth"
      },
      {
        "family": "Tuggle",
        "given": "Preston"
      },
      {
        "family": "Turley",
        "given": "Nick"
      },
      {
        "family": "Tworek",
        "given": "Jerry"
      },
      {
        "family": "Uribe",
        "given": "Juan Felipe CerÃ³n"
      },
      {
        "family": "Vallone",
        "given": "Andrea"
      },
      {
        "family": "Vijayvergiya",
        "given": "Arun"
      },
      {
        "family": "Voss",
        "given": "Chelsea"
      },
      {
        "family": "Wainwright",
        "given": "Carroll"
      },
      {
        "family": "Wang",
        "given": "Justin Jay"
      },
      {
        "family": "Wang",
        "given": "Alvin"
      },
      {
        "family": "Wang",
        "given": "Ben"
      },
      {
        "family": "Ward",
        "given": "Jonathan"
      },
      {
        "family": "Wei",
        "given": "Jason"
      },
      {
        "family": "Weinmann",
        "given": "CJ"
      },
      {
        "family": "Welihinda",
        "given": "Akila"
      },
      {
        "family": "Welinder",
        "given": "Peter"
      },
      {
        "family": "Weng",
        "given": "Jiayi"
      },
      {
        "family": "Weng",
        "given": "Lilian"
      },
      {
        "family": "Wiethoff",
        "given": "Matt"
      },
      {
        "family": "Willner",
        "given": "Dave"
      },
      {
        "family": "Winter",
        "given": "Clemens"
      },
      {
        "family": "Wolrich",
        "given": "Samuel"
      },
      {
        "family": "Wong",
        "given": "Hannah"
      },
      {
        "family": "Workman",
        "given": "Lauren"
      },
      {
        "family": "Wu",
        "given": "Sherwin"
      },
      {
        "family": "Wu",
        "given": "Jeff"
      },
      {
        "family": "Wu",
        "given": "Michael"
      },
      {
        "family": "Xiao",
        "given": "Kai"
      },
      {
        "family": "Xu",
        "given": "Tao"
      },
      {
        "family": "Yoo",
        "given": "Sarah"
      },
      {
        "family": "Yu",
        "given": "Kevin"
      },
      {
        "family": "Yuan",
        "given": "Qiming"
      },
      {
        "family": "Zaremba",
        "given": "Wojciech"
      },
      {
        "family": "Zellers",
        "given": "Rowan"
      },
      {
        "family": "Zhang",
        "given": "Chong"
      },
      {
        "family": "Zhang",
        "given": "Marvin"
      },
      {
        "family": "Zhao",
        "given": "Shengjia"
      },
      {
        "family": "Zheng",
        "given": "Tianhao"
      },
      {
        "family": "Zhuang",
        "given": "Juntang"
      },
      {
        "family": "Zhuk",
        "given": "William"
      },
      {
        "family": "Zoph",
        "given": "Barret"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",
    "DOI": "10.48550/arxiv.2303.08774",
    "publisher": "arXiv",
    "title": "GPT-4 Technical Report",
    "URL": "https://doi.org/grx4cb",
    "version": "6",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2303.08774"
  },
  {
    "type": "article",
    "id": "10hcHcmAG",
    "categories": [
      "Computation and Language (cs.CL)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Patel",
        "given": "Ajay"
      },
      {
        "family": "Rao",
        "given": "Delip"
      },
      {
        "family": "Kothary",
        "given": "Ansh"
      },
      {
        "family": "McKeown",
        "given": "Kathleen"
      },
      {
        "family": "Callison-Burch",
        "given": "Chris"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "abstract": "Style representation learning builds content-independent representations of author style in text. Stylometry, the analysis of style in text, is often performed by expert forensic linguists and no large dataset of stylometric annotations exists for training. Current style representation learning uses neural methods to disentangle style from content to create style vectors, however, these approaches result in uninterpretable representations, complicating their usage in downstream applications like authorship attribution where auditing and explainability is critical. In this work, we use prompting to perform stylometry on a large number of texts to create a synthetic dataset and train human-interpretable style representations we call LISA embeddings. We release our synthetic stylometry dataset and our interpretable style models as resources.",
    "DOI": "10.48550/arxiv.2305.12696",
    "publisher": "arXiv",
    "title": "Learning Interpretable Style Embeddings via Prompting LLMs",
    "URL": "https://doi.org/g9t22h",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2305.12696"
  },
  {
    "type": "article",
    "id": "zWwbz3cX",
    "categories": [
      "Computation and Language (cs.CL)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Shen",
        "given": "Sheng"
      },
      {
        "family": "Hou",
        "given": "Le"
      },
      {
        "family": "Zhou",
        "given": "Yanqi"
      },
      {
        "family": "Du",
        "given": "Nan"
      },
      {
        "family": "Longpre",
        "given": "Shayne"
      },
      {
        "family": "Wei",
        "given": "Jason"
      },
      {
        "family": "Chung",
        "given": "Hyung Won"
      },
      {
        "family": "Zoph",
        "given": "Barret"
      },
      {
        "family": "Fedus",
        "given": "William"
      },
      {
        "family": "Chen",
        "given": "Xinyun"
      },
      {
        "family": "Vu",
        "given": "Tu"
      },
      {
        "family": "Wu",
        "given": "Yuexin"
      },
      {
        "family": "Chen",
        "given": "Wuyang"
      },
      {
        "family": "Webson",
        "given": "Albert"
      },
      {
        "family": "Li",
        "given": "Yunxuan"
      },
      {
        "family": "Zhao",
        "given": "Vincent"
      },
      {
        "family": "Yu",
        "given": "Hongkun"
      },
      {
        "family": "Keutzer",
        "given": "Kurt"
      },
      {
        "family": "Darrell",
        "given": "Trevor"
      },
      {
        "family": "Zhou",
        "given": "Denny"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "abstract": "Sparse Mixture-of-Experts (MoE) is a neural architecture design that can be utilized to add learnable parameters to Large Language Models (LLMs) without increasing inference cost. Instruction tuning is a technique for training LLMs to follow instructions. We advocate combining these two approaches, as we find that MoE models benefit more from instruction tuning than dense models. In particular, we conduct empirical studies across three experimental setups: (i) Direct finetuning on individual downstream tasks devoid of instruction tuning; (ii) Instructiontuning followed by in-context few-shot or zero-shot generalization on downstream tasks; and (iii) Instruction tuning supplemented by further finetuning on individual downstream tasks. In the first scenario, MoE models overall underperform dense models of identical computational capacity. This narrative, however, dramatically changes with the introduction of instruction tuning (second and third scenario), used independently or in conjunction with task-specific finetuning. Our most powerful model, FLAN-MOE-32B, surpasses the performance of FLAN-PALM-62B on four benchmark tasks, while using only a third of the FLOPs. The advancements embodied byFLAN-MOE inspire a reevaluation of the design principles of large-scale, high-performance language models in the framework of task-agnostic learning.",
    "DOI": "10.48550/arxiv.2305.14705",
    "publisher": "arXiv",
    "title": "Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models",
    "URL": "https://doi.org/g9t22j",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2305.14705"
  },
  {
    "type": "article",
    "id": "nYipTPML",
    "categories": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Deuschel",
        "given": "Jannik"
      },
      {
        "family": "Ellington",
        "given": "Caleb N."
      },
      {
        "family": "Luo",
        "given": "Yingtao"
      },
      {
        "family": "Lengerich",
        "given": "Benjamin J."
      },
      {
        "family": "Friederich",
        "given": "Pascal"
      },
      {
        "family": "Xing",
        "given": "Eric P."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "abstract": "Interpretable policy learning seeks to estimate intelligible decision policies from observed actions; however, existing models force a tradeoff between accuracy and interpretability, limiting data-driven interpretations of human decision-making processes. Fundamentally, existing approaches are burdened by this tradeoff because they represent the underlying decision process as a universal policy, when in fact human decisions are dynamic and can change drastically under different contexts. Thus, we develop Contextualized Policy Recovery (CPR), which re-frames the problem of modeling complex decision processes as a multi-task learning problem, where each context poses a unique task and complex decision policies can be constructed piece-wise from many simple context-specific policies. CPR models each context-specific policy as a linear map, and generates new policy models $\\textit{on-demand}$ as contexts are updated with new observations. We provide two flavors of the CPR framework: one focusing on exact local interpretability, and one retaining full global interpretability. We assess CPR through studies on simulated and real data, achieving state-of-the-art performance on predicting antibiotic prescription in intensive care units ($+22\\%$ AUROC vs. previous SOTA) and predicting MRI prescription for Alzheimer's patients ($+7.7\\%$ AUROC vs. previous SOTA). With this improvement, CPR closes the accuracy gap between interpretable and black-box methods, allowing high-resolution exploration and analysis of context-specific decision models.",
    "DOI": "10.48550/arxiv.2310.07918",
    "publisher": "arXiv",
    "title": "Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning",
    "URL": "https://doi.org/gt68jf",
    "version": "4",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2310.07918"
  },
  {
    "type": "article",
    "id": "HYsEq2UQ",
    "categories": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Lengerich",
        "given": "Benjamin"
      },
      {
        "family": "Ellington",
        "given": "Caleb N."
      },
      {
        "family": "Rubbi",
        "given": "Andrea"
      },
      {
        "family": "Kellis",
        "given": "Manolis"
      },
      {
        "family": "Xing",
        "given": "Eric P."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "abstract": "We examine Contextualized Machine Learning (ML), a paradigm for learning heterogeneous and context-dependent effects. Contextualized ML estimates heterogeneous functions by applying deep learning to the meta-relationship between contextual information and context-specific parametric models. This is a form of varying-coefficient modeling that unifies existing frameworks including cluster analysis and cohort modeling by introducing two reusable concepts: a context encoder which translates sample context into model parameters, and sample-specific model which operates on sample predictors. We review the process of developing contextualized models, nonparametric inference from contextualized models, and identifiability conditions of contextualized models. Finally, we present the open-source PyTorch package ContextualizedML.",
    "DOI": "10.48550/arxiv.2310.11340",
    "publisher": "arXiv",
    "title": "Contextualized Machine Learning",
    "URL": "https://doi.org/gt68jg",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2310.11340"
  },
  {
    "type": "article",
    "id": "R3AtGoca",
    "categories": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences",
      "FOS: Electrical engineering, electronic engineering, information engineering",
      "FOS: Electrical engineering, electronic engineering, information engineering"
    ],
    "author": [
      {
        "family": "Sekeran",
        "given": "Maya"
      },
      {
        "family": "Syed",
        "given": "Arslan Ali"
      },
      {
        "family": "Lindner",
        "given": "Johannes"
      },
      {
        "family": "Margreiter",
        "given": "Martin"
      },
      {
        "family": "Bogenberger",
        "given": "Klaus"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "abstract": "Lane-free traffic (LFT) is a new traffic system that relies on connected and automated vehicles (CAV) to increase road capacity and utilization by removing traditional lane markings using coordinated maneuvering of CAVs in LFT strategies. LFT is based on two main principles: upstream nudging and vehicles moving without adhering to any lane markings. By leveraging CAV capabilities to communicate and exchange information, LFT represents a promising future traffic system. While current research uses LFT simulations in two-dimensional space, driving simulators are necessary to investigate human behavior and perceived safety in LFT. This paper proposes a conceptual framework for LFT driving simulations and describes the assumptions, requirements, and recent technological developments that make it possible to investigate the human perspective and acceptance of LFT. Additionally, we propose a scenario matrix that can act as a test guide to building driving simulation scenarios for the LFT.",
    "DOI": "10.48550/arxiv.2311.16142",
    "publisher": "arXiv",
    "title": "Investigating Lane-Free Traffic with a Dynamic Driving Simulator",
    "URL": "https://doi.org/g93rnq",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2311.16142"
  },
  {
    "type": "article",
    "id": "9S6tI5yv",
    "categories": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Sristi",
        "given": "Ram Dyuthi"
      },
      {
        "family": "Lindenbaum",
        "given": "Ofir"
      },
      {
        "family": "Lifshitz",
        "given": "Shira"
      },
      {
        "family": "Lavzin",
        "given": "Maria"
      },
      {
        "family": "Schiller",
        "given": "Jackie"
      },
      {
        "family": "Mishne",
        "given": "Gal"
      },
      {
        "family": "Benisty",
        "given": "Hadas"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "abstract": "Feature selection is a crucial tool in machine learning and is widely applied across various scientific disciplines. Traditional supervised methods generally identify a universal set of informative features for the entire population. However, feature relevance often varies with context, while the context itself may not directly affect the outcome variable. Here, we propose a novel architecture for contextual feature selection where the subset of selected features is conditioned on the value of context variables. Our new approach, Conditional Stochastic Gates (c-STG), models the importance of features using conditional Bernoulli variables whose parameters are predicted based on contextual variables. We introduce a hypernetwork that maps context variables to feature selection parameters to learn the context-dependent gates along with a prediction model. We further present a theoretical analysis of our model, indicating that it can improve performance and flexibility over population-level methods in complex feature selection settings. Finally, we conduct an extensive benchmark using simulated and real-world datasets across multiple domains demonstrating that c-STG can lead to improved feature selection capabilities while enhancing prediction accuracy and interpretability.",
    "DOI": "10.48550/arxiv.2312.14254",
    "publisher": "arXiv",
    "title": "Contextual Feature Selection with Conditional Stochastic Gates",
    "URL": "https://doi.org/gt68jh",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2312.14254"
  },
  {
    "type": "article",
    "id": "UskQdlu3",
    "categories": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Wu",
        "given": "Xun"
      },
      {
        "family": "Huang",
        "given": "Shaohan"
      },
      {
        "family": "Wei",
        "given": "Furu"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2024
        ]
      ]
    },
    "abstract": "LoRA has gained widespread acceptance in the fine-tuning of large pre-trained models to cater to a diverse array of downstream tasks, showcasing notable effectiveness and efficiency, thereby solidifying its position as one of the most prevalent fine-tuning techniques. Due to the modular nature of LoRA's plug-and-play plugins, researchers have delved into the amalgamation of multiple LoRAs to empower models to excel across various downstream tasks. Nonetheless, extant approaches for LoRA fusion grapple with inherent challenges. Direct arithmetic merging may result in the loss of the original pre-trained model's generative capabilities or the distinct identity of LoRAs, thereby yielding suboptimal outcomes. On the other hand, Reference tuning-based fusion exhibits limitations concerning the requisite flexibility for the effective combination of multiple LoRAs. In response to these challenges, this paper introduces the Mixture of LoRA Experts (MoLE) approach, which harnesses hierarchical control and unfettered branch selection. The MoLE approach not only achieves superior LoRA fusion performance in comparison to direct arithmetic merging but also retains the crucial flexibility for combining LoRAs effectively. Extensive experimental evaluations conducted in both the Natural Language Processing (NLP) and Vision &amp; Language (V&amp;L) domains substantiate the efficacy of MoLE.",
    "DOI": "10.48550/arxiv.2404.13628",
    "publisher": "arXiv",
    "title": "Mixture of LoRA Experts",
    "URL": "https://doi.org/g93rnr",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2404.13628"
  },
  {
    "type": "article",
    "id": "yWg7tQr1",
    "categories": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Grattafiori",
        "given": "Aaron"
      },
      {
        "family": "Dubey",
        "given": "Abhimanyu"
      },
      {
        "family": "Jauhri",
        "given": "Abhinav"
      },
      {
        "family": "Pandey",
        "given": "Abhinav"
      },
      {
        "family": "Kadian",
        "given": "Abhishek"
      },
      {
        "family": "Al-Dahle",
        "given": "Ahmad"
      },
      {
        "family": "Letman",
        "given": "Aiesha"
      },
      {
        "family": "Mathur",
        "given": "Akhil"
      },
      {
        "family": "Schelten",
        "given": "Alan"
      },
      {
        "family": "Vaughan",
        "given": "Alex"
      },
      {
        "family": "Yang",
        "given": "Amy"
      },
      {
        "family": "Fan",
        "given": "Angela"
      },
      {
        "family": "Goyal",
        "given": "Anirudh"
      },
      {
        "family": "Hartshorn",
        "given": "Anthony"
      },
      {
        "family": "Yang",
        "given": "Aobo"
      },
      {
        "family": "Mitra",
        "given": "Archi"
      },
      {
        "family": "Sravankumar",
        "given": "Archie"
      },
      {
        "family": "Korenev",
        "given": "Artem"
      },
      {
        "family": "Hinsvark",
        "given": "Arthur"
      },
      {
        "family": "Rao",
        "given": "Arun"
      },
      {
        "family": "Zhang",
        "given": "Aston"
      },
      {
        "family": "Rodriguez",
        "given": "Aurelien"
      },
      {
        "family": "Gregerson",
        "given": "Austen"
      },
      {
        "family": "Spataru",
        "given": "Ava"
      },
      {
        "family": "Roziere",
        "given": "Baptiste"
      },
      {
        "family": "Biron",
        "given": "Bethany"
      },
      {
        "family": "Tang",
        "given": "Binh"
      },
      {
        "family": "Chern",
        "given": "Bobbie"
      },
      {
        "family": "Caucheteux",
        "given": "Charlotte"
      },
      {
        "family": "Nayak",
        "given": "Chaya"
      },
      {
        "family": "Bi",
        "given": "Chloe"
      },
      {
        "family": "Marra",
        "given": "Chris"
      },
      {
        "family": "McConnell",
        "given": "Chris"
      },
      {
        "family": "Keller",
        "given": "Christian"
      },
      {
        "family": "Touret",
        "given": "Christophe"
      },
      {
        "family": "Wu",
        "given": "Chunyang"
      },
      {
        "family": "Wong",
        "given": "Corinne"
      },
      {
        "family": "Ferrer",
        "given": "Cristian Canton"
      },
      {
        "family": "Nikolaidis",
        "given": "Cyrus"
      },
      {
        "family": "Allonsius",
        "given": "Damien"
      },
      {
        "family": "Song",
        "given": "Daniel"
      },
      {
        "family": "Pintz",
        "given": "Danielle"
      },
      {
        "family": "Livshits",
        "given": "Danny"
      },
      {
        "family": "Wyatt",
        "given": "Danny"
      },
      {
        "family": "Esiobu",
        "given": "David"
      },
      {
        "family": "Choudhary",
        "given": "Dhruv"
      },
      {
        "family": "Mahajan",
        "given": "Dhruv"
      },
      {
        "family": "Garcia-Olano",
        "given": "Diego"
      },
      {
        "family": "Perino",
        "given": "Diego"
      },
      {
        "family": "Hupkes",
        "given": "Dieuwke"
      },
      {
        "family": "Lakomkin",
        "given": "Egor"
      },
      {
        "family": "AlBadawy",
        "given": "Ehab"
      },
      {
        "family": "Lobanova",
        "given": "Elina"
      },
      {
        "family": "Dinan",
        "given": "Emily"
      },
      {
        "family": "Smith",
        "given": "Eric Michael"
      },
      {
        "family": "Radenovic",
        "given": "Filip"
      },
      {
        "family": "GuzmÃ¡n",
        "given": "Francisco"
      },
      {
        "family": "Zhang",
        "given": "Frank"
      },
      {
        "family": "Synnaeve",
        "given": "Gabriel"
      },
      {
        "family": "Lee",
        "given": "Gabrielle"
      },
      {
        "family": "Anderson",
        "given": "Georgia Lewis"
      },
      {
        "family": "Thattai",
        "given": "Govind"
      },
      {
        "family": "Nail",
        "given": "Graeme"
      },
      {
        "family": "Mialon",
        "given": "Gregoire"
      },
      {
        "family": "Pang",
        "given": "Guan"
      },
      {
        "family": "Cucurell",
        "given": "Guillem"
      },
      {
        "family": "Nguyen",
        "given": "Hailey"
      },
      {
        "family": "Korevaar",
        "given": "Hannah"
      },
      {
        "family": "Xu",
        "given": "Hu"
      },
      {
        "family": "Touvron",
        "given": "Hugo"
      },
      {
        "family": "Zarov",
        "given": "Iliyan"
      },
      {
        "family": "Ibarra",
        "given": "Imanol Arrieta"
      },
      {
        "family": "Kloumann",
        "given": "Isabel"
      },
      {
        "family": "Misra",
        "given": "Ishan"
      },
      {
        "family": "Evtimov",
        "given": "Ivan"
      },
      {
        "family": "Zhang",
        "given": "Jack"
      },
      {
        "family": "Copet",
        "given": "Jade"
      },
      {
        "family": "Lee",
        "given": "Jaewon"
      },
      {
        "family": "Geffert",
        "given": "Jan"
      },
      {
        "family": "Vranes",
        "given": "Jana"
      },
      {
        "family": "Park",
        "given": "Jason"
      },
      {
        "family": "Mahadeokar",
        "given": "Jay"
      },
      {
        "family": "Shah",
        "given": "Jeet"
      },
      {
        "family": "van der Linde",
        "given": "Jelmer"
      },
      {
        "family": "Billock",
        "given": "Jennifer"
      },
      {
        "family": "Hong",
        "given": "Jenny"
      },
      {
        "family": "Lee",
        "given": "Jenya"
      },
      {
        "family": "Fu",
        "given": "Jeremy"
      },
      {
        "family": "Chi",
        "given": "Jianfeng"
      },
      {
        "family": "Huang",
        "given": "Jianyu"
      },
      {
        "family": "Liu",
        "given": "Jiawen"
      },
      {
        "family": "Wang",
        "given": "Jie"
      },
      {
        "family": "Yu",
        "given": "Jiecao"
      },
      {
        "family": "Bitton",
        "given": "Joanna"
      },
      {
        "family": "Spisak",
        "given": "Joe"
      },
      {
        "family": "Park",
        "given": "Jongsoo"
      },
      {
        "family": "Rocca",
        "given": "Joseph"
      },
      {
        "family": "Johnstun",
        "given": "Joshua"
      },
      {
        "family": "Saxe",
        "given": "Joshua"
      },
      {
        "family": "Jia",
        "given": "Junteng"
      },
      {
        "family": "Alwala",
        "given": "Kalyan Vasuden"
      },
      {
        "family": "Prasad",
        "given": "Karthik"
      },
      {
        "family": "Upasani",
        "given": "Kartikeya"
      },
      {
        "family": "Plawiak",
        "given": "Kate"
      },
      {
        "family": "Li",
        "given": "Ke"
      },
      {
        "family": "Heafield",
        "given": "Kenneth"
      },
      {
        "family": "Stone",
        "given": "Kevin"
      },
      {
        "family": "El-Arini",
        "given": "Khalid"
      },
      {
        "family": "Iyer",
        "given": "Krithika"
      },
      {
        "family": "Malik",
        "given": "Kshitiz"
      },
      {
        "family": "Chiu",
        "given": "Kuenley"
      },
      {
        "family": "Bhalla",
        "given": "Kunal"
      },
      {
        "family": "Lakhotia",
        "given": "Kushal"
      },
      {
        "family": "Rantala-Yeary",
        "given": "Lauren"
      },
      {
        "family": "van der Maaten",
        "given": "Laurens"
      },
      {
        "family": "Chen",
        "given": "Lawrence"
      },
      {
        "family": "Tan",
        "given": "Liang"
      },
      {
        "family": "Jenkins",
        "given": "Liz"
      },
      {
        "family": "Martin",
        "given": "Louis"
      },
      {
        "family": "Madaan",
        "given": "Lovish"
      },
      {
        "family": "Malo",
        "given": "Lubo"
      },
      {
        "family": "Blecher",
        "given": "Lukas"
      },
      {
        "family": "Landzaat",
        "given": "Lukas"
      },
      {
        "family": "de Oliveira",
        "given": "Luke"
      },
      {
        "family": "Muzzi",
        "given": "Madeline"
      },
      {
        "family": "Pasupuleti",
        "given": "Mahesh"
      },
      {
        "family": "Singh",
        "given": "Mannat"
      },
      {
        "family": "Paluri",
        "given": "Manohar"
      },
      {
        "family": "Kardas",
        "given": "Marcin"
      },
      {
        "family": "Tsimpoukelli",
        "given": "Maria"
      },
      {
        "family": "Oldham",
        "given": "Mathew"
      },
      {
        "family": "Rita",
        "given": "Mathieu"
      },
      {
        "family": "Pavlova",
        "given": "Maya"
      },
      {
        "family": "Kambadur",
        "given": "Melanie"
      },
      {
        "family": "Lewis",
        "given": "Mike"
      },
      {
        "family": "Si",
        "given": "Min"
      },
      {
        "family": "Singh",
        "given": "Mitesh Kumar"
      },
      {
        "family": "Hassan",
        "given": "Mona"
      },
      {
        "family": "Goyal",
        "given": "Naman"
      },
      {
        "family": "Torabi",
        "given": "Narjes"
      },
      {
        "family": "Bashlykov",
        "given": "Nikolay"
      },
      {
        "family": "Bogoychev",
        "given": "Nikolay"
      },
      {
        "family": "Chatterji",
        "given": "Niladri"
      },
      {
        "family": "Zhang",
        "given": "Ning"
      },
      {
        "family": "Duchenne",
        "given": "Olivier"
      },
      {
        "family": "Ãelebi",
        "given": "Onur"
      },
      {
        "family": "Alrassy",
        "given": "Patrick"
      },
      {
        "family": "Zhang",
        "given": "Pengchuan"
      },
      {
        "family": "Li",
        "given": "Pengwei"
      },
      {
        "family": "Vasic",
        "given": "Petar"
      },
      {
        "family": "Weng",
        "given": "Peter"
      },
      {
        "family": "Bhargava",
        "given": "Prajjwal"
      },
      {
        "family": "Dubal",
        "given": "Pratik"
      },
      {
        "family": "Krishnan",
        "given": "Praveen"
      },
      {
        "family": "Koura",
        "given": "Punit Singh"
      },
      {
        "family": "Xu",
        "given": "Puxin"
      },
      {
        "family": "He",
        "given": "Qing"
      },
      {
        "family": "Dong",
        "given": "Qingxiao"
      },
      {
        "family": "Srinivasan",
        "given": "Ragavan"
      },
      {
        "family": "Ganapathy",
        "given": "Raj"
      },
      {
        "family": "Calderer",
        "given": "Ramon"
      },
      {
        "family": "Cabral",
        "given": "Ricardo Silveira"
      },
      {
        "family": "Stojnic",
        "given": "Robert"
      },
      {
        "family": "Raileanu",
        "given": "Roberta"
      },
      {
        "family": "Maheswari",
        "given": "Rohan"
      },
      {
        "family": "Girdhar",
        "given": "Rohit"
      },
      {
        "family": "Patel",
        "given": "Rohit"
      },
      {
        "family": "Sauvestre",
        "given": "Romain"
      },
      {
        "family": "Polidoro",
        "given": "Ronnie"
      },
      {
        "family": "Sumbaly",
        "given": "Roshan"
      },
      {
        "family": "Taylor",
        "given": "Ross"
      },
      {
        "family": "Silva",
        "given": "Ruan"
      },
      {
        "family": "Hou",
        "given": "Rui"
      },
      {
        "family": "Wang",
        "given": "Rui"
      },
      {
        "family": "Hosseini",
        "given": "Saghar"
      },
      {
        "family": "Chennabasappa",
        "given": "Sahana"
      },
      {
        "family": "Singh",
        "given": "Sanjay"
      },
      {
        "family": "Bell",
        "given": "Sean"
      },
      {
        "family": "Kim",
        "given": "Seohyun Sonia"
      },
      {
        "family": "Edunov",
        "given": "Sergey"
      },
      {
        "family": "Nie",
        "given": "Shaoliang"
      },
      {
        "family": "Narang",
        "given": "Sharan"
      },
      {
        "family": "Raparthy",
        "given": "Sharath"
      },
      {
        "family": "Shen",
        "given": "Sheng"
      },
      {
        "family": "Wan",
        "given": "Shengye"
      },
      {
        "family": "Bhosale",
        "given": "Shruti"
      },
      {
        "family": "Zhang",
        "given": "Shun"
      },
      {
        "family": "Vandenhende",
        "given": "Simon"
      },
      {
        "family": "Batra",
        "given": "Soumya"
      },
      {
        "family": "Whitman",
        "given": "Spencer"
      },
      {
        "family": "Sootla",
        "given": "Sten"
      },
      {
        "family": "Collot",
        "given": "Stephane"
      },
      {
        "family": "Gururangan",
        "given": "Suchin"
      },
      {
        "family": "Borodinsky",
        "given": "Sydney"
      },
      {
        "family": "Herman",
        "given": "Tamar"
      },
      {
        "family": "Fowler",
        "given": "Tara"
      },
      {
        "family": "Sheasha",
        "given": "Tarek"
      },
      {
        "family": "Georgiou",
        "given": "Thomas"
      },
      {
        "family": "Scialom",
        "given": "Thomas"
      },
      {
        "family": "Speckbacher",
        "given": "Tobias"
      },
      {
        "family": "Mihaylov",
        "given": "Todor"
      },
      {
        "family": "Xiao",
        "given": "Tong"
      },
      {
        "family": "Karn",
        "given": "Ujjwal"
      },
      {
        "family": "Goswami",
        "given": "Vedanuj"
      },
      {
        "family": "Gupta",
        "given": "Vibhor"
      },
      {
        "family": "Ramanathan",
        "given": "Vignesh"
      },
      {
        "family": "Kerkez",
        "given": "Viktor"
      },
      {
        "family": "Gonguet",
        "given": "Vincent"
      },
      {
        "family": "Do",
        "given": "Virginie"
      },
      {
        "family": "Vogeti",
        "given": "Vish"
      },
      {
        "family": "Albiero",
        "given": "VÃ­tor"
      },
      {
        "family": "Petrovic",
        "given": "Vladan"
      },
      {
        "family": "Chu",
        "given": "Weiwei"
      },
      {
        "family": "Xiong",
        "given": "Wenhan"
      },
      {
        "family": "Fu",
        "given": "Wenyin"
      },
      {
        "family": "Meers",
        "given": "Whitney"
      },
      {
        "family": "Martinet",
        "given": "Xavier"
      },
      {
        "family": "Wang",
        "given": "Xiaodong"
      },
      {
        "family": "Wang",
        "given": "Xiaofang"
      },
      {
        "family": "Tan",
        "given": "Xiaoqing Ellen"
      },
      {
        "family": "Xia",
        "given": "Xide"
      },
      {
        "family": "Xie",
        "given": "Xinfeng"
      },
      {
        "family": "Jia",
        "given": "Xuchao"
      },
      {
        "family": "Wang",
        "given": "Xuewei"
      },
      {
        "family": "Goldschlag",
        "given": "Yaelle"
      },
      {
        "family": "Gaur",
        "given": "Yashesh"
      },
      {
        "family": "Babaei",
        "given": "Yasmine"
      },
      {
        "family": "Wen",
        "given": "Yi"
      },
      {
        "family": "Song",
        "given": "Yiwen"
      },
      {
        "family": "Zhang",
        "given": "Yuchen"
      },
      {
        "family": "Li",
        "given": "Yue"
      },
      {
        "family": "Mao",
        "given": "Yuning"
      },
      {
        "family": "Coudert",
        "given": "Zacharie Delpierre"
      },
      {
        "family": "Yan",
        "given": "Zheng"
      },
      {
        "family": "Chen",
        "given": "Zhengxing"
      },
      {
        "family": "Papakipos",
        "given": "Zoe"
      },
      {
        "family": "Singh",
        "given": "Aaditya"
      },
      {
        "family": "Srivastava",
        "given": "Aayushi"
      },
      {
        "family": "Jain",
        "given": "Abha"
      },
      {
        "family": "Kelsey",
        "given": "Adam"
      },
      {
        "family": "Shajnfeld",
        "given": "Adam"
      },
      {
        "family": "Gangidi",
        "given": "Adithya"
      },
      {
        "family": "Victoria",
        "given": "Adolfo"
      },
      {
        "family": "Goldstand",
        "given": "Ahuva"
      },
      {
        "family": "Menon",
        "given": "Ajay"
      },
      {
        "family": "Sharma",
        "given": "Ajay"
      },
      {
        "family": "Boesenberg",
        "given": "Alex"
      },
      {
        "family": "Baevski",
        "given": "Alexei"
      },
      {
        "family": "Feinstein",
        "given": "Allie"
      },
      {
        "family": "Kallet",
        "given": "Amanda"
      },
      {
        "family": "Sangani",
        "given": "Amit"
      },
      {
        "family": "Teo",
        "given": "Amos"
      },
      {
        "family": "Yunus",
        "given": "Anam"
      },
      {
        "family": "Lupu",
        "given": "Andrei"
      },
      {
        "family": "Alvarado",
        "given": "Andres"
      },
      {
        "family": "Caples",
        "given": "Andrew"
      },
      {
        "family": "Gu",
        "given": "Andrew"
      },
      {
        "family": "Ho",
        "given": "Andrew"
      },
      {
        "family": "Poulton",
        "given": "Andrew"
      },
      {
        "family": "Ryan",
        "given": "Andrew"
      },
      {
        "family": "Ramchandani",
        "given": "Ankit"
      },
      {
        "family": "Dong",
        "given": "Annie"
      },
      {
        "family": "Franco",
        "given": "Annie"
      },
      {
        "family": "Goyal",
        "given": "Anuj"
      },
      {
        "family": "Saraf",
        "given": "Aparajita"
      },
      {
        "family": "Chowdhury",
        "given": "Arkabandhu"
      },
      {
        "family": "Gabriel",
        "given": "Ashley"
      },
      {
        "family": "Bharambe",
        "given": "Ashwin"
      },
      {
        "family": "Eisenman",
        "given": "Assaf"
      },
      {
        "family": "Yazdan",
        "given": "Azadeh"
      },
      {
        "family": "James",
        "given": "Beau"
      },
      {
        "family": "Maurer",
        "given": "Ben"
      },
      {
        "family": "Leonhardi",
        "given": "Benjamin"
      },
      {
        "family": "Huang",
        "given": "Bernie"
      },
      {
        "family": "Loyd",
        "given": "Beth"
      },
      {
        "family": "De Paola",
        "given": "Beto"
      },
      {
        "family": "Paranjape",
        "given": "Bhargavi"
      },
      {
        "family": "Liu",
        "given": "Bing"
      },
      {
        "family": "Wu",
        "given": "Bo"
      },
      {
        "family": "Ni",
        "given": "Boyu"
      },
      {
        "family": "Hancock",
        "given": "Braden"
      },
      {
        "family": "Wasti",
        "given": "Bram"
      },
      {
        "family": "Spence",
        "given": "Brandon"
      },
      {
        "family": "Stojkovic",
        "given": "Brani"
      },
      {
        "family": "Gamido",
        "given": "Brian"
      },
      {
        "family": "Montalvo",
        "given": "Britt"
      },
      {
        "family": "Parker",
        "given": "Carl"
      },
      {
        "family": "Burton",
        "given": "Carly"
      },
      {
        "family": "Mejia",
        "given": "Catalina"
      },
      {
        "family": "Liu",
        "given": "Ce"
      },
      {
        "family": "Wang",
        "given": "Changhan"
      },
      {
        "family": "Kim",
        "given": "Changkyu"
      },
      {
        "family": "Zhou",
        "given": "Chao"
      },
      {
        "family": "Hu",
        "given": "Chester"
      },
      {
        "family": "Chu",
        "given": "Ching-Hsiang"
      },
      {
        "family": "Cai",
        "given": "Chris"
      },
      {
        "family": "Tindal",
        "given": "Chris"
      },
      {
        "family": "Feichtenhofer",
        "given": "Christoph"
      },
      {
        "family": "Gao",
        "given": "Cynthia"
      },
      {
        "family": "Civin",
        "given": "Damon"
      },
      {
        "family": "Beaty",
        "given": "Dana"
      },
      {
        "family": "Kreymer",
        "given": "Daniel"
      },
      {
        "family": "Li",
        "given": "Daniel"
      },
      {
        "family": "Adkins",
        "given": "David"
      },
      {
        "family": "Xu",
        "given": "David"
      },
      {
        "family": "Testuggine",
        "given": "Davide"
      },
      {
        "family": "David",
        "given": "Delia"
      },
      {
        "family": "Parikh",
        "given": "Devi"
      },
      {
        "family": "Liskovich",
        "given": "Diana"
      },
      {
        "family": "Foss",
        "given": "Didem"
      },
      {
        "family": "Wang",
        "given": "Dingkang"
      },
      {
        "family": "Le",
        "given": "Duc"
      },
      {
        "family": "Holland",
        "given": "Dustin"
      },
      {
        "family": "Dowling",
        "given": "Edward"
      },
      {
        "family": "Jamil",
        "given": "Eissa"
      },
      {
        "family": "Montgomery",
        "given": "Elaine"
      },
      {
        "family": "Presani",
        "given": "Eleonora"
      },
      {
        "family": "Hahn",
        "given": "Emily"
      },
      {
        "family": "Wood",
        "given": "Emily"
      },
      {
        "family": "Le",
        "given": "Eric-Tuan"
      },
      {
        "family": "Brinkman",
        "given": "Erik"
      },
      {
        "family": "Arcaute",
        "given": "Esteban"
      },
      {
        "family": "Dunbar",
        "given": "Evan"
      },
      {
        "family": "Smothers",
        "given": "Evan"
      },
      {
        "family": "Sun",
        "given": "Fei"
      },
      {
        "family": "Kreuk",
        "given": "Felix"
      },
      {
        "family": "Tian",
        "given": "Feng"
      },
      {
        "family": "Kokkinos",
        "given": "Filippos"
      },
      {
        "family": "Ozgenel",
        "given": "Firat"
      },
      {
        "family": "Caggioni",
        "given": "Francesco"
      },
      {
        "family": "Kanayet",
        "given": "Frank"
      },
      {
        "family": "Seide",
        "given": "Frank"
      },
      {
        "family": "Florez",
        "given": "Gabriela Medina"
      },
      {
        "family": "Schwarz",
        "given": "Gabriella"
      },
      {
        "family": "Badeer",
        "given": "Gada"
      },
      {
        "family": "Swee",
        "given": "Georgia"
      },
      {
        "family": "Halpern",
        "given": "Gil"
      },
      {
        "family": "Herman",
        "given": "Grant"
      },
      {
        "family": "Sizov",
        "given": "Grigory"
      },
      {
        "family": "Guangyi"
      },
      {
        "family": "Zhang"
      },
      {
        "family": "Lakshminarayanan",
        "given": "Guna"
      },
      {
        "family": "Inan",
        "given": "Hakan"
      },
      {
        "family": "Shojanazeri",
        "given": "Hamid"
      },
      {
        "family": "Zou",
        "given": "Han"
      },
      {
        "family": "Wang",
        "given": "Hannah"
      },
      {
        "family": "Zha",
        "given": "Hanwen"
      },
      {
        "family": "Habeeb",
        "given": "Haroun"
      },
      {
        "family": "Rudolph",
        "given": "Harrison"
      },
      {
        "family": "Suk",
        "given": "Helen"
      },
      {
        "family": "Aspegren",
        "given": "Henry"
      },
      {
        "family": "Goldman",
        "given": "Hunter"
      },
      {
        "family": "Zhan",
        "given": "Hongyuan"
      },
      {
        "family": "Damlaj",
        "given": "Ibrahim"
      },
      {
        "family": "Molybog",
        "given": "Igor"
      },
      {
        "family": "Tufanov",
        "given": "Igor"
      },
      {
        "family": "Leontiadis",
        "given": "Ilias"
      },
      {
        "family": "Veliche",
        "given": "Irina-Elena"
      },
      {
        "family": "Gat",
        "given": "Itai"
      },
      {
        "family": "Weissman",
        "given": "Jake"
      },
      {
        "family": "Geboski",
        "given": "James"
      },
      {
        "family": "Kohli",
        "given": "James"
      },
      {
        "family": "Lam",
        "given": "Janice"
      },
      {
        "family": "Asher",
        "given": "Japhet"
      },
      {
        "family": "Gaya",
        "given": "Jean-Baptiste"
      },
      {
        "family": "Marcus",
        "given": "Jeff"
      },
      {
        "family": "Tang",
        "given": "Jeff"
      },
      {
        "family": "Chan",
        "given": "Jennifer"
      },
      {
        "family": "Zhen",
        "given": "Jenny"
      },
      {
        "family": "Reizenstein",
        "given": "Jeremy"
      },
      {
        "family": "Teboul",
        "given": "Jeremy"
      },
      {
        "family": "Zhong",
        "given": "Jessica"
      },
      {
        "family": "Jin",
        "given": "Jian"
      },
      {
        "family": "Yang",
        "given": "Jingyi"
      },
      {
        "family": "Cummings",
        "given": "Joe"
      },
      {
        "family": "Carvill",
        "given": "Jon"
      },
      {
        "family": "Shepard",
        "given": "Jon"
      },
      {
        "family": "McPhie",
        "given": "Jonathan"
      },
      {
        "family": "Torres",
        "given": "Jonathan"
      },
      {
        "family": "Ginsburg",
        "given": "Josh"
      },
      {
        "family": "Wang",
        "given": "Junjie"
      },
      {
        "family": "Wu",
        "given": "Kai"
      },
      {
        "family": "U",
        "given": "Kam Hou"
      },
      {
        "family": "Saxena",
        "given": "Karan"
      },
      {
        "family": "Khandelwal",
        "given": "Kartikay"
      },
      {
        "family": "Zand",
        "given": "Katayoun"
      },
      {
        "family": "Matosich",
        "given": "Kathy"
      },
      {
        "family": "Veeraraghavan",
        "given": "Kaushik"
      },
      {
        "family": "Michelena",
        "given": "Kelly"
      },
      {
        "family": "Li",
        "given": "Keqian"
      },
      {
        "family": "Jagadeesh",
        "given": "Kiran"
      },
      {
        "family": "Huang",
        "given": "Kun"
      },
      {
        "family": "Chawla",
        "given": "Kunal"
      },
      {
        "family": "Huang",
        "given": "Kyle"
      },
      {
        "family": "Chen",
        "given": "Lailin"
      },
      {
        "family": "Garg",
        "given": "Lakshya"
      },
      {
        "family": "A",
        "given": "Lavender"
      },
      {
        "family": "Silva",
        "given": "Leandro"
      },
      {
        "family": "Bell",
        "given": "Lee"
      },
      {
        "family": "Zhang",
        "given": "Lei"
      },
      {
        "family": "Guo",
        "given": "Liangpeng"
      },
      {
        "family": "Yu",
        "given": "Licheng"
      },
      {
        "family": "Moshkovich",
        "given": "Liron"
      },
      {
        "family": "Wehrstedt",
        "given": "Luca"
      },
      {
        "family": "Khabsa",
        "given": "Madian"
      },
      {
        "family": "Avalani",
        "given": "Manav"
      },
      {
        "family": "Bhatt",
        "given": "Manish"
      },
      {
        "family": "Mankus",
        "given": "Martynas"
      },
      {
        "family": "Hasson",
        "given": "Matan"
      },
      {
        "family": "Lennie",
        "given": "Matthew"
      },
      {
        "family": "Reso",
        "given": "Matthias"
      },
      {
        "family": "Groshev",
        "given": "Maxim"
      },
      {
        "family": "Naumov",
        "given": "Maxim"
      },
      {
        "family": "Lathi",
        "given": "Maya"
      },
      {
        "family": "Keneally",
        "given": "Meghan"
      },
      {
        "family": "Liu",
        "given": "Miao"
      },
      {
        "family": "Seltzer",
        "given": "Michael L."
      },
      {
        "family": "Valko",
        "given": "Michal"
      },
      {
        "family": "Restrepo",
        "given": "Michelle"
      },
      {
        "family": "Patel",
        "given": "Mihir"
      },
      {
        "family": "Vyatskov",
        "given": "Mik"
      },
      {
        "family": "Samvelyan",
        "given": "Mikayel"
      },
      {
        "family": "Clark",
        "given": "Mike"
      },
      {
        "family": "Macey",
        "given": "Mike"
      },
      {
        "family": "Wang",
        "given": "Mike"
      },
      {
        "family": "Hermoso",
        "given": "Miquel Jubert"
      },
      {
        "family": "Metanat",
        "given": "Mo"
      },
      {
        "family": "Rastegari",
        "given": "Mohammad"
      },
      {
        "family": "Bansal",
        "given": "Munish"
      },
      {
        "family": "Santhanam",
        "given": "Nandhini"
      },
      {
        "family": "Parks",
        "given": "Natascha"
      },
      {
        "family": "White",
        "given": "Natasha"
      },
      {
        "family": "Bawa",
        "given": "Navyata"
      },
      {
        "family": "Singhal",
        "given": "Nayan"
      },
      {
        "family": "Egebo",
        "given": "Nick"
      },
      {
        "family": "Usunier",
        "given": "Nicolas"
      },
      {
        "family": "Mehta",
        "given": "Nikhil"
      },
      {
        "family": "Laptev",
        "given": "Nikolay Pavlovich"
      },
      {
        "family": "Dong",
        "given": "Ning"
      },
      {
        "family": "Cheng",
        "given": "Norman"
      },
      {
        "family": "Chernoguz",
        "given": "Oleg"
      },
      {
        "family": "Hart",
        "given": "Olivia"
      },
      {
        "family": "Salpekar",
        "given": "Omkar"
      },
      {
        "family": "Kalinli",
        "given": "Ozlem"
      },
      {
        "family": "Kent",
        "given": "Parkin"
      },
      {
        "family": "Parekh",
        "given": "Parth"
      },
      {
        "family": "Saab",
        "given": "Paul"
      },
      {
        "family": "Balaji",
        "given": "Pavan"
      },
      {
        "family": "Rittner",
        "given": "Pedro"
      },
      {
        "family": "Bontrager",
        "given": "Philip"
      },
      {
        "family": "Roux",
        "given": "Pierre"
      },
      {
        "family": "Dollar",
        "given": "Piotr"
      },
      {
        "family": "Zvyagina",
        "given": "Polina"
      },
      {
        "family": "Ratanchandani",
        "given": "Prashant"
      },
      {
        "family": "Yuvraj",
        "given": "Pritish"
      },
      {
        "family": "Liang",
        "given": "Qian"
      },
      {
        "family": "Alao",
        "given": "Rachad"
      },
      {
        "family": "Rodriguez",
        "given": "Rachel"
      },
      {
        "family": "Ayub",
        "given": "Rafi"
      },
      {
        "family": "Murthy",
        "given": "Raghotham"
      },
      {
        "family": "Nayani",
        "given": "Raghu"
      },
      {
        "family": "Mitra",
        "given": "Rahul"
      },
      {
        "family": "Parthasarathy",
        "given": "Rangaprabhu"
      },
      {
        "family": "Li",
        "given": "Raymond"
      },
      {
        "family": "Hogan",
        "given": "Rebekkah"
      },
      {
        "family": "Battey",
        "given": "Robin"
      },
      {
        "family": "Wang",
        "given": "Rocky"
      },
      {
        "family": "Howes",
        "given": "Russ"
      },
      {
        "family": "Rinott",
        "given": "Ruty"
      },
      {
        "family": "Mehta",
        "given": "Sachin"
      },
      {
        "family": "Siby",
        "given": "Sachin"
      },
      {
        "family": "Bondu",
        "given": "Sai Jayesh"
      },
      {
        "family": "Datta",
        "given": "Samyak"
      },
      {
        "family": "Chugh",
        "given": "Sara"
      },
      {
        "family": "Hunt",
        "given": "Sara"
      },
      {
        "family": "Dhillon",
        "given": "Sargun"
      },
      {
        "family": "Sidorov",
        "given": "Sasha"
      },
      {
        "family": "Pan",
        "given": "Satadru"
      },
      {
        "family": "Mahajan",
        "given": "Saurabh"
      },
      {
        "family": "Verma",
        "given": "Saurabh"
      },
      {
        "family": "Yamamoto",
        "given": "Seiji"
      },
      {
        "family": "Ramaswamy",
        "given": "Sharadh"
      },
      {
        "family": "Lindsay",
        "given": "Shaun"
      },
      {
        "family": "Lindsay",
        "given": "Shaun"
      },
      {
        "family": "Feng",
        "given": "Sheng"
      },
      {
        "family": "Lin",
        "given": "Shenghao"
      },
      {
        "family": "Zha",
        "given": "Shengxin Cindy"
      },
      {
        "family": "Patil",
        "given": "Shishir"
      },
      {
        "family": "Shankar",
        "given": "Shiva"
      },
      {
        "family": "Zhang",
        "given": "Shuqiang"
      },
      {
        "family": "Zhang",
        "given": "Shuqiang"
      },
      {
        "family": "Wang",
        "given": "Sinong"
      },
      {
        "family": "Agarwal",
        "given": "Sneha"
      },
      {
        "family": "Sajuyigbe",
        "given": "Soji"
      },
      {
        "family": "Chintala",
        "given": "Soumith"
      },
      {
        "family": "Max",
        "given": "Stephanie"
      },
      {
        "family": "Chen",
        "given": "Stephen"
      },
      {
        "family": "Kehoe",
        "given": "Steve"
      },
      {
        "family": "Satterfield",
        "given": "Steve"
      },
      {
        "family": "Govindaprasad",
        "given": "Sudarshan"
      },
      {
        "family": "Gupta",
        "given": "Sumit"
      },
      {
        "family": "Deng",
        "given": "Summer"
      },
      {
        "family": "Cho",
        "given": "Sungmin"
      },
      {
        "family": "Virk",
        "given": "Sunny"
      },
      {
        "family": "Subramanian",
        "given": "Suraj"
      },
      {
        "family": "Choudhury",
        "given": "Sy"
      },
      {
        "family": "Goldman",
        "given": "Sydney"
      },
      {
        "family": "Remez",
        "given": "Tal"
      },
      {
        "family": "Glaser",
        "given": "Tamar"
      },
      {
        "family": "Best",
        "given": "Tamara"
      },
      {
        "family": "Koehler",
        "given": "Thilo"
      },
      {
        "family": "Robinson",
        "given": "Thomas"
      },
      {
        "family": "Li",
        "given": "Tianhe"
      },
      {
        "family": "Zhang",
        "given": "Tianjun"
      },
      {
        "family": "Matthews",
        "given": "Tim"
      },
      {
        "family": "Chou",
        "given": "Timothy"
      },
      {
        "family": "Shaked",
        "given": "Tzook"
      },
      {
        "family": "Vontimitta",
        "given": "Varun"
      },
      {
        "family": "Ajayi",
        "given": "Victoria"
      },
      {
        "family": "Montanez",
        "given": "Victoria"
      },
      {
        "family": "Mohan",
        "given": "Vijai"
      },
      {
        "family": "Kumar",
        "given": "Vinay Satish"
      },
      {
        "family": "Mangla",
        "given": "Vishal"
      },
      {
        "family": "Ionescu",
        "given": "Vlad"
      },
      {
        "family": "Poenaru",
        "given": "Vlad"
      },
      {
        "family": "Mihailescu",
        "given": "Vlad Tiberiu"
      },
      {
        "family": "Ivanov",
        "given": "Vladimir"
      },
      {
        "family": "Li",
        "given": "Wei"
      },
      {
        "family": "Wang",
        "given": "Wenchen"
      },
      {
        "family": "Jiang",
        "given": "Wenwen"
      },
      {
        "family": "Bouaziz",
        "given": "Wes"
      },
      {
        "family": "Constable",
        "given": "Will"
      },
      {
        "family": "Tang",
        "given": "Xiaocheng"
      },
      {
        "family": "Wu",
        "given": "Xiaojian"
      },
      {
        "family": "Wang",
        "given": "Xiaolan"
      },
      {
        "family": "Wu",
        "given": "Xilun"
      },
      {
        "family": "Gao",
        "given": "Xinbo"
      },
      {
        "family": "Kleinman",
        "given": "Yaniv"
      },
      {
        "family": "Chen",
        "given": "Yanjun"
      },
      {
        "family": "Hu",
        "given": "Ye"
      },
      {
        "family": "Jia",
        "given": "Ye"
      },
      {
        "family": "Qi",
        "given": "Ye"
      },
      {
        "family": "Li",
        "given": "Yenda"
      },
      {
        "family": "Zhang",
        "given": "Yilin"
      },
      {
        "family": "Zhang",
        "given": "Ying"
      },
      {
        "family": "Adi",
        "given": "Yossi"
      },
      {
        "family": "Nam",
        "given": "Youngjin"
      },
      {
        "family": "Yu"
      },
      {
        "literal": "Wang"
      },
      {
        "family": "Zhao",
        "given": "Yu"
      },
      {
        "family": "Hao",
        "given": "Yuchen"
      },
      {
        "family": "Qian",
        "given": "Yundi"
      },
      {
        "family": "Li",
        "given": "Yunlu"
      },
      {
        "family": "He",
        "given": "Yuzi"
      },
      {
        "family": "Rait",
        "given": "Zach"
      },
      {
        "family": "DeVito",
        "given": "Zachary"
      },
      {
        "family": "Rosnbrick",
        "given": "Zef"
      },
      {
        "family": "Wen",
        "given": "Zhaoduo"
      },
      {
        "family": "Yang",
        "given": "Zhenyu"
      },
      {
        "family": "Zhao",
        "given": "Zhiwei"
      },
      {
        "family": "Ma",
        "given": "Zhiyu"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2024
        ]
      ]
    },
    "abstract": "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",
    "DOI": "10.48550/arxiv.2407.21783",
    "publisher": "arXiv",
    "title": "The Llama 3 Herd of Models",
    "URL": "https://doi.org/ndw6",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2407.21783"
  },
  {
    "type": "article",
    "id": "XpHq6HEw",
    "categories": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Zhong",
        "given": "Ruiqi"
      },
      {
        "family": "Wang",
        "given": "Heng"
      },
      {
        "family": "Klein",
        "given": "Dan"
      },
      {
        "family": "Steinhardt",
        "given": "Jacob"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2024
        ]
      ]
    },
    "abstract": "To make sense of massive data, we often fit simplified models and then interpret the parameters; for example, we cluster the text embeddings and then interpret the mean parameters of each cluster. However, these parameters are often high-dimensional and hard to interpret. To make model parameters directly interpretable, we introduce a family of statistical models -- including clustering, time series, and classification models -- parameterized by natural language predicates. For example, a cluster of text about COVID could be parameterized by the predicate \"discusses COVID\". To learn these statistical models effectively, we develop a model-agnostic algorithm that optimizes continuous relaxations of predicate parameters with gradient descent and discretizes them by prompting language models (LMs). Finally, we apply our framework to a wide range of problems: taxonomizing user chat dialogues, characterizing how they evolve across time, finding categories where one language model is better than the other, clustering math problems based on subareas, and explaining visual features in memorable images. Our framework is highly versatile, applicable to both textual and visual domains, can be easily steered to focus on specific properties (e.g. subareas), and explains sophisticated concepts that classical methods (e.g. n-gram analysis) struggle to produce.",
    "DOI": "10.48550/arxiv.2409.08466",
    "publisher": "arXiv",
    "title": "Explaining Datasets in Words: Statistical Models with Natural Language Parameters",
    "URL": "https://doi.org/g9t22k",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2409.08466"
  },
  {
    "id": "lGpaLCJV",
    "type": "article-journal",
    "author": [
      {
        "family": "Lei",
        "given": "Yaguo"
      },
      {
        "family": "Li",
        "given": "Naipeng"
      },
      {
        "family": "Gontarz",
        "given": "Stanislaw"
      },
      {
        "family": "Lin",
        "given": "Jing"
      },
      {
        "family": "Radkowski",
        "given": "Slawomir"
      },
      {
        "family": "Dybala",
        "given": "Jacek"
      }
    ],
    "title": "A Model-based method for remaining useful life prediction of machinery",
    "container-title": "IEEE Transactions on Reliability",
    "volume": "65",
    "issue": "3",
    "page": "1314--1326",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: lei2018machinery"
  },
  {
    "title": "A review of multivariate longitudinal data analysis.",
    "volume": "20",
    "issue": "4",
    "page": "299-330",
    "container-title": "Statistical methods in medical research",
    "container-title-short": "Stat Methods Med Res",
    "ISSN": "1477-0334",
    "issued": {
      "date-parts": [
        [
          2010,
          3,
          8
        ]
      ]
    },
    "author": [
      {
        "given": "S",
        "family": "Bandyopadhyay"
      },
      {
        "given": "B",
        "family": "Ganguli"
      },
      {
        "given": "A",
        "family": "Chatterjee"
      }
    ],
    "PMID": "20212072",
    "DOI": "10.1177/0962280209340191",
    "abstract": "Repeated observation of multiple outcomes is common in biomedical and public health research. Such experiments result in multivariate longitudinal data, which are unique in the sense that they allow the researcher to study the joint evolution of these outcomes over time. Special methods are required to analyse such data because repeated observations on any given response are likely to be correlated over time while multiple responses measured at a given time point will also be correlated. We review three approaches for analysing such data in the light of the associated theory, applications and software. The first method consists of the application of univariate longitudinal tools to a single summary outcome. The second method aims at estimating regression coefficients without explicitly modelling the underlying covariance structure of the data. The third method combines all the outcomes into a single joint multivariate model. We also introduce a multivariate longitudinal dataset and use it to illustrate some of the techniques discussed in the article.",
    "URL": "https://www.ncbi.nlm.nih.gov/pubmed/20212072",
    "type": "article-journal",
    "id": "9GZOsj4z",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: pubmed:20212072"
  },
  {
    "title": "A Bayesian multilevel time-varying framework for joint modeling of hospitalization and survival in patients on dialysis.",
    "volume": "41",
    "issue": "29",
    "page": "5597-5611",
    "container-title": "Statistics in medicine",
    "container-title-short": "Stat Med",
    "ISSN": "1097-0258",
    "issued": {
      "date-parts": [
        [
          2022,
          10,
          1
        ]
      ]
    },
    "author": [
      {
        "given": "Esra",
        "family": "KÃ¼rÃ¼m"
      },
      {
        "given": "Danh V",
        "family": "Nguyen"
      },
      {
        "given": "Sudipto",
        "family": "Banerjee"
      },
      {
        "given": "Yihao",
        "family": "Li"
      },
      {
        "given": "Connie M",
        "family": "Rhee"
      },
      {
        "given": "Damla",
        "family": "ÅentÃ¼rk"
      }
    ],
    "PMID": "36181392",
    "PMCID": "PMC9931182",
    "DOI": "10.1002/sim.9582",
    "abstract": "Over 782â000 individuals in the United States have end-stage kidney disease with about 72% of patients on dialysis, a life-sustaining treatment. Dialysis patients experience high mortality and frequent hospitalizations, at about twice per year. These poor outcomes are exacerbated at key time periods, such as the fragile period after transition to dialysis. In order to study the time-varying effects of modifiable patient and dialysis facility risk factors on hospitalization and mortality, we propose a novel Bayesian multilevel time-varying joint model. Efficient estimation and inference is achieved within the Bayesian framework using Markov chain Monte Carlo, where multilevel (patient- and dialysis facility-level) varying coefficient functions are targeted via Bayesian P-splines. Applications to the United States Renal Data System, a national database which contains data on nearly all patients on dialysis in the United States, highlight significant time-varying effects of patient- and facility-level risk factors on hospitalization risk and mortality. Finite sample performance of the proposed methodology is studied through simulations.",
    "URL": "https://www.ncbi.nlm.nih.gov/pubmed/36181392",
    "type": "article-journal",
    "id": "5V2lk8fC",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: pubmed:36181392"
  },
  {
    "title": "[A field study: piroxicam in arthrosis and soft tissue rheumatism. Results of a multicenter study].",
    "volume": "72",
    "issue": "37",
    "page": "1189-94",
    "container-title": "Schweizerische Rundschau fur Medizin Praxis = Revue suisse de medecine Praxis",
    "container-title-short": "Schweiz Rundsch Med Prax",
    "ISSN": "1013-2058",
    "issued": {
      "date-parts": [
        [
          1983,
          9,
          13
        ]
      ]
    },
    "author": [
      {
        "given": "O",
        "family": "KnÃ¼sel"
      },
      {
        "given": "G",
        "family": "Hinz"
      }
    ],
    "PMID": "6356121",
    "URL": "https://www.ncbi.nlm.nih.gov/pubmed/6356121",
    "type": "article-journal",
    "id": "D6xnJJaY",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: pubmed:PMC6356121"
  },
  {
    "id": "15RY4QKp3",
    "type": "paper-conference",
    "title": "Principles of Risk Minimization for Learning Theory",
    "author": [
      {
        "family": "Vapnik",
        "given": "Vladimir N."
      }
    ],
    "issued": {
      "date-parts": [
        [
          1991
        ]
      ]
    },
    "event": "Advances in Neural Information Processing Systems (NIPS 1991)",
    "URL": "https://proceedings.neurips.cc/paper/1991/file/ff4d5fbbafdf976cfdc032e3bde78de5-Abstract.html",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: vapnik1991principles"
  },
  {
    "id": "Do5Pmykz",
    "type": "manuscript",
    "author": [
      {
        "family": "Dzhoha",
        "given": "Andrii"
      },
      {
        "family": "Mironenko",
        "given": "Alisa"
      },
      {
        "family": "Labzin",
        "given": "Evgeny"
      },
      {
        "family": "Vlasov",
        "given": "Vladimir"
      },
      {
        "family": "Versteegh",
        "given": "Maarten"
      },
      {
        "family": "Celikik",
        "given": "Marjan"
      }
    ],
    "title": "Efficient and Effective Query Context-Aware Learning-to-Rank Model for Sequential Recommendation",
    "archive": "arXiv",
    "note": "cs.IR\nLoaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: wang2019efficiency"
  }
]
