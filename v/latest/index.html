<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Ben Lengerich" />
  <meta name="author" content="Caleb N. Ellington" />
  <meta name="author" content="Yue Yao" />
  <meta name="dcterms.date" content="2025-07-31" />
  <meta name="keywords" content="markdown, publishing, manubot" />
  <title>Context-Adaptive Inference: Bridging Statistical and Foundation Models</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
  <!--
  Manubot generated metadata rendered from header-includes-template.html.
  Suggest improvements at https://github.com/manubot/manubot/blob/main/manubot/process/header-includes-template.html
  -->
  <meta name="dc.format" content="text/html" />
  <meta property="og:type" content="article" />
  <meta name="dc.title" content="Context-Adaptive Inference: Bridging Statistical and Foundation Models" />
  <meta name="citation_title" content="Context-Adaptive Inference: Bridging Statistical and Foundation Models" />
  <meta property="og:title" content="Context-Adaptive Inference: Bridging Statistical and Foundation Models" />
  <meta property="twitter:title" content="Context-Adaptive Inference: Bridging Statistical and Foundation Models" />
  <meta name="dc.date" content="2025-07-31" />
  <meta name="citation_publication_date" content="2025-07-31" />
  <meta property="article:published_time" content="2025-07-31" />
  <meta name="dc.modified" content="2025-07-31T21:28:38+00:00" />
  <meta property="article:modified_time" content="2025-07-31T21:28:38+00:00" />
  <meta name="dc.language" content="en-US" />
  <meta name="citation_language" content="en-US" />
  <meta name="dc.relation.ispartof" content="Manubot" />
  <meta name="dc.publisher" content="Manubot" />
  <meta name="citation_journal_title" content="Manubot" />
  <meta name="citation_technical_report_institution" content="Manubot" />
  <meta name="citation_author" content="Ben Lengerich" />
  <meta name="citation_author_institution" content="Department of Statistics, University of Wisconsin-Madison" />
  <meta name="citation_author_orcid" content="0000-0001-8690-9554" />
  <meta name="twitter:creator" content="@ben_lengerich" />
  <meta name="citation_author" content="Caleb N. Ellington" />
  <meta name="citation_author_institution" content="Computational Biology Department, Carnegie Mellon University" />
  <meta name="citation_author_orcid" content="0000-0001-7029-8023" />
  <meta name="twitter:creator" content="@probablybots" />
  <meta name="citation_author" content="Yue Yao" />
  <meta name="citation_author_institution" content="Department of Statistics, University of Wisconsin-Madison" />
  <meta name="citation_author_orcid" content="0009-0000-8195-3943" />
  <link rel="canonical" href="https://AdaptInfer.github.io/context-review/" />
  <meta property="og:url" content="https://AdaptInfer.github.io/context-review/" />
  <meta property="twitter:url" content="https://AdaptInfer.github.io/context-review/" />
  <meta name="citation_fulltext_html_url" content="https://AdaptInfer.github.io/context-review/" />
  <meta name="citation_pdf_url" content="https://AdaptInfer.github.io/context-review/manuscript.pdf" />
  <link rel="alternate" type="application/pdf" href="https://AdaptInfer.github.io/context-review/manuscript.pdf" />
  <link rel="alternate" type="text/html" href="https://AdaptInfer.github.io/context-review/v/95d13f217a04d8c30e96b2d36b397577de4a12f5/" />
  <meta name="manubot_html_url_versioned" content="https://AdaptInfer.github.io/context-review/v/95d13f217a04d8c30e96b2d36b397577de4a12f5/" />
  <meta name="manubot_pdf_url_versioned" content="https://AdaptInfer.github.io/context-review/v/95d13f217a04d8c30e96b2d36b397577de4a12f5/manuscript.pdf" />
  <meta property="og:type" content="article" />
  <meta property="twitter:card" content="summary_large_image" />
  <link rel="icon" type="image/png" sizes="192x192" href="https://manubot.org/favicon-192x192.png" />
  <link rel="mask-icon" href="https://manubot.org/safari-pinned-tab.svg" color="#ad1457" />
  <meta name="theme-color" content="#ad1457" />
  <!-- end Manubot generated metadata -->
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Context-Adaptive Inference: Bridging Statistical and Foundation Models</h1>
</header>
<p><small><em>
This manuscript
(<a href="https://AdaptInfer.github.io/context-review/v/95d13f217a04d8c30e96b2d36b397577de4a12f5/">permalink</a>)
was automatically generated
from <a href="https://github.com/AdaptInfer/context-review/tree/95d13f217a04d8c30e96b2d36b397577de4a12f5">AdaptInfer/context-review@95d13f2</a>
on July 31, 2025.
</em></small></p>
<h2 id="authors">Authors</h2>
<ul>
<li><p><strong>Ben Lengerich</strong>
<br>
<img src="images/orcid.svg" class="inline_icon" width="16" height="16" alt="ORCID icon" />
<a href="https://orcid.org/0000-0001-8690-9554">0000-0001-8690-9554</a>
· <img src="images/github.svg" class="inline_icon" width="16" height="16" alt="GitHub icon" />
<a href="https://github.com/blengerich">blengerich</a>
· <img src="images/twitter.svg" class="inline_icon" width="16" height="16" alt="Twitter icon" />
<a href="https://twitter.com/ben_lengerich">ben_lengerich</a>
<br>
<small>
Department of Statistics, University of Wisconsin-Madison
· Funded by None
</small></p></li>
<li><p><strong>Caleb N. Ellington</strong>
<br>
<img src="images/orcid.svg" class="inline_icon" width="16" height="16" alt="ORCID icon" />
<a href="https://orcid.org/0000-0001-7029-8023">0000-0001-7029-8023</a>
· <img src="images/github.svg" class="inline_icon" width="16" height="16" alt="GitHub icon" />
<a href="https://github.com/cnellington">cnellington</a>
· <img src="images/twitter.svg" class="inline_icon" width="16" height="16" alt="Twitter icon" />
<a href="https://twitter.com/probablybots">probablybots</a>
<br>
<small>
Computational Biology Department, Carnegie Mellon University
· Funded by None
</small></p></li>
<li><p><strong>Yue Yao</strong>
<br>
<img src="images/orcid.svg" class="inline_icon" width="16" height="16" alt="ORCID icon" />
<a href="https://orcid.org/0009-0000-8195-3943">0009-0000-8195-3943</a>
· <img src="images/github.svg" class="inline_icon" width="16" height="16" alt="GitHub icon" />
<a href="https://github.com/YueYao-stat">YueYao-stat</a>
<br>
<small>
Department of Statistics, University of Wisconsin-Madison
· Funded by None
</small></p></li>
</ul>
<div id="correspondence">
<p>✉ — Correspondence possible via <a href="https://github.com/AdaptInfer/context-review/issues">GitHub Issues</a></p>
</div>
<h2 class="page_break_before" id="abstract">Abstract</h2>
<p>Context-adaptive inference enables models to adjust their behavior across individuals, environments, or tasks.
This adaptivity may be <em>explicit</em>, through parameterized functions of context, or <em>implicit</em>, as in foundation models that respond to prompts and support in-context learning.
In this review, we connect recent developments in varying-coefficient models, contextualized learning, and in-context learning.
We highlight how foundation models can serve as flexible encoders of context, and how statistical methods offer structure and interpretability.
We propose a unified view of context-adaptive inference and outline open challenges in developing scalable, principled, and personalized models that adapt to the complexities of real-world data.</p>
<h2 id="introduction">Introduction</h2>
<p>A convenient simplifying assumption in statistical modeling is that observations are independent and identically distributed (i.i.d.).
This assumption allows us to use a single model to make predictions across all data points.
But in practice, this assumption rarely holds.
Data are collected across different individuals, environments, and tasks – each with their own characteristics, constraints, and dynamics.</p>
<p>To model this heterogeneity, a growing class of methods aim to make inference <em>adaptive to context</em>. These include varying-coefficient models in statistics, transfer and meta-learning in machine learning, and in-context learning in large foundation models. Though these approaches arise from different traditions, they share a common goal: to use contextual information – whether covariates, environments, or support sets – to inform sample-specific inference.</p>
<p>We formalize this by assuming each observation <span class="math inline">\(x_i\)</span> is drawn from a distribution governed by parameters <span class="math inline">\(\theta_i\)</span>:</p>
<p><span class="math display">\[
x_i \sim P(x; \theta_i).
\]</span></p>
<p>In population models, the assumption is that <span class="math inline">\(\theta_i = \theta\)</span> for all <span class="math inline">\(i\)</span>. In context-adaptive models, we instead posit that the parameters vary with context:</p>
<p><span class="math display">\[
\theta_i = f(c_i) \quad \text{or} \quad \theta_i \sim P(\theta \mid c_i),
\]</span></p>
<p>where <span class="math inline">\(c_i\)</span> captures the relevant covariates or environment for observation <span class="math inline">\(i\)</span>. The goal is to estimate either a deterministic function <span class="math inline">\(f\)</span> or a conditional distribution over parameters.</p>
<p>This shift raises new modeling challenges.
Estimating a unique <span class="math inline">\(\theta_i\)</span> from a single observation is ill-posed unless we impose structure—smoothness, sparsity, shared representations, or latent grouping.
And as adaptivity becomes more implicit (e.g., via neural networks or black-box inference), we must develop tools to recover, interpret, or constrain the underlying parameter variation.</p>
<p>In this review, we examine methods that use context to guide inference, either by specifying how parameters change with covariates or by learning to adapt behavior implicitly.
We begin with classical models that impose explicit structure—such as varying-coefficient models and multi-task learning—and then turn to more flexible approaches like meta-learning and in-context learning with foundation models.
Though these methods arise from different traditions, they share a common goal: to tailor inference to the local characteristics of each observation or task.
Along the way, we highlight recurring themes: complex models often decompose into simpler, context-specific components; foundation models can both adapt to and generate context; and context-awareness challenges classical assumptions of homogeneity.
These perspectives offer a unifying lens on recent advances and open new directions for building adaptive, interpretable, and personalized models.</p>
<h2 id="from-population-assumptions-to-context-adaptive-inference">From Population Assumptions to Context-Adaptive Inference</h2>
<p>Most statistical and machine learning models begin with a foundational assumption: that all samples are drawn independently and identically from a shared population distribution. This assumption simplifies estimation and enables generalization from limited data, but it collapses in the presence of meaningful heterogeneity.</p>
<p>In practice, data often reflect differences across individuals, environments, or conditions. These differences may stem from biological variation, temporal drift, site effects, or shifts in measurement context. Treating heterogeneous data as if it were homogeneous can obscure real effects, inflate variance, and lead to brittle predictions.</p>
<h3 id="failure-modes-of-population-models">Failure Modes of Population Models</h3>
<p>Even when traditional models appear to fit aggregate data well, they may hide systematic failure modes.</p>
<p><strong>Mode Collapse</strong><br />
When one subpopulation is much larger than another, standard models are biased toward the dominant group, underrepresenting the minority group in both fit and predictions.</p>
<p><strong>Outlier Sensitivity</strong><br />
In the parameter-averaging regime, small but extreme groups can disproportionately distort the global model, especially in methods like ordinary least squares.</p>
<p><strong>Phantom Populations</strong><br />
When multiple subpopulations are equally represented, the global model may fit none of them well, instead converging to a solution that represents a non-existent average case.</p>
<p>These behaviors reflect a deeper problem: the assumption of identically distributed samples is not just incorrect, but actively harmful in heterogeneous settings.</p>
<h3 id="toward-context-aware-models">Toward Context-Aware Models</h3>
<p>To account for heterogeneity, we must relax the assumption of shared parameters and allow the data-generating process to vary across samples. A general formulation assumes each observation is governed by its own latent parameters:
<span class="math display">\[
x_i \sim P(x; \theta_i),
\]</span></p>
<p>However, estimating <span class="math inline">\(N\)</span> free parameters from <span class="math inline">\(N\)</span> samples is underdetermined.
Context-aware approaches resolve this by introducing structure on how parameters vary, often by assuming that <span class="math inline">\(\theta_i\)</span> depends on an observed context <span class="math inline">\(c_i\)</span>:</p>
<p><span class="math display">\[
\theta_i = f(c_i) \quad \text{or} \quad \theta_i \sim P(\theta \mid c_i).
\]</span></p>
<p>This formulation makes the model estimable, but it raises new challenges.
How should <span class="math inline">\(f\)</span> be chosen? How smooth, flexible, or structured should it be? The remainder of this review explores different answers to this question, and shows how implicit and explicit representations of context can lead to powerful, personalized models.</p>
<h3 id="early-remedies-grouped-and-distance-based-models">Early Remedies: Grouped and Distance-Based Models</h3>
<p>Before diving into flexible estimators of <span class="math inline">\(f(c)\)</span>, we review early modeling strategies that attempt to break away from homogeneity.</p>
<h4 id="conditional-and-clustered-models">Conditional and Clustered Models</h4>
<p>One approach is to group observations into C contexts, either by manually defining conditions (e.g. male vs. female) or using unsupervised clustering. Each group is then assigned a distinct parameter vector:</p>
<p><span class="math display">\[
\{\widehat{\theta}_0, \ldots, \widehat{\theta}_C\} = \arg\max_{\theta_0, \ldots, \theta_C} \sum_{c \in \mathcal{C}} \ell(X_c; \theta_c),
\]</span>
where <span class="math inline">\(\ell(X; \theta)\)</span> is the log-likelihood of <span class="math inline">\(\theta\)</span> on <span class="math inline">\(X\)</span> and <span class="math inline">\(c\)</span> specifies the covariate group that samples are assigned to. This reduces variance but limits granularity. It assumes that all members of a group share the same distribution and fails to capture variation within a group.</p>
<h4 id="distance-regularized-estimation">Distance-Regularized Estimation</h4>
<p>A more flexible alternative assumes that observations with similar contexts should have similar parameters. This is encoded as a regularization penalty that discourages large differences in <span class="math inline">\(\theta_i\)</span> for nearby <span class="math inline">\(c_i\)</span>:</p>
<p><span class="math display">\[
\{\widehat{\theta}_0, \ldots, \widehat{\theta}_N\} = \arg\max_{\theta_0, \ldots, \theta_N} \left( \sum_i \ell(x_i; \theta_i) - \sum_{i,j} \frac{\|\theta_i - \theta_j\|}{D(c_i, c_j)} \right),
\]</span></p>
<p>where <span class="math inline">\(D(c_i, c_j)\)</span> is a distance metric between contexts. This approach allows for smoother parameter variation but requires careful choice of <span class="math inline">\(D\)</span> and regularization strength <span class="math inline">\(\lambda\)</span> to balance bias and variance.<br />
The choice of distance metric D and regularization strength λ controls the bias–variance tradeoff.</p>
<h4 id="parametric-varying-coefficient-models">Parametric Varying-coefficient models</h4>
<p>Original paper (based on a smoothing spline function): <span class="citation" data-cites="ugXwusl0">[<a href="#ref-ugXwusl0" role="doc-biblioref">1</a>]</span>
Markov networks: <span class="citation" data-cites="TULLRYDp">[<a href="#ref-TULLRYDp" role="doc-biblioref">2</a>]</span>
Linear varying-coefficient models assume that parameters vary linearly with covariates, a much stronger assumption than the classic varying-coefficient model but making a conceptual leap that allows us to define a form for the relationship between the parameters and covariates.
<span class="math display">\[\widehat{\theta}_0, ..., \widehat{\theta}_N = \widehat{A} C^T\]</span>
<span class="math display">\[ \widehat{A} = \arg\max_A \sum_i \ell(x_i; A c_i) \]</span></p>
<p>TODO: Note that they achieve distance-matching by using a distance metric under Euclidean distance, which is a special case of the distance-regularized estimation above.</p>
<h5 id="semi-parametric-varying-coefficient-models">Semi-parametric varying-coefficient Models</h5>
<p>Original paper: <span class="citation" data-cites="l6vMkIsa">[<a href="#ref-l6vMkIsa" role="doc-biblioref">3</a>]</span>
2-step estimation with RBF kernels: <span class="citation" data-cites="1CkxORTSX">[<a href="#ref-1CkxORTSX" role="doc-biblioref">4</a>]</span></p>
<p>Classic varying-coefficient models assume that models with similar covariates have similar parameters, or – more formally – that changes in parameters are smooth over the covariate space.
This assumption is encoded as a sample weighting, often using a kernel, where the relevance of a sample to a model is equivalent to its kernel similarity over the covariate space.
<span class="math display">\[\widehat{\theta}_0, ..., \widehat{\theta}_N = \arg\max_{\theta_0, ..., \theta_N} \sum_{i, j} \frac{K(c_i, c_j)}{\sum_{k} K(c_i, c_k)} \ell(x_j; \theta_i)\]</span>
This estimator is the simplest to recover <span class="math inline">\(N\)</span> unique parameter estimates.
However, the assumption here is contradictory to the partition model estimator.
When the relationship between covariates and parameters is discontinuous or abrupt, this estimator will fail.</p>
<h5 id="contextualized-models">Contextualized Models</h5>
<p>Seminal work <span class="citation" data-cites="SfCo6pSp">[<a href="#ref-SfCo6pSp" role="doc-biblioref">5</a>]</span>
Contextualized ML generalization and applications: <span class="citation" data-cites="HYsEq2UQ">[<a href="#ref-HYsEq2UQ" role="doc-biblioref">6</a>]</span>, <span class="citation" data-cites="grNza1Og">[<a href="#ref-grNza1Og" role="doc-biblioref">7</a>]</span>, <span class="citation" data-cites="4cK1tiec">[<a href="#ref-4cK1tiec" role="doc-biblioref">8</a>]</span>, <span class="citation" data-cites="nYipTPML">[<a href="#ref-nYipTPML" role="doc-biblioref">9</a>]</span>, <span class="citation" data-cites="esxxcr9l">[<a href="#ref-esxxcr9l" role="doc-biblioref">10</a>]</span>, <span class="citation" data-cites="O1UU4a5P">[<a href="#ref-O1UU4a5P" role="doc-biblioref">11</a>]</span>, <span class="citation" data-cites="Rt6voTFN">[<a href="#ref-Rt6voTFN" role="doc-biblioref">12</a>]</span>, <span class="citation" data-cites="9S6tI5yv">[<a href="#ref-9S6tI5yv" role="doc-biblioref">13</a>]</span></p>
<p>Contextualized models make the assumption that parameters are some function of context, but make no assumption on the form of that function.
In this regime, we seek to estimate the function often using a deep learner (if we have some differentiable proxy for probability):
<span class="math display">\[ \widehat{f} = \arg \max_{f \in \mathcal{F}} \sum_i \ell(x_i; f(c_i)) \]</span></p>
<h3 id="latent-structure-models">Latent-structure Models</h3>
<h5 id="partition-models">Partition Models</h5>
<p>Markov networks: <span class="citation" data-cites="lAsTg3IH">[<a href="#ref-lAsTg3IH" role="doc-biblioref">14</a>]</span>
Partition models also assume that parameters can be partitioned into homogeneous groups over the covariate space, but make no assumption about where these partitions occur.
This allows the use of information from different groups in estimating a model for a each covariate.
Partition model estimators are most often utilized to infer abrupt model changes over time and take the form
<span class="math display">\[ \widehat{\theta}_0, ..., \widehat{\theta}_N = \arg\max_{\theta_0, ..., \theta_N} \sum_i \ell(x_i; \theta_i) + \sum_{i = 2}^N \text{TV}(\theta_i, \theta_{i-1})\]</span>
Where the regularizaiton term might take the form
<span class="math display">\[\text{TV}(\theta_i, \theta_{i - 1}) = |\theta_i - \theta_{i-1}|\]</span>
This still fails to recover a unique parameter estimate for each sample, but gets closer to the spirit of personalized modeling by putting the model likelihood and partition regularizer in competition to find the optimal partitions.</p>
<h3 id="fine-tuned-models-and-transfer-learning">Fine-tuned Models and Transfer Learning</h3>
<p>Review: <span class="citation" data-cites="k6r0UwSv">[<a href="#ref-k6r0UwSv" role="doc-biblioref">15</a>]</span>
Noted in foundational literature for linear varying coefficient models <span class="citation" data-cites="l6vMkIsa">[<a href="#ref-l6vMkIsa" role="doc-biblioref">3</a>]</span></p>
<p>Estimate a population model, freeze these parameters, and then include a smaller set of personalized parameters to estimate on a smaller subpopulation.
<span class="math display">\[ \widehat{\gamma} = \arg\max_{\gamma} = \ell(\gamma; X) \]</span>
<span class="math display">\[ \widehat{\theta_c} = \arg\max_{\theta_c} = \ell(\theta_c; \widehat{\gamma}, X_c) \]</span></p>
<h3 id="context-informed-and-latent-structure-models">Context-informed and Latent-structure models</h3>
<p>Seminal paper: <span class="citation" data-cites="WlwUpYp">[<a href="#ref-WlwUpYp" role="doc-biblioref">16</a>]</span></p>
<p>Key idea: negative information sharing. Different models should be pushed apart.
<span class="math display">\[ \widehat{\theta}_0, ..., \widehat{\theta}_N = \arg\max_{\theta_0, ..., \theta_N, D} \sum_{i=0}^N \prod_{j=0 s.t. D(c_i, c_j) &lt; d}^N P(x_j; \theta_i) P(\theta_i ; \theta_j) \]</span></p>
<h3 id="a-spectrum-of-context-awareness">A Spectrum of Context-Awareness</h3>
<p>Context-aware models can be viewed along a spectrum of assumptions about the relationship between context and parameters.</p>
<p><strong>Global models</strong>: <span class="math inline">\(\theta_i = \theta\)</span> for all <span class="math inline">\(i\)</span><br />
<strong>Grouped models</strong>: <span class="math inline">\(\theta_i = \theta_c\)</span> for some finite set of groups<br />
<strong>Smooth models</strong>: <span class="math inline">\(\theta_i = f(c_i)\)</span>, with <span class="math inline">\(f\)</span> assumed to be continuous or low-complexity<br />
<strong>Latent models</strong>: <span class="math inline">\(\theta_i \sim P(\theta | c_i)\)</span>, with <span class="math inline">\(f\)</span> learned implicitly</p>
<p>Each of these choices encodes different beliefs about how parameters vary. The next section formalizes this variation and examines general principles for adaptivity in statistical modeling.</p>
<p>Relevant references:</p>
<ul>
<li>Can Subpopulation Shifts Explain Disagreement in Model Generalization? <span class="citation" data-cites="HzzgJQN0">[<a href="#ref-HzzgJQN0" role="doc-biblioref">17</a>]</span></li>
</ul>
<h2 id="principles-of-context-adaptive-inference">Principles of Context-Adaptive Inference</h2>
<p>What makes a model adaptive? When is it good for a model to be adaptive? While the appeal of adaptivity lies in flexibility and personalized inference, not all adaptivity is good adaptivity. In this section, we formalize the core principles that underlie adaptive modeling.</p>
<h3 id="adaptivity-requires-flexibility">1. Adaptivity requires flexibility</h3>
<p>A model cannot adapt unless it has the capacity to represent multiple behaviors. Flexibility may take the form of nonlinearity, hierarchical structure, or modular components that allow different responses in different settings.</p>
<ul>
<li>Interaction effects in regression models <span class="citation" data-cites="gSmt16Rh">[<a href="#ref-gSmt16Rh" role="doc-biblioref">18</a>]</span></li>
<li>Hierarchical models that allow for varying effects across groups</li>
<li>Meta-learning and mixtures-of-experts models that learn to adapt based on context</li>
<li>Varying-coefficient models that allow coefficients to change with context <span class="citation" data-cites="ugXwusl0">[<a href="#ref-ugXwusl0" role="doc-biblioref">1</a>]</span></li>
</ul>
<h3 id="adaptivity-requires-a-signal-of-heterogeneity">2. Adaptivity requires a signal of heterogeneity</h3>
<ul>
<li>Varying-coefficient models adapt parameters based on observed context <span class="citation" data-cites="ugXwusl0">[<a href="#ref-ugXwusl0" role="doc-biblioref">1</a>]</span></li>
<li>Contextual bandits adapt actions to context features <span class="citation" data-cites="FJALdE9T">[<a href="#ref-FJALdE9T" role="doc-biblioref">19</a>]</span></li>
<li>Multi-domain models adapt across known environments or inferred partitions <span class="citation" data-cites="10151coVE">[<a href="#ref-10151coVE" role="doc-biblioref">20</a>]</span></li>
</ul>
<h3 id="modularity-improves-adaptivity">3. Modularity improves adaptivity</h3>
<p>Adaptive systems are easier to design, debug, and interpret when built from modular parts. Modularity supports targeted adaptation, transferability, and disentanglement.</p>
<ul>
<li>[]</li>
</ul>
<h3 id="adaptivity-implies-selectivity">4. Adaptivity implies selectivity</h3>
<p>Adaptation must be earned. Overreacting to limited data leads to overfitting. The best adaptive methods include mechanisms for deciding when not to adapt.
- Lepski’s method <span class="citation" data-cites="wK1jbWkS">[<a href="#ref-wK1jbWkS" role="doc-biblioref">21</a>]</span>
- Aggregation of classifiers <span class="citation" data-cites="12npMQT1q">[<a href="#ref-12npMQT1q" role="doc-biblioref">22</a>]</span></p>
<h3 id="adaptivity-is-bounded-by-data-efficiency">5. Adaptivity is bounded by data efficiency</h3>
<p><span class="citation" data-cites="IdMLJI3A">[<a href="#ref-IdMLJI3A" role="doc-biblioref">23</a>]</span></p>
<h3 id="adaptivity-is-not-a-free-lunch">6. Adaptivity is not a free lunch</h3>
<p>Adaptivity improves performance when heterogeneity is real and informative, but it can degrade performance when variation is spurious. Key tradeoffs include:</p>
<ul>
<li><strong>Bias vs. variance</strong>: More flexible adaptation can reduce bias but increase variance</li>
<li><strong>Stability vs. personalization</strong>: Highly adaptive models may overfit to noise or adversarial context</li>
<li><strong>Inference cost</strong>: Adaptive inference may be more computationally intensive than global prediction</li>
</ul>
<p>Understanding these tradeoffs is essential when designing systems for real-world deployment.</p>
<h3 id="when-adaptivity-fails-common-failure-modes">When Adaptivity Fails: Common Failure Modes</h3>
<p>Even when all the ingredients are present, adaptivity can backfire. Common failure modes include:</p>
<ul>
<li>Spurious Adaptation: Adapting to unstable or confounded features <span class="citation" data-cites="1FLMzrLE9">[<a href="#ref-1FLMzrLE9" role="doc-biblioref">24</a>]</span></li>
<li>Overfitting in Low-Data Contexts: Attempting fine-grained adaptation with insufficient signal</li>
<li>Modularity Mis-specification: Adapting in the wrong units or groupings <span class="citation" data-cites="Jm8Kx8HW">[<a href="#ref-Jm8Kx8HW" role="doc-biblioref">25</a>]</span></li>
<li>Feedback Loops: Models that change the data distribution they rely on <span class="citation" data-cites="MGkiKe9y">[<a href="#ref-MGkiKe9y" role="doc-biblioref">26</a>]</span></li>
</ul>
<p>Related references:</p>
<h2 id="explicit-adaptivity-structured-estimation-of-fc">Explicit Adaptivity: Structured Estimation of <span class="math inline">\(f(c)\)</span></h2>
<p>In classical statistical modeling, all observations are typically assumed to share a common set of parameters. However, modern datasets often display significant heterogeneity across individuals, locations, or experimental conditions, making this assumption unrealistic in many real-world applications. To better capture such heterogeneity, recent approaches model parameters as explicit functions of observed context, formalized as <span class="math inline">\(\theta_i = f(c_i)\)</span>, where <span class="math inline">\(f\)</span> maps each context to a sample-specific parameter <span class="citation" data-cites="nJe4KvW1">[<a href="#ref-nJe4KvW1" role="doc-biblioref">27</a>]</span>.</p>
<p>This section systematically reviews explicit adaptivity methods, with a focus on structured estimation of <span class="math inline">\(f(c)\)</span>. We begin by revisiting classical varying-coefficient models, which provide a conceptual and methodological foundation for modeling context-dependent effects. We then categorize recent advances in explicit adaptivity according to three principal strategies for estimating <span class="math inline">\(f(c)\)</span>: (1) smooth nonparametric models that generalize classical techniques, (2) structurally constrained models that incorporate domain-specific knowledge such as spatial or network structure, and (3) learned function approximators that leverage machine learning methods for high-dimensional or complex contexts. Finally, we summarize key theoretical developments and highlight promising directions for future research in this rapidly evolving field.</p>
<h3 id="classical-varying-coefficient-models-a-foundation">Classical Varying-Coefficient Models: A Foundation</h3>
<p>Varying-coefficient models (VCMs) are a foundational tool for modeling heterogeneity, as they allow model parameters to vary smoothly with observed context variables <span class="citation" data-cites="nJe4KvW1 5WaW1b3E 159keZX88">[<a href="#ref-nJe4KvW1" role="doc-biblioref">27</a>,<a href="#ref-5WaW1b3E" role="doc-biblioref">28</a>,<a href="#ref-159keZX88" role="doc-biblioref">29</a>]</span>. In their original formulation, the regression coefficients are treated as nonparametric functions of low-dimensional covariates, such as time or age. The standard VCM takes the form
<span class="math display">\[
y_i = \sum_{j=1}^{p} \beta_j(c_i) x_{ij} + \varepsilon_i,
\]</span>
where each <span class="math inline">\(\beta_j(c)\)</span> is an unknown smooth function, typically estimated using kernel smoothing, local polynomials, or penalized splines <span class="citation" data-cites="5WaW1b3E">[<a href="#ref-5WaW1b3E" role="doc-biblioref">28</a>]</span>.</p>
<p>This approach provides greater flexibility than fixed-coefficient models and is widely used for longitudinal and functional data analysis. The assumption of smoothness makes estimation and theoretical analysis more tractable, but also imposes limitations. Classical VCMs work best when the context is low-dimensional and continuous. They may struggle with abrupt changes, discontinuities, or high-dimensional and structured covariates. In such cases, interpretability and accuracy can be compromised, motivating the development of a variety of modern extensions, which will be discussed in the following sections.</p>
<h3 id="advances-in-modeling-fc">Advances in Modeling <span class="math inline">\(f(c)\)</span></h3>
<p>Recent years have seen substantial progress in the modeling of <span class="math inline">\(f(c)\)</span>, the function mapping context to model parameters. These advances can be grouped into three major strategies: (1) <strong>smooth non-parametric models</strong> that extend classical flexibility; (2) <strong>structurally constrained approaches</strong> that encode domain knowledge such as spatial or network topology; and (3) high-capacity <strong>learned function approximators</strong> from machine learning designed for high-dimensional, unstructured contexts. Each strategy addresses specific challenges in modeling heterogeneity, and together they provide a comprehensive toolkit for explicit adaptivity.</p>
<h4 id="smooth-non-parametric-models">Smooth Non-parametric Models</h4>
<p>This family of models generalizes the classical VCM by expressing <span class="math inline">\(f(c)\)</span> as a flexible, smooth function estimated with basis expansions and regularization. Common approaches include spline-based methods, local polynomial regression, and RKHS-based frameworks. For instance, <span class="citation" data-cites="5WaW1b3E">[<a href="#ref-5WaW1b3E" role="doc-biblioref">28</a>]</span> developed a semi-nonparametric VCM using RKHS techniques for imaging genetics, enabling the model to capture complex nonlinear effects. Such methods are central to generalized additive models, supporting both flexibility and interpretability. Theoretical work has shown that penalized splines and kernel methods offer strong statistical guarantees in moderate dimensions, although computational cost and overfitting can become issues as the dimension of <span class="math inline">\(c\)</span> increases.</p>
<h4 id="structurally-constrained-models">Structurally Constrained Models</h4>
<p>Another direction focuses on incorporating structural information into <span class="math inline">\(f(c)\)</span>, especially when the context is discrete, clustered, or topologically organized.</p>
<p><strong>Piecewise-Constant and Partition-Based Models.</strong>
Here, model parameters are allowed to remain constant within specific regions or clusters of the context space, rather than vary smoothly. Approaches include classical grouped estimators and modern partition models, which may learn changepoints using regularization tools like total variation penalties or the fused lasso. This framework is particularly effective for data with abrupt transitions or heterogeneous subgroups.</p>
<p><strong>Structured Regularization for Spatial, Graph, and Network Data.</strong>
When context exhibits known structure, regularization terms can be designed to promote similarity among neighboring coefficients <span class="citation" data-cites="f8UfQHob">[<a href="#ref-f8UfQHob" role="doc-biblioref">30</a>]</span>. For example, spatially varying-coefficient models have been applied to problems in geographical analysis and econometrics, where local effects are expected to vary across adjacent regions <span class="citation" data-cites="HmeksLbr YcdobJB OWMnVMab UiqiqyOG">[<a href="#ref-HmeksLbr" role="doc-biblioref">31</a>,<a href="#ref-YcdobJB" role="doc-biblioref">32</a>,<a href="#ref-OWMnVMab" role="doc-biblioref">33</a>,<a href="#ref-UiqiqyOG" role="doc-biblioref">34</a>]</span>. On networked data, the network VCM of <span class="citation" data-cites="eIFyeYdp">[<a href="#ref-eIFyeYdp" role="doc-biblioref">35</a>]</span> generalizes these ideas by learning both the latent positions and the parameter functions on graphs, allowing the model to accommodate complex relational heterogeneity. Such structural constraints allow models to leverage domain knowledge, improving efficiency and interpretability where smooth models may struggle.</p>
<h4 id="learned-function-approximators">Learned Function Approximators</h4>
<p>A third class of methods is rooted in modern machine learning, leveraging high-capacity models to approximate <span class="math inline">\(f(c)\)</span> directly from data. These approaches are especially valuable when the context is high-dimensional or unstructured, where classical assumptions may no longer be sufficient.</p>
<p><strong>Tree-Based Ensembles.</strong>
Gradient boosting decision trees (GBDTs) and related ensemble methods are well suited to tabular and mixed-type contexts. The framework developed by <span class="citation" data-cites="VphcPzBL">[<a href="#ref-VphcPzBL" role="doc-biblioref">36</a>]</span> extends varying-coefficient models by integrating gradient boosting, achieving strong predictive performance with a level of interpretability. These models are typically easier to train and tune than deep neural networks, and their structure lends itself to interpretation with tools such as SHAP.</p>
<p><strong>Deep Neural Networks.</strong>
For contexts defined by complex, high-dimensional features such as images, text, or sequential data, deep neural networks offer unique advantages for modeling <span class="math inline">\(f(c)\)</span>. These architectures can learn adaptive, data-driven representations that capture intricate relationships beyond the scope of classical models. Applications include personalized medicine, natural language processing, and behavioral science, where outcomes may depend on subtle or latent features of the context.</p>
<p>The decision between these machine learning approaches depends on the specific characteristics of the data, the priority placed on interpretability, and computational considerations. Collectively, these advances have significantly broadened the scope of explicit adaptivity, making it feasible to model heterogeneity in ever more complex settings.</p>
<h3 id="key-theoretical-advances">Key Theoretical Advances</h3>
<p>The expanding landscape of varying-coefficient models (VCMs) has been supported by substantial theoretical progress, which secures the validity of flexible modeling strategies and guides their practical use. The nature of these theoretical results often reflects the core structural assumptions of each model class.</p>
<p><strong>Theory for Smooth Non-parametric Models.</strong>
For classical VCMs based on kernel smoothing, local polynomial estimation, or penalized splines, extensive theoretical work has characterized their convergence rates and statistical efficiency. Under standard regularity conditions, these estimators are known to achieve minimax optimality for function estimation in moderate dimensions <span class="citation" data-cites="nJe4KvW1">[<a href="#ref-nJe4KvW1" role="doc-biblioref">27</a>]</span>. Recent developments, such as the work of <span class="citation" data-cites="5WaW1b3E">[<a href="#ref-5WaW1b3E" role="doc-biblioref">28</a>]</span>, have established asymptotic normality in semi-nonparametric settings, which enables valid confidence interval construction and hypothesis testing even in complex applications.</p>
<p><strong>Theory for Structurally Constrained Models.</strong>
When discrete or network structure is incorporated into VCMs, theoretical analysis focuses on identifiability, regularization properties, and conditions for consistent estimation. For example, <span class="citation" data-cites="eIFyeYdp">[<a href="#ref-eIFyeYdp" role="doc-biblioref">35</a>]</span> provide non-asymptotic error bounds for estimators in network VCMs, demonstrating that consistency can be attained when the underlying graph topology satisfies certain connectivity properties. In piecewise-constant and partition-based models, results from change-point analysis and total variation regularization guarantee that abrupt parameter changes can be recovered accurately under suitable sparsity and signal strength conditions.</p>
<p><strong>Theory for High-Capacity and Learned Models.</strong>
The incorporation of machine learning models into VCMs introduces new theoretical challenges. For high-dimensional and sparse settings, oracle inequalities and penalized likelihood theory establish conditions for consistent variable selection and accurate estimation, as seen in methods based on boosting and other regularization techniques <span class="citation" data-cites="VphcPzBL 1GTolz1Ss">[<a href="#ref-VphcPzBL" role="doc-biblioref">36</a>,<a href="#ref-1GTolz1Ss" role="doc-biblioref">37</a>]</span>. In the context of neural network-based VCMs, the theory is still developing, with current research focused on understanding generalization properties and identifiability in non-convex optimization. This remains an active and important frontier for both statistical and machine learning communities.</p>
<p>These theoretical advances provide a rigorous foundation for explicit adaptivity, ensuring that VCMs can be deployed confidently across a wide range of complex and structured modeling scenarios.</p>
<h3 id="synthesis-and-future-directions">Synthesis and Future Directions</h3>
<p>Selecting an appropriate modeling strategy for <span class="math inline">\(f(c)\)</span> involves weighing flexibility, interpretability, computational cost, and the extent of available domain knowledge. Learned function approximators, such as deep neural networks, offer unmatched capacity for modeling complex, high-dimensional relationships. However, classical smooth models and structurally constrained approaches often provide greater interpretability, transparency, and statistical efficiency. The choice of prior assumptions and the scalability of the estimation procedure are also central considerations in applied contexts.</p>
<p>Looking forward, several trends are shaping the field. One important direction is the integration of varying-coefficient models with foundation models from natural language processing and computer vision. By using pre-trained embeddings as context variables <span class="math inline">\(c_i\)</span>, it becomes possible to incorporate large amounts of prior knowledge and extend VCMs to multi-modal and unstructured data sources. Another active area concerns the principled combination of cross-modal contexts, bringing together information from text, images, and structured covariates within a unified VCM framework.</p>
<p>Advances in interpretability and visualization for high-dimensional or black-box coefficient functions are equally important. Developing tools that allow users to understand and trust model outputs is critical for the adoption of VCMs in sensitive areas such as healthcare and policy analysis.</p>
<p>Finally, closing the gap between methodological innovation and practical deployment remains a priority. Although the literature has produced many powerful variants of VCMs, practical adoption is often limited by the availability of software and the clarity of methodological guidance <span class="citation" data-cites="159keZX88">[<a href="#ref-159keZX88" role="doc-biblioref">29</a>]</span>. Continued investment in user-friendly implementations, open-source libraries, and empirical benchmarks will facilitate broader adoption and greater impact.</p>
<p>In summary, explicit adaptivity through structured estimation of <span class="math inline">\(f(c)\)</span> now forms a core paradigm at the interface of statistical modeling and machine learning. Future progress will focus not only on expanding the expressive power of these models, but also on making them more accessible, interpretable, and practically useful in real-world applications.</p>
<h2 id="implicit-adaptivity-emergent-contextualization-within-complex-models">Implicit Adaptivity: Emergent Contextualization within Complex Models</h2>
<p>Not all models adapt through explicit parameterization. In many modern systems, adaptation emerges from architecture, training data, or inference dynamics—without being hard-coded as a function of context.</p>
<p>We refer to this as <em>implicit adaptivity</em>. These methods do not model <span class="math inline">\(\theta_i\)</span> directly as a function of <span class="math inline">\(c_i\)</span>, nor do they always define context formally. Instead, they internalize patterns across training distributions in a way that enables flexible behavior at inference time.</p>
<p>A canonical example is <strong>in-context learning</strong> with foundation models. Given a prompt consisting of a few examples, the model adjusts its behavior—often achieving personalization or task adaptation—without updating weights or making any explicit inference over <span class="math inline">\(\theta\)</span>. This capacity arises from pretraining on diverse data and from the model’s architecture, not from structured estimation.</p>
<p>Other forms of implicit adaptivity include:</p>
<ul>
<li><strong>Fine-tuned models</strong> that generalize across tasks or domains by adjusting shared components.</li>
<li><strong>Attention-based architectures</strong> that condition on context without defining a parametric mapping.</li>
<li><strong>Gradient-based meta-learners</strong> trained to produce fast adaptation without modeling <span class="math inline">\(\theta(c)\)</span> explicitly.</li>
</ul>
<p>These methods challenge the boundary between training and inference. They blur the distinction between model parameters and data inputs, and they rely on massive-scale training to amortize the cost of adaptation.</p>
<p>In this section, we examine:</p>
<ul>
<li>How implicit adaptivity arises in foundation models</li>
<li>What assumptions these models make (implicitly or explicitly) about context</li>
<li>How their performance compares to structured, explicit approaches</li>
<li>When it’s valuable to make the adaptation process more interpretable or modular</li>
</ul>
<p>Implicit adaptivity offers powerful capabilities, but it also hides structure that could be useful for analysis, debugging, or control. The next section explores efforts to <em>make the implicit explicit</em>—by approximating, interpreting, or extracting the latent adaptation mechanisms inside black-box models.</p>
<h3 id="defining-implicit-adaptation">Defining Implicit Adaptation</h3>
<h3 id="neural-networks-with-context-inputs-e.g.-interaction-effects-attention-mechanisms-etc.">Neural Networks with context inputs (e.g. interaction effects, attention mechanisms, etc.)</h3>
<h3 id="amortized-inference-and-meta-learning">Amortized Inference and Meta-Learning</h3>
<h3 id="in-context-learning-in-transformers-and-foundation-models">In-context learning in transformers and foundation models</h3>
<h2 id="making-implicit-adaptivity-explicit-local-models-surrogates-and-post-hoc-approximations">Making Implicit Adaptivity Explicit: Local Models, Surrogates and Post Hoc Approximations</h2>
<p>This section focuses on methods that aim to extract, approximate, or control the internal adaptivity mechanisms of black-box models. These approaches recognize that implicit adaptivity—while powerful—can be opaque, hard to debug, and brittle to distribution shift. By surfacing structure, we gain interpretability, composability, and sometimes improved generalization.</p>
<h3 id="motivation">Motivation</h3>
<ul>
<li>Implicit adaptivity can succeed without explicit modeling, but:
<ul>
<li>It obscures <em>why</em> and <em>how</em> a model adapts</li>
<li>It limits modular reuse and inspection</li>
<li>It makes personalization hard to constrain or audit</li>
</ul></li>
<li>Making adaptivity explicit supports:
<ul>
<li>Better alignment with downstream goals</li>
<li>Composability of learned modules</li>
<li>Debugging and error attribution</li>
</ul></li>
</ul>
<h3 id="approaches">Approaches</h3>
<h4 id="surrogate-modeling">Surrogate Modeling</h4>
<ul>
<li>Fit interpretable surrogates (e.g., linear models, decision trees) to approximate model behavior locally</li>
<li>Applications:
<ul>
<li>Explaining predictions post-hoc</li>
<li>Approximating <span class="math inline">\(f(c)\)</span> from input-output behavior</li>
</ul></li>
<li>References:
<ul>
<li>LIME, SHAP</li>
</ul></li>
</ul>
<h4 id="prototype-and-nearest-neighbor-methods">Prototype and Nearest-Neighbor Methods</h4>
<ul>
<li>Use nearest neighbors in representation space to approximate model adaptation</li>
<li>Enables interpretability and modular updates</li>
<li>Related to contextual bandits, exemplar models</li>
</ul>
<h4 id="amortization-diagnostics">Amortization Diagnostics</h4>
<ul>
<li>For amortized inference (e.g., variational autoencoders), analyze encoder mappings to understand how <span class="math inline">\(q(\theta | x)\)</span> varies with <span class="math inline">\(x\)</span></li>
<li>Could treat encoder as a learned <span class="math inline">\(f(c)\)</span> and evaluate its fidelity</li>
</ul>
<h4 id="disentangled-representations">Disentangled Representations</h4>
<ul>
<li>Train models with constraints (e.g., variational regularization, info bottlenecks) to encourage explicit factors of variation</li>
<li>Goal: make parameter changes traceable to distinct contextual causes</li>
</ul>
<h4 id="parameter-extraction">Parameter Extraction</h4>
<ul>
<li>Techniques like linear probes, weight attribution, or synthetic tasks to reverse-engineer how models adapt internally</li>
<li>Example: “what part of the weights encode the task?”</li>
</ul>
<h3 id="tradeoffs">Tradeoffs</h3>
<ul>
<li>Fidelity vs interpretability</li>
<li>Local vs global explanations</li>
<li>Approximation error vs modular control</li>
</ul>
<h3 id="open-questions">Open Questions</h3>
<ul>
<li>Can we extract <em>portable</em> modules from foundation models?</li>
<li>When does making structure explicit improve performance?</li>
<li>What is the right level of abstraction—parameters, functions, latent causes?</li>
</ul>
<p>This section bridges black-box adaptation and structured inference. It highlights how interpretability and performance need not be at odds—especially when the goal is robust, composable, and trustworthy adaptation.</p>
<p>TODO: Discussing the implications of context-adaptive interpretations for traditional models. Related work including LIME/DeepLift/DeepSHAP.</p>
<p>Relevant references:</p>
<ul>
<li><span class="citation" data-cites="1DNxggO7I">[<a href="#ref-1DNxggO7I" role="doc-biblioref">38</a>]</span></li>
<li>Interpretations are statistics <span class="citation" data-cites="tEt0Suzf">[<a href="#ref-tEt0Suzf" role="doc-biblioref">39</a>]</span></li>
</ul>
<h2 id="context-invariant-training-a-view-from-the-converse">Context-Invariant Training: A View from the Converse</h2>
<p>TODO: The converse of context-adaptive models, exploring the implications of training context-invariant models.
e.g. out-of-distribution generalization, robustness to adversarial attacks.</p>
<p>Relevant references:</p>
<ul>
<li>Invariant Risk Minimization <span class="citation" data-cites="1DeCFT6PA">[<a href="#ref-1DeCFT6PA" role="doc-biblioref">40</a>]</span></li>
<li>Out-of-Distribution Generalization via Risk Extrapolation <span class="citation" data-cites="11IMWGprl">[<a href="#ref-11IMWGprl" role="doc-biblioref">41</a>]</span></li>
<li>The Risks of Invariant Risk Minimization <span class="citation" data-cites="1FLMzrLE9">[<a href="#ref-1FLMzrLE9" role="doc-biblioref">24</a>]</span></li>
<li>Conditional Variance Penalties and Domain Adaptation <span class="citation" data-cites="L6xa6qzg">[<a href="#ref-L6xa6qzg" role="doc-biblioref">42</a>]</span></li>
<li>Can Subpopulation Shifts Explain Disagreement in Model Generalization? <span class="citation" data-cites="HzzgJQN0">[<a href="#ref-HzzgJQN0" role="doc-biblioref">17</a>]</span></li>
</ul>
<h3 id="adversarial-robustness-as-context-invariant-training">Adversarial Robustness as Context-Invariant Training</h3>
<p>Related references:</p>
<ul>
<li>Towards Deep Learning Models Resistant to Adversarial Attacks <span class="citation" data-cites="B0L8KJ8W">[<a href="#ref-B0L8KJ8W" role="doc-biblioref">43</a>]</span></li>
<li>Robustness May Be at Odds with Accuracy <span class="citation" data-cites="ylSNfYug">[<a href="#ref-ylSNfYug" role="doc-biblioref">44</a>]</span></li>
</ul>
<h3 id="training-methods-for-context-invariant-models">Training methods for Context-Invariant Models</h3>
<ul>
<li>Just Train Twice: Improving Group Robustness without Training Group Information <span class="citation" data-cites="lwLz9yZT">[<a href="#ref-lwLz9yZT" role="doc-biblioref">45</a>]</span></li>
<li>Environment Inference for Invariant Learning <span class="citation" data-cites="pn0xfUyE">[<a href="#ref-pn0xfUyE" role="doc-biblioref">46</a>]</span></li>
<li>Distributionally Robust Neural Networks for Group Shifts <span class="citation" data-cites="Jm8Kx8HW">[<a href="#ref-Jm8Kx8HW" role="doc-biblioref">25</a>]</span></li>
</ul>
<h2 id="applications-case-studies-evaluation-metrics-and-tools">Applications, Case Studies, Evaluation Metrics, and Tools</h2>
<h3 id="implementation-across-sectors">Implementation Across Sectors</h3>
<p>TODO: Detailed examination of context-adaptive models in sectors like healthcare and finance.</p>
<p>Relevant references:</p>
<ul>
<li><span class="citation" data-cites="14C9q1ybi">[<a href="#ref-14C9q1ybi" role="doc-biblioref">47</a>]</span></li>
<li><span class="citation" data-cites="dvWlfk21">[<a href="#ref-dvWlfk21" role="doc-biblioref">48</a>]</span></li>
</ul>
<h3 id="performance-evaluation">Performance Evaluation</h3>
<p>TODO: Successes, failures, and comparative analyses of context-adaptive models across applications.</p>
<h3 id="survey-of-tools">Survey of Tools</h3>
<p>TODO: Reviewing current technological supports for context-adaptive models.</p>
<h3 id="selection-and-usage-guidance">Selection and Usage Guidance</h3>
<p>TODO: Offering practical advice on tool selection and use for optimal outcomes.</p>
<h2 id="future-trends-and-opportunities-with-foundation-models">Future Trends and Opportunities with Foundation Models</h2>
<h3 id="emerging-technologies">Emerging Technologies</h3>
<p>TODO: Identifying upcoming technologies and predicting their impact on context-adaptive learning.</p>
<h3 id="advances-in-methodologies">Advances in Methodologies</h3>
<p>TODO: Speculating on potential future methodological enhancements.</p>
<h3 id="expanding-frameworks-with-foundation-models">Expanding Frameworks with Foundation Models</h3>
<p>Foundation models refer to large-scale, general-purpose neural networks, predominantly transformer-based architectures, trained on vast datasets using self-supervised learning <span class="citation" data-cites="1GbAsSOZV">[<a href="#ref-1GbAsSOZV" role="doc-biblioref">49</a>]</span>. These models have significantly transformed modern statistical modeling and machine learning due to their flexibility, adaptability, and strong performance across diverse domains. Notably, large language models (LLMs) such as GPT-4 <span class="citation" data-cites="17lpGtuH5">[<a href="#ref-17lpGtuH5" role="doc-biblioref">50</a>]</span> and LLaMA-3.1 <span class="citation" data-cites="yWg7tQr1">[<a href="#ref-yWg7tQr1" role="doc-biblioref">51</a>]</span> have achieved substantial advancements in natural language processing (NLP), demonstrating proficiency in tasks ranging from text generation and summarization to question-answering and dialogue systems. Beyond NLP, foundation models also excel in multimodal (text-vision) tasks <span class="citation" data-cites="17tnf46zM">[<a href="#ref-17tnf46zM" role="doc-biblioref">52</a>]</span>, text embedding generation <span class="citation" data-cites="urJgpE6q">[<a href="#ref-urJgpE6q" role="doc-biblioref">53</a>]</span>, and structured tabular data analysis <span class="citation" data-cites="rYveVDKJ">[<a href="#ref-rYveVDKJ" role="doc-biblioref">54</a>]</span>, highlighting their broad applicability.</p>
<p>A key strength of foundation models lies in their capacity to dynamically adapt to different contexts provided by inputs. This adaptability is primarily achieved through techniques such as prompting, which involves designing queries to guide the model’s behavior implicitly, allowing task-specific responses without additional fine-tuning <span class="citation" data-cites="12nAa0T4v">[<a href="#ref-12nAa0T4v" role="doc-biblioref">55</a>]</span>. Furthermore, mixture-of-experts (MoE) architectures amplify this contextual adaptability by employing routing mechanisms that select specialized sub-models or “experts” tailored to specific input data, thus optimizing computational efficiency and performance <span class="citation" data-cites="g7RE7G0h">[<a href="#ref-g7RE7G0h" role="doc-biblioref">56</a>]</span>.</p>
<h4 id="foundation-models-as-context"><strong>Foundation Models as Context</strong></h4>
<p>Foundation models offer significant opportunities by supplying context-aware information that enhances various stages of statistical modeling and inference:</p>
<p><strong>Feature Extraction and Interpretation:</strong> Foundation models transform raw, unstructured data into structured and interpretable representations. For example, targeted prompts enable LLMs to extract insightful features from text, providing meaningful insights and facilitating interpretability <span class="citation" data-cites="11RvF4F7q">[<a href="#ref-kAJDlMwy" role="doc-biblioref">59</a>]</span>. This allows statistical models to operate directly on semantically meaningful features rather than on raw, less interpretable data.</p>
<p><strong>Contextualized Representations for Downstream Modeling:</strong> Foundation models produce adaptable embeddings and intermediate representations useful as inputs for downstream models, such as decision trees or linear models <span class="citation" data-cites="R7y5TKp9">[<a href="#ref-R7y5TKp9" role="doc-biblioref">60</a>]</span>. These embeddings significantly enhance the training of both complex, black-box models <span class="citation" data-cites="1AazNaZYl">[<a href="#ref-1AazNaZYl" role="doc-biblioref">61</a>]</span> and simpler statistical methods like n-gram-based analyses <span class="citation" data-cites="HQXzkG4Q">[<a href="#ref-HQXzkG4Q" role="doc-biblioref">62</a>]</span>, thereby broadening the application scope and effectiveness of statistical approaches.</p>
<p><strong>Post-hoc Interpretability:</strong> Foundation models support interpretability by generating natural-language explanations for decisions made by complex models. This capability enhances transparency and trust in statistical inference, providing clear insights into how and why certain predictions or decisions are made <span class="citation" data-cites="XpHq6HEw">[<a href="#ref-XpHq6HEw" role="doc-biblioref">63</a>]</span>.</p>
<p>Recent innovations underscore the role of foundation models in context-sensitive inference and enhanced interpretability:</p>
<p><strong>FLAN-MoE</strong> (Fine-tuned Language Model with Mixture of Experts) <span class="citation" data-cites="zWwbz3cX">[<a href="#ref-zWwbz3cX" role="doc-biblioref">64</a>]</span> combines instruction tuning with expert selection, dynamically activating relevant sub-models based on the context. This method significantly improves performance across diverse NLP tasks, offering superior few-shot and zero-shot capabilities. It also facilitates interpretability through explicit expert activations. Future directions may explore advanced expert-selection techniques and multilingual capabilities.</p>
<p><strong>LMPriors</strong> (Pre-Trained Language Models as Task-Specific Priors) <span class="citation" data-cites="Xtwwrjzy">[<a href="#ref-Xtwwrjzy" role="doc-biblioref">65</a>]</span> leverages semantic insights from pre-trained models like GPT-3 to guide tasks such as causal inference, feature selection, and reinforcement learning. This method markedly enhances decision accuracy and efficiency without requiring extensive supervised datasets. However, it necessitates careful prompt engineering to mitigate biases and ethical concerns.</p>
<p><strong>Mixture of In-Context Experts</strong> (MoICE) <span class="citation" data-cites="Xtwwrjzy">[<a href="#ref-Xtwwrjzy" role="doc-biblioref">65</a>]</span> introduces a dynamic routing mechanism within attention heads, utilizing multiple Rotary Position Embeddings (RoPE) angles to effectively capture token positions in sequences. MoICE significantly enhances performance on long-context sequences and retrieval-augmented generation tasks by ensuring complete contextual coverage. Efficiency is achieved through selective router training, and interpretability is improved by explicitly visualizing attention distributions, providing detailed insights into the model’s reasoning process.</p>
<h2 id="open-problems">Open Problems</h2>
<h3 id="theoretical-challenges">Theoretical Challenges</h3>
<p>TODO: Critically examining unresolved theoretical issues like identifiability, etc.</p>
<h3 id="ethical-and-regulatory-considerations">Ethical and Regulatory Considerations</h3>
<p>TODO: Discussing the ethical landscape and regulatory challenges, with focus on benefits of interpretability and regulatability.</p>
<h3 id="complexity-in-implementation">Complexity in Implementation</h3>
<p>TODO: Addressing obstacles in practical applications and gathering insights from real-world data.</p>
<p>TODO: Other open problems?</p>
<h2 id="conclusion">Conclusion</h2>
<h3 id="overview-of-insights">Overview of Insights</h3>
<p>TODO: Summarizing the main findings and contributions of this review.</p>
<h3 id="future-directions">Future Directions</h3>
<p>TODO: Discussing potential developments and innovations in context-adaptive statistical inference.</p>
<h2 class="page_break_before" id="references">References</h2>
<!-- Explicitly insert bibliography here -->
<div id="refs" class="references csl-bib-body" role="doc-bibliography">
<div id="ref-ugXwusl0" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">1. </div><div class="csl-right-inline"><strong>Varying-Coefficient Models</strong> <div class="csl-block">Trevor Hastie, Robert Tibshirani</div> <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em> (1993-09-01) <a href="https://doi.org/gmfvmb">https://doi.org/gmfvmb</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1111/j.2517-6161.1993.tb01939.x">10.1111/j.2517-6161.1993.tb01939.x</a></div></div>
</div>
<div id="ref-TULLRYDp" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">2. </div><div class="csl-right-inline"><strong>Bayesian Edge Regression in Undirected Graphical Models to Characterize Interpatient Heterogeneity in Cancer</strong> <div class="csl-block">Zeya Wang, Veerabhadran Baladandayuthapani, Ahmed O Kaseb, Hesham M Amin, Manal M Hassan, Wenyi Wang, Jeffrey S Morris</div> <em>Journal of the American Statistical Association</em> (2022-01-05) <a href="https://doi.org/gt68hr">https://doi.org/gt68hr</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1080/01621459.2021.2000866">10.1080/01621459.2021.2000866</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/36090952">36090952</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9454401">PMC9454401</a></div></div>
</div>
<div id="ref-l6vMkIsa" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">3. </div><div class="csl-right-inline"><strong>Statistical estimation in varying coefficient models</strong> <div class="csl-block">Jianqing Fan, Wenyang Zhang</div> <em>The Annals of Statistics</em> (1999-10-01) <a href="https://doi.org/dsxd4s">https://doi.org/dsxd4s</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1214/aos/1017939139">10.1214/aos/1017939139</a></div></div>
</div>
<div id="ref-1CkxORTSX" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">4. </div><div class="csl-right-inline"><strong>Time-Varying Coefficient Model Estimation Through Radial Basis Functions</strong> <div class="csl-block">Juan Sosa, Lina Buitrago</div> <em>arXiv</em> (2021-03-02) <a href="https://arxiv.org/abs/2103.00315">https://arxiv.org/abs/2103.00315</a></div>
</div>
<div id="ref-SfCo6pSp" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">5. </div><div class="csl-right-inline"><strong>Contextual Explanation Networks</strong> <div class="csl-block">Maruan Al-Shedivat, Avinava Dubey, Eric P Xing</div> <em>arXiv</em> (2017) <a href="https://doi.org/gt68h9">https://doi.org/gt68h9</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1705.10301">10.48550/arxiv.1705.10301</a></div></div>
</div>
<div id="ref-HYsEq2UQ" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">6. </div><div class="csl-right-inline"><strong>Contextualized Machine Learning</strong> <div class="csl-block">Benjamin Lengerich, Caleb N Ellington, Andrea Rubbi, Manolis Kellis, Eric P Xing</div> <em>arXiv</em> (2023) <a href="https://doi.org/gt68jg">https://doi.org/gt68jg</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2310.11340">10.48550/arxiv.2310.11340</a></div></div>
</div>
<div id="ref-grNza1Og" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">7. </div><div class="csl-right-inline"><strong>NOTMAD: Estimating Bayesian Networks with Sample-Specific Structures and Parameters</strong> <div class="csl-block">Ben Lengerich, Caleb Ellington, Bryon Aragam, Eric P Xing, Manolis Kellis</div> <em>arXiv</em> (2021) <a href="https://doi.org/gt68jc">https://doi.org/gt68jc</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2111.01104">10.48550/arxiv.2111.01104</a></div></div>
</div>
<div id="ref-4cK1tiec" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">8. </div><div class="csl-right-inline"><strong>Contextualized: Heterogeneous Modeling Toolbox</strong> <div class="csl-block">Caleb N Ellington, Benjamin J Lengerich, Wesley Lo, Aaron Alvarez, Andrea Rubbi, Manolis Kellis, Eric P Xing</div> <em>Journal of Open Source Software</em> (2024-05-08) <a href="https://doi.org/gt68h8">https://doi.org/gt68h8</a> <div class="csl-block">DOI: <a href="https://doi.org/10.21105/joss.06469">10.21105/joss.06469</a></div></div>
</div>
<div id="ref-nYipTPML" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">9. </div><div class="csl-right-inline"><strong>Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning</strong> <div class="csl-block">Jannik Deuschel, Caleb N Ellington, Yingtao Luo, Benjamin J Lengerich, Pascal Friederich, Eric P Xing</div> <em>arXiv</em> (2023) <a href="https://doi.org/gt68jf">https://doi.org/gt68jf</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2310.07918">10.48550/arxiv.2310.07918</a></div></div>
</div>
<div id="ref-esxxcr9l" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">10. </div><div class="csl-right-inline"><strong>Automated interpretable discovery of heterogeneous treatment effectiveness: A COVID-19 case study</strong> <div class="csl-block">Benjamin J Lengerich, Mark E Nunnally, Yin Aphinyanaphongs, Caleb Ellington, Rich Caruana</div> <em>Journal of Biomedical Informatics</em> (2022-06) <a href="https://doi.org/gt68h5">https://doi.org/gt68h5</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/j.jbi.2022.104086">10.1016/j.jbi.2022.104086</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/35504543">35504543</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9055753">PMC9055753</a></div></div>
</div>
<div id="ref-O1UU4a5P" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">11. </div><div class="csl-right-inline"><strong>Discriminative Subtyping of Lung Cancers from Histopathology Images via Contextual Deep Learning</strong> <div class="csl-block">Benjamin J Lengerich, Maruan Al-Shedivat, Amir Alavi, Jennifer Williams, Sami Labbaki, Eric P Xing</div> <em>Cold Spring Harbor Laboratory</em> (2020-06-26) <a href="https://doi.org/gt68h6">https://doi.org/gt68h6</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1101/2020.06.25.20140053">10.1101/2020.06.25.20140053</a></div></div>
</div>
<div id="ref-Rt6voTFN" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">12. </div><div class="csl-right-inline"><strong>Learning to Estimate Sample-specific Transcriptional Networks for 7000 Tumors</strong> <div class="csl-block">Caleb N Ellington, Benjamin J Lengerich, Thomas BK Watkins, Jiekun Yang, Abhinav Adduri, Sazan Mahbub, Hanxi Xiao, Manolis Kellis, Eric P Xing</div> <em>Cold Spring Harbor Laboratory</em> (2023-12-04) <a href="https://doi.org/gt68h7">https://doi.org/gt68h7</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1101/2023.12.01.569658">10.1101/2023.12.01.569658</a></div></div>
</div>
<div id="ref-9S6tI5yv" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">13. </div><div class="csl-right-inline"><strong>Contextual Feature Selection with Conditional Stochastic Gates</strong> <div class="csl-block">Ram Dyuthi Sristi, Ofir Lindenbaum, Shira Lifshitz, Maria Lavzin, Jackie Schiller, Gal Mishne, Hadas Benisty</div> <em>arXiv</em> (2023) <a href="https://doi.org/gt68jh">https://doi.org/gt68jh</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2312.14254">10.48550/arxiv.2312.14254</a></div></div>
</div>
<div id="ref-lAsTg3IH" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">14. </div><div class="csl-right-inline"><strong>Estimating time-varying networks</strong> <div class="csl-block">Mladen Kolar, Le Song, Amr Ahmed, Eric P Xing</div> <em>The Annals of Applied Statistics</em> (2010-03-01) <a href="https://doi.org/b3rn6q">https://doi.org/b3rn6q</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1214/09-aoas308">10.1214/09-aoas308</a></div></div>
</div>
<div id="ref-k6r0UwSv" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">15. </div><div class="csl-right-inline"><strong>When Personalization Harms: Reconsidering the Use of Group Attributes in Prediction</strong> <div class="csl-block">Vinith M Suriyakumar, Marzyeh Ghassemi, Berk Ustun</div> <em>arXiv</em> (2022) <a href="https://doi.org/gt68jd">https://doi.org/gt68jd</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2206.02058">10.48550/arxiv.2206.02058</a></div></div>
</div>
<div id="ref-WlwUpYp" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">16. </div><div class="csl-right-inline"><strong>Learning Sample-Specific Models with Low-Rank Personalized Regression</strong> <div class="csl-block">Benjamin Lengerich, Bryon Aragam, Eric P Xing</div> <em>arXiv</em> (2019) <a href="https://doi.org/gt68jb">https://doi.org/gt68jb</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1910.06939">10.48550/arxiv.1910.06939</a></div></div>
</div>
<div id="ref-HzzgJQN0" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">17. </div><div class="csl-right-inline"><strong>Sketch-Based Anomaly Detection in Streaming Graphs</strong> <div class="csl-block">Siddharth Bhatia, Mohit Wadhwa, Kenji Kawaguchi, Neil Shah, Philip S Yu, Bryan Hooi</div> <em>arXiv</em> (2023-07-18) <a href="https://arxiv.org/abs/2106.04486">https://arxiv.org/abs/2106.04486</a></div>
</div>
<div id="ref-gSmt16Rh" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">18. </div><div class="csl-right-inline"><strong>Intelligible Models for HealthCare</strong> <div class="csl-block">Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, Noemie Elhadad</div> <em>Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em> (2015-08-10) <a href="https://doi.org/gftgxk">https://doi.org/gftgxk</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1145/2783258.2788613">10.1145/2783258.2788613</a></div></div>
</div>
<div id="ref-FJALdE9T" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">19. </div><div class="csl-right-inline"><strong>Adapting multi-armed bandits policies to contextual bandits scenarios</strong> <div class="csl-block">David Cortes</div> <em>arXiv</em> (2019-11-26) <a href="https://arxiv.org/abs/1811.04383">https://arxiv.org/abs/1811.04383</a></div>
</div>
<div id="ref-10151coVE" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">20. </div><div class="csl-right-inline"><strong>Environment Inference for Invariant Learning</strong> <div class="csl-block">Elliot Creager, JÃ¶rn-Henrik Jacobsen, Richard Zemel</div> <em>arXiv</em> (2021-07-16) <a href="https://arxiv.org/abs/2010.07249">https://arxiv.org/abs/2010.07249</a></div>
</div>
<div id="ref-wK1jbWkS" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">21. </div><div class="csl-right-inline"><strong>Lepski's Method and Adaptive Estimation of Nonlinear Integral Functionals of Density</strong> <div class="csl-block">Rajarshi Mukherjee, Eric Tchetgen Tchetgen, James Robins</div> <em>arXiv</em> (2016-01-12) <a href="https://arxiv.org/abs/1508.00249">https://arxiv.org/abs/1508.00249</a></div>
</div>
<div id="ref-12npMQT1q" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">22. </div><div class="csl-right-inline"><strong>Optimal Rates of Aggregation</strong> <div class="csl-block">Alexandre B Tsybakov</div> <em>Lecture Notes in Computer Science</em> (2003) <a href="https://doi.org/czntw5">https://doi.org/czntw5</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1007/978-3-540-45167-9_23">10.1007/978-3-540-45167-9_23</a></div></div>
</div>
<div id="ref-IdMLJI3A" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">23. </div><div class="csl-right-inline"><strong>Optimal Estimation of Change in a Population of Parameters</strong> <div class="csl-block">Ramya Korlakai Vinayak, Weihao Kong, Sham M Kakade</div> <em>arXiv</em> (2019-12-02) <a href="https://arxiv.org/abs/1911.12568">https://arxiv.org/abs/1911.12568</a></div>
</div>
<div id="ref-1FLMzrLE9" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">24. </div><div class="csl-right-inline"><strong>The Risks of Invariant Risk Minimization</strong> <div class="csl-block">Elan Rosenfeld, Pradeep Ravikumar, Andrej Risteski</div> <em>arXiv</em> (2021-03-30) <a href="https://arxiv.org/abs/2010.05761">https://arxiv.org/abs/2010.05761</a></div>
</div>
<div id="ref-Jm8Kx8HW" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">25. </div><div class="csl-right-inline"><strong>Distributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Case Generalization</strong> <div class="csl-block">Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, Percy Liang</div> <em>arXiv</em> (2020-04-03) <a href="https://arxiv.org/abs/1911.08731">https://arxiv.org/abs/1911.08731</a></div>
</div>
<div id="ref-MGkiKe9y" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">26. </div><div class="csl-right-inline"><strong>The Selective Labels Problem</strong> <div class="csl-block">Himabindu Lakkaraju, Jon Kleinberg, Jure Leskovec, Jens Ludwig, Sendhil Mullainathan</div> <em>Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em> (2017-08-04) <a href="https://doi.org/ggd7hz">https://doi.org/ggd7hz</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1145/3097983.3098066">10.1145/3097983.3098066</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/29780658">29780658</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5958915">PMC5958915</a></div></div>
</div>
<div id="ref-nJe4KvW1" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">27. </div><div class="csl-right-inline"><strong>Varying-coefficient models</strong> <div class="csl-block">Trevor Hastie, Robert Tibshirani</div> <em>Journal of the Royal Statistical Society: Series B (Methodological)</em></div>
</div>
<div id="ref-5WaW1b3E" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">28. </div><div class="csl-right-inline"><strong>Semi-nonparametric Varying Coefficients Models for Imaging Genetics</strong> <div class="csl-block">Ting Li, Yang Yu, Xiao Wang, JS Marron, Hongtu Zhu</div> <em>Statistica Sinica</em> <div class="csl-block">DOI: <a href="https://doi.org/10.5705/ss.202024.0118">10.5705/ss.202024.0118</a></div></div>
</div>
<div id="ref-159keZX88" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">29. </div><div class="csl-right-inline"><strong>Publication Trends on the Varying Coefficients Model: Estimating the Actual (Under)Utilization of a Highly Acclaimed Method for Studying Statistical Interactions</strong> <div class="csl-block">Assaf Botzer</div> <em>Publications</em> <div class="csl-block">DOI: <a href="https://doi.org/10.3390/publications13020019">10.3390/publications13020019</a></div></div>
</div>
<div id="ref-f8UfQHob" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">30. </div><div class="csl-right-inline"><strong>Graph-Regularized Estimation for Context-Varying Models</strong> <div class="csl-block">Yu Shi, Yang Liu, Hao Yan</div></div>
</div>
<div id="ref-HmeksLbr" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">31. </div><div class="csl-right-inline"><strong>Fast Spatio-Temporally Varying Coefficient Modeling With Reluctant Interaction Selection</strong> <div class="csl-block">Daisuke Murakami, Shinichiro Shirota, Seiji Kajita, Mami Kajita</div> <em>Geographical Analysis</em> <div class="csl-block">DOI: <a href="https://doi.org/10.1111/gean.70005">10.1111/gean.70005</a></div></div>
</div>
<div id="ref-YcdobJB" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">32. </div><div class="csl-right-inline"><strong>Spatially Varying Coefficient Models for Estimating Heterogeneous Mixture Effects</strong> <div class="csl-block">Jacob Englert, Howard Chang</div></div>
</div>
<div id="ref-OWMnVMab" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">33. </div><div class="csl-right-inline"><strong>Varying-Coefficient Panel Models with Spatial Dependence</strong> <div class="csl-block">Yiqing Hu, Qingyuan Zhao</div> <em>Journal of Econometrics</em> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/j.jeconom.2024.105883">10.1016/j.jeconom.2024.105883</a></div></div>
</div>
<div id="ref-UiqiqyOG" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">34. </div><div class="csl-right-inline"><strong>Urban Economic Modeling with Spatially Varying Coefficients</strong> <div class="csl-block">Chongliang Luo, Yihong Du, Peng Zhao</div> <em>Regional Science and Urban Economics</em> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/j.regsciurbeco.2024.104009">10.1016/j.regsciurbeco.2024.104009</a></div></div>
</div>
<div id="ref-eIFyeYdp" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">35. </div><div class="csl-right-inline"><strong>Network Varying Coefficient Model</strong> <div class="csl-block">Xinyan Fan, Kuangnan Fang, Wei Lan, Chih-Ling Tsai</div> <em>Journal of the American Statistical Association</em> <div class="csl-block">DOI: <a href="https://doi.org/10.1080/01621459.2025.2470481">10.1080/01621459.2025.2470481</a></div></div>
</div>
<div id="ref-VphcPzBL" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">36. </div><div class="csl-right-inline"><strong>Boosted Trees for Varying-Coefficient Models</strong> <div class="csl-block">Yunfei Wang, Qiang Sun</div> <em>Machine Learning</em> <div class="csl-block">DOI: <a href="https://doi.org/10.1007/s00180-025-01603-8">10.1007/s00180-025-01603-8</a></div></div>
</div>
<div id="ref-1GTolz1Ss" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">37. </div><div class="csl-right-inline"><strong>XGBoost-Inspired Estimation for High-Dimensional Varying Coefficient Models</strong> <div class="csl-block">Yu Cheng, Dongdong Yang, Denny Zhou</div></div>
</div>
<div id="ref-1DNxggO7I" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">38. </div><div class="csl-right-inline"><strong>In-Context Explainers: Harnessing LLMs for Explaining Black Box Models</strong> <div class="csl-block">Nicholas Kroeger, Dan Ley, Satyapriya Krishna, Chirag Agarwal, Himabindu Lakkaraju</div> <em>arXiv</em> (2024-07-12) <a href="https://arxiv.org/abs/2310.05797">https://arxiv.org/abs/2310.05797</a></div>
</div>
<div id="ref-tEt0Suzf" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">39. </div><div class="csl-right-inline"><strong>Rethinking Explainable Machine Learning as Applied Statistics</strong> <div class="csl-block">Sebastian Bordt, Eric Raidl, Ulrike von Luxburg</div> <em>arXiv</em> (2025-06-17) <a href="https://arxiv.org/abs/2402.02870">https://arxiv.org/abs/2402.02870</a></div>
</div>
<div id="ref-1DeCFT6PA" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">40. </div><div class="csl-right-inline"><strong>Invariant Risk Minimization</strong> <div class="csl-block">Martin Arjovsky, LÃ©on Bottou, Ishaan Gulrajani, David Lopez-Paz</div> <em>arXiv</em> (2020-03-31) <a href="https://arxiv.org/abs/1907.02893">https://arxiv.org/abs/1907.02893</a></div>
</div>
<div id="ref-11IMWGprl" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">41. </div><div class="csl-right-inline"><strong>Out-of-Distribution Generalization via Risk Extrapolation (REx)</strong> <div class="csl-block">David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, Aaron Courville</div> <em>arXiv</em> (2021-02-26) <a href="https://arxiv.org/abs/2003.00688">https://arxiv.org/abs/2003.00688</a></div>
</div>
<div id="ref-L6xa6qzg" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">42. </div><div class="csl-right-inline"><strong>Conditional Variance Penalties and Domain Shift Robustness</strong> <div class="csl-block">Christina Heinze-Deml, Nicolai Meinshausen</div> <em>arXiv</em> (2019-04-16) <a href="https://arxiv.org/abs/1710.11469">https://arxiv.org/abs/1710.11469</a></div>
</div>
<div id="ref-B0L8KJ8W" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">43. </div><div class="csl-right-inline"><strong>Towards Deep Learning Models Resistant to Adversarial Attacks</strong> <div class="csl-block">Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu</div> <em>arXiv</em> (2019-09-06) <a href="https://arxiv.org/abs/1706.06083">https://arxiv.org/abs/1706.06083</a></div>
</div>
<div id="ref-ylSNfYug" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">44. </div><div class="csl-right-inline"><strong>Robustness May Be at Odds with Accuracy</strong> <div class="csl-block">Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, Aleksander Madry</div> <em>arXiv</em> (2019-09-10) <a href="https://arxiv.org/abs/1805.12152">https://arxiv.org/abs/1805.12152</a></div>
</div>
<div id="ref-lwLz9yZT" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">45. </div><div class="csl-right-inline"><strong>On the Sample Complexity of Adversarial Multi-Source PAC Learning</strong> <div class="csl-block">Nikola Konstantinov, Elias Frantar, Dan Alistarh, Christoph H Lampert</div> <em>arXiv</em> (2020-07-01) <a href="https://arxiv.org/abs/2002.10384">https://arxiv.org/abs/2002.10384</a></div>
</div>
<div id="ref-pn0xfUyE" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">46. </div><div class="csl-right-inline"><strong>Conflict-Averse Gradient Descent for Multi-task Learning</strong> <div class="csl-block">Bo Liu, Xingchao Liu, Xiaojie Jin, Peter Stone, Qiang Liu</div> <em>arXiv</em> (2024-02-22) <a href="https://arxiv.org/abs/2110.14048">https://arxiv.org/abs/2110.14048</a></div>
</div>
<div id="ref-14C9q1ybi" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">47. </div><div class="csl-right-inline"><strong>Exact Inference for Transformed Large-Scale Varying Coefficient Models with Applications</strong> <div class="csl-block">Tianyu Chen, Robert Habans, Thomas Douthat, Jenna Losh, Lida Chalangar Jalili Dehkharghani, Li-Hsiang Lin</div> <em>Journal of Data Science</em> (2025-01-01) <a href="https://doi.org/g9t2rs">https://doi.org/g9t2rs</a> <div class="csl-block">DOI: <a href="https://doi.org/10.6339/25-jds1181">10.6339/25-jds1181</a></div></div>
</div>
<div id="ref-dvWlfk21" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">48. </div><div class="csl-right-inline"><strong>Variable Selection for Generalized Single-Index Varying-Coefficient Models with Applications to Synergistic G × E Interactions</strong> <div class="csl-block">Shunjie Guan, Xu Liu, Yuehua Cui</div> <em>Mathematics</em> (2025-01-31) <a href="https://doi.org/g9t2rp">https://doi.org/g9t2rp</a> <div class="csl-block">DOI: <a href="https://doi.org/10.3390/math13030469">10.3390/math13030469</a></div></div>
</div>
<div id="ref-1GbAsSOZV" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">49. </div><div class="csl-right-inline"><strong>On the Opportunities and Risks of Foundation Models</strong> <div class="csl-block">Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, … Percy Liang</div> <em>arXiv</em> (2021) <a href="https://doi.org/hw3v">https://doi.org/hw3v</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2108.07258">10.48550/arxiv.2108.07258</a></div></div>
</div>
<div id="ref-17lpGtuH5" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">50. </div><div class="csl-right-inline"><strong>GPT-4 Technical Report</strong> <div class="csl-block">OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, … Barret Zoph</div> <em>arXiv</em> (2023) <a href="https://doi.org/grx4cb">https://doi.org/grx4cb</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2303.08774">10.48550/arxiv.2303.08774</a></div></div>
</div>
<div id="ref-yWg7tQr1" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">51. </div><div class="csl-right-inline"><strong>The Llama 3 Herd of Models</strong> <div class="csl-block">Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, … Zhiyu Ma</div> <em>arXiv</em> (2024) <a href="https://doi.org/ndw6">https://doi.org/ndw6</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2407.21783">10.48550/arxiv.2407.21783</a></div></div>
</div>
<div id="ref-17tnf46zM" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">52. </div><div class="csl-right-inline"><strong>Learning Transferable Visual Models From Natural Language Supervision</strong> <div class="csl-block">Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, … Ilya Sutskever</div> <em>arXiv</em> (2021) <a href="https://doi.org/hs7z">https://doi.org/hs7z</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2103.00020">10.48550/arxiv.2103.00020</a></div></div>
</div>
<div id="ref-urJgpE6q" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">53. </div><div class="csl-right-inline"><strong>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</strong> <div class="csl-block">Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova</div> <em>arXiv</em> (2018) <a href="https://doi.org/hm65">https://doi.org/hm65</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1810.04805">10.48550/arxiv.1810.04805</a></div></div>
</div>
<div id="ref-rYveVDKJ" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">54. </div><div class="csl-right-inline"><strong>TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second</strong> <div class="csl-block">Noah Hollmann, Samuel Müller, Katharina Eggensperger, Frank Hutter</div> <em>arXiv</em> (2022) <a href="https://doi.org/g9t22b">https://doi.org/g9t22b</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2207.01848">10.48550/arxiv.2207.01848</a></div></div>
</div>
<div id="ref-12nAa0T4v" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">55. </div><div class="csl-right-inline"><strong>Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing</strong> <div class="csl-block">Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig</div> <em>ACM Computing Surveys</em> (2023-01-16) <a href="https://doi.org/gq5fh2">https://doi.org/gq5fh2</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1145/3560815">10.1145/3560815</a></div></div>
</div>
<div id="ref-g7RE7G0h" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">56. </div><div class="csl-right-inline"><strong>Mixture of experts: a literature survey</strong> <div class="csl-block">Saeed Masoudnia, Reza Ebrahimpour</div> <em>Artificial Intelligence Review</em> (2012-05-12) <a href="https://doi.org/f59sxs">https://doi.org/f59sxs</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1007/s10462-012-9338-y">10.1007/s10462-012-9338-y</a></div></div>
</div>
<div id="ref-11RvF4F7q" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">57. </div><div class="csl-right-inline"><strong>CHiLL: Zero-shot Custom Interpretable Feature Extraction from Clinical Notes with Large Language Models</strong> <div class="csl-block">Denis Jered McInerney, Geoffrey Young, Jan-Willem van de Meent, Byron C Wallace</div> <em>arXiv</em> (2023) <a href="https://doi.org/g9t22g">https://doi.org/g9t22g</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2302.12343">10.48550/arxiv.2302.12343</a></div></div>
</div>
<div id="ref-10hcHcmAG" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">58. </div><div class="csl-right-inline"><strong>Learning Interpretable Style Embeddings via Prompting LLMs</strong> <div class="csl-block">Ajay Patel, Delip Rao, Ansh Kothary, Kathleen McKeown, Chris Callison-Burch</div> <em>arXiv</em> (2023) <a href="https://doi.org/g9t22h">https://doi.org/g9t22h</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2305.12696">10.48550/arxiv.2305.12696</a></div></div>
</div>
<div id="ref-kAJDlMwy" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">59. </div><div class="csl-right-inline"><strong>Tree Prompting: Efficient Task Adaptation without Fine-Tuning</strong> <div class="csl-block">Chandan Singh, John Morris, Alexander Rush, Jianfeng Gao, Yuntian Deng</div> <em>Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em> (2023) <a href="https://doi.org/gtgrkq">https://doi.org/gtgrkq</a> <div class="csl-block">DOI: <a href="https://doi.org/10.18653/v1/2023.emnlp-main.384">10.18653/v1/2023.emnlp-main.384</a></div></div>
</div>
<div id="ref-R7y5TKp9" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">60. </div><div class="csl-right-inline"><strong>What Can Transformers Learn In-Context? A Case Study of Simple Function Classes</strong> <div class="csl-block">Shivam Garg, Dimitris Tsipras, Percy Liang, Gregory Valiant</div> <em>arXiv</em> (2022) <a href="https://doi.org/g9t22c">https://doi.org/g9t22c</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2208.01066">10.48550/arxiv.2208.01066</a></div></div>
</div>
<div id="ref-1AazNaZYl" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">61. </div><div class="csl-right-inline"><strong>One Embedder, Any Task: Instruction-Finetuned Text Embeddings</strong> <div class="csl-block">Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A Smith, Luke Zettlemoyer, Tao Yu</div> <em>arXiv</em> (2022) <a href="https://doi.org/g9t22f">https://doi.org/g9t22f</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2212.09741">10.48550/arxiv.2212.09741</a></div></div>
</div>
<div id="ref-HQXzkG4Q" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">62. </div><div class="csl-right-inline"><strong>Augmenting interpretable models with large language models during training</strong> <div class="csl-block">Chandan Singh, Armin Askari, Rich Caruana, Jianfeng Gao</div> <em>Nature Communications</em> (2023-11-30) <a href="https://doi.org/g9t2z9">https://doi.org/g9t2z9</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/s41467-023-43713-1">10.1038/s41467-023-43713-1</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/38036543">38036543</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10689442">PMC10689442</a></div></div>
</div>
<div id="ref-XpHq6HEw" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">63. </div><div class="csl-right-inline"><strong>Explaining Datasets in Words: Statistical Models with Natural Language Parameters</strong> <div class="csl-block">Ruiqi Zhong, Heng Wang, Dan Klein, Jacob Steinhardt</div> <em>arXiv</em> (2024) <a href="https://doi.org/g9t22k">https://doi.org/g9t22k</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2409.08466">10.48550/arxiv.2409.08466</a></div></div>
</div>
<div id="ref-zWwbz3cX" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">64. </div><div class="csl-right-inline"><strong>Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models</strong> <div class="csl-block">Sheng Shen, Le Hou, Yanqi Zhou, Nan Du, Shayne Longpre, Jason Wei, Hyung Won Chung, Barret Zoph, William Fedus, Xinyun Chen, … Denny Zhou</div> <em>arXiv</em> (2023) <a href="https://doi.org/g9t22j">https://doi.org/g9t22j</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2305.14705">10.48550/arxiv.2305.14705</a></div></div>
</div>
<div id="ref-Xtwwrjzy" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">65. </div><div class="csl-right-inline"><strong>LMPriors: Pre-Trained Language Models as Task-Specific Priors</strong> <div class="csl-block">Kristy Choi, Chris Cundy, Sanjari Srivastava, Stefano Ermon</div> <em>arXiv</em> (2022) <a href="https://doi.org/g9t22d">https://doi.org/g9t22d</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2210.12530">10.48550/arxiv.2210.12530</a></div></div>
</div>
</div>
<!-- default theme -->

<style>
  /* import google fonts */
  @import url("https://fonts.googleapis.com/css?family=Open+Sans:400,600,700");
  @import url("https://fonts.googleapis.com/css?family=Source+Code+Pro");

  /* -------------------------------------------------- */
  /* global */
  /* -------------------------------------------------- */

  /* all elements */
  * {
    /* force sans-serif font unless specified otherwise */
    font-family: "Open Sans", "Helvetica", sans-serif;

    /* prevent text inflation on some mobile browsers */
    -webkit-text-size-adjust: none !important;
    -moz-text-size-adjust: none !important;
    -o-text-size-adjust: none !important;
    text-size-adjust: none !important;
  }

  @media only screen {
    /* "page" element */
    body {
      position: relative;
      box-sizing: border-box;
      font-size: 12pt;
      line-height: 1.5;
      max-width: 8.5in;
      margin: 20px auto;
      padding: 40px;
      border-radius: 5px;
      border: solid 1px #bdbdbd;
      box-shadow: 0 0 20px rgba(0, 0, 0, 0.05);
      background: #ffffff;
    }
  }

  /* when on screen < 8.5in wide */
  @media only screen and (max-width: 8.5in) {
    /* "page" element */
    body {
      padding: 20px;
      margin: 0;
      border-radius: 0;
      border: none;
      box-shadow: 0 0 20px rgba(0, 0, 0, 0.05) inset;
      background: none;
    }
  }

  /* -------------------------------------------------- */
  /* headings */
  /* -------------------------------------------------- */

  /* all headings */
  h1,
  h2,
  h3,
  h4,
  h5,
  h6 {
    margin: 20px 0;
    padding: 0;
    font-weight: bold;
  }

  /* biggest heading */
  h1 {
    margin: 40px 0;
    text-align: center;
  }

  /* second biggest heading */
  h2 {
    margin-top: 30px;
    padding-bottom: 5px;
    border-bottom: solid 1px #bdbdbd;
  }

  /* heading font sizes */
  h1 {
    font-size: 2em;
  }
  h2 {
    font-size: 1.5em;
  }
  h3 {
    font-size: 1.35em;
  }
  h4 {
    font-size: 1.25em;
  }
  h5 {
    font-size: 1.15em;
  }
  h6 {
    font-size: 1em;
  }

  /* -------------------------------------------------- */
  /* manuscript header */
  /* -------------------------------------------------- */

  /* manuscript title */
  header > h1 {
    margin: 0;
  }

  /* manuscript title caption text (ie "automatically generated on") */
  header + p {
    text-align: center;
    margin-top: 10px;
  }

  /* -------------------------------------------------- */
  /* text elements */
  /* -------------------------------------------------- */

  /* links */
  a {
    color: #2196f3;
    overflow-wrap: break-word;
  }

  /* superscripts and subscripts */
  sub,
  sup {
    /* prevent from affecting line height */
    line-height: 0;
  }

  /* unordered and ordered lists*/
  ul,
  ol {
    padding-left: 20px;
  }

  /* class for styling text semibold */
  .semibold {
    font-weight: 600;
  }

  /* class for styling elements horizontally left aligned */
  .left {
    display: block;
    text-align: left;
    margin-left: auto;
    margin-right: 0;
    justify-content: left;
  }

  /* class for styling elements horizontally centered */
  .center {
    display: block;
    text-align: center;
    margin-left: auto;
    margin-right: auto;
    justify-content: center;
  }

  /* class for styling elements horizontally right aligned */
  .right {
    display: block;
    text-align: right;
    margin-left: 0;
    margin-right: auto;
    justify-content: right;
  }

  /* -------------------------------------------------- */
  /* section elements */
  /* -------------------------------------------------- */

  /* horizontal divider line */
  hr {
    border: none;
    height: 1px;
    background: #bdbdbd;
  }

  /* paragraphs, horizontal dividers, figures, tables, code */
  p,
  hr,
  figure,
  table,
  pre {
    /* treat all as "paragraphs", with consistent vertical margins */
    margin-top: 20px;
    margin-bottom: 20px;
  }

  /* -------------------------------------------------- */
  /* figures */
  /* -------------------------------------------------- */

  /* figure */
  figure {
    max-width: 100%;
    margin-left: auto;
    margin-right: auto;
  }

  /* figure caption */
  figcaption {
    padding: 0;
    padding-top: 10px;
  }

  /* figure image element */
  figure > img,
  figure > svg {
    max-width: 100%;
    display: block;
    margin-left: auto;
    margin-right: auto;
  }

  /* figure auto-number */
  img + figcaption > span:first-of-type,
  svg + figcaption > span:first-of-type {
    font-weight: bold;
    margin-right: 5px;
  }

  /* -------------------------------------------------- */
  /* tables */
  /* -------------------------------------------------- */

  /* table */
  table {
    border-collapse: collapse;
    border-spacing: 0;
    width: 100%;
    margin-left: auto;
    margin-right: auto;
  }

  /* table cells */
  th,
  td {
    border: solid 1px #bdbdbd;
    padding: 10px;
    /* squash table if too wide for page by forcing line breaks */
    overflow-wrap: break-word;
    word-break: break-word;
  }

  /* header row and even rows */
  th,
  tr:nth-child(2n) {
    background-color: #fafafa;
  }

  /* odd rows */
  tr:nth-child(2n + 1) {
    background-color: #ffffff;
  }

  /* table caption */
  caption {
    text-align: left;
    padding: 0;
    padding-bottom: 10px;
  }

  /* table auto-number */
  table > caption > span:first-of-type {
    font-weight: bold;
    margin-right: 5px;
  }

  /* -------------------------------------------------- */
  /* code */
  /* -------------------------------------------------- */

  /* multi-line code block */
  pre {
    padding: 10px;
    background-color: #eeeeee;
    color: #000000;
    border-radius: 5px;
    break-inside: avoid;
    text-align: left;
  }

  /* inline code, ie code within normal text */
  :not(pre) > code {
    padding: 0 4px;
    background-color: #eeeeee;
    color: #000000;
    border-radius: 5px;
  }

  /* code text */
  /* apply all children, to reach syntax highlighting sub-elements */
  code,
  code * {
    /* force monospace font */
    font-family: "Source Code Pro", "Courier New", monospace;
  }

  /* -------------------------------------------------- */
  /* quotes */
  /* -------------------------------------------------- */

  /* quoted text */
  blockquote {
    margin: 0;
    padding: 0;
    border-left: 4px solid #bdbdbd;
    padding-left: 16px;
    break-inside: avoid;
  }

  /* -------------------------------------------------- */
  /* banners */
  /* -------------------------------------------------- */

  /* info banners */
  .banner {
    box-sizing: border-box;
    display: block;
    position: relative;
    width: 100%;
    margin-top: 20px;
    margin-bottom: 20px;
    padding: 20px;
    text-align: center;
  }

  /* paragraph in banner */
  .banner > p {
    margin: 0;
  }

  /* -------------------------------------------------- */
  /* highlight colors */
  /* -------------------------------------------------- */

  .white {
    background: #ffffff;
  }
  .lightgrey {
    background: #eeeeee;
  }
  .grey {
    background: #757575;
  }
  .darkgrey {
    background: #424242;
  }
  .black {
    background: #000000;
  }
  .lightred {
    background: #ffcdd2;
  }
  .lightyellow {
    background: #ffecb3;
  }
  .lightgreen {
    background: #dcedc8;
  }
  .lightblue {
    background: #e3f2fd;
  }
  .lightpurple {
    background: #f3e5f5;
  }
  .red {
    background: #f44336;
  }
  .orange {
    background: #ff9800;
  }
  .yellow {
    background: #ffeb3b;
  }
  .green {
    background: #4caf50;
  }
  .blue {
    background: #2196f3;
  }
  .purple {
    background: #9c27b0;
  }
  .white,
  .lightgrey,
  .lightred,
  .lightyellow,
  .lightgreen,
  .lightblue,
  .lightpurple,
  .orange,
  .yellow,
  .white a,
  .lightgrey a,
  .lightred a,
  .lightyellow a,
  .lightgreen a,
  .lightblue a,
  .lightpurple a,
  .orange a,
  .yellow a {
    color: #000000;
  }
  .grey,
  .darkgrey,
  .black,
  .red,
  .green,
  .blue,
  .purple,
  .grey a,
  .darkgrey a,
  .black a,
  .red a,
  .green a,
  .blue a,
  .purple a {
    color: #ffffff;
  }

  /* -------------------------------------------------- */
  /* buttons */
  /* -------------------------------------------------- */

  /* class for styling links like buttons */
  .button {
    display: inline-flex;
    justify-content: center;
    align-items: center;
    margin: 5px;
    padding: 10px 20px;
    font-size: 0.75em;
    font-weight: 600;
    text-transform: uppercase;
    text-decoration: none;
    letter-spacing: 1px;
    background: none;
    color: #2196f3;
    border: solid 1px #bdbdbd;
    border-radius: 5px;
  }

  /* buttons when hovered */
  .button:hover:not([disabled]),
  .icon_button:hover:not([disabled]) {
    cursor: pointer;
    background: #f5f5f5;
  }

  /* buttons when disabled */
  .button[disabled],
  .icon_button[disabled] {
    opacity: 0.35;
    pointer-events: none;
  }

  /* class for styling buttons containg only single icon */
  .icon_button {
    display: inline-flex;
    justify-content: center;
    align-items: center;
    text-decoration: none;
    margin: 0;
    padding: 0;
    background: none;
    border-radius: 5px;
    border: none;
    width: 20px;
    height: 20px;
    min-width: 20px;
    min-height: 20px;
  }

  /* icon button inner svg image */
  .icon_button > svg {
    height: 16px;
  }

  /* -------------------------------------------------- */
  /* icons */
  /* -------------------------------------------------- */

  /* class for styling icons inline with text */
  .inline_icon {
    height: 1em;
    position: relative;
    top: 0.125em;
  }

  /* -------------------------------------------------- */
  /* references */
  /* -------------------------------------------------- */

  .csl-entry {
    margin-top: 15px;
    margin-bottom: 15px;
  }

  /* -------------------------------------------------- */
  /* print control */
  /* -------------------------------------------------- */

  @media print {
    @page {
      /* suggested printing margin */
      margin: 0.5in;
    }

    /* document and "page" elements */
    html,
    body {
      margin: 0;
      padding: 0;
      width: 100%;
      height: 100%;
    }

    /* "page" element */
    body {
      font-size: 11pt !important;
      line-height: 1.35;
    }

    /* all headings */
    h1,
    h2,
    h3,
    h4,
    h5,
    h6 {
      margin: 15px 0;
    }

    /* figures and tables */
    figure,
    table {
      font-size: 0.85em;
    }

    /* table cells */
    th,
    td {
      padding: 5px;
    }

    /* shrink font awesome icons */
    i.fas,
    i.fab,
    i.far,
    i.fal {
      transform: scale(0.85);
    }

    /* decrease banner margins */
    .banner {
      margin-top: 15px;
      margin-bottom: 15px;
      padding: 15px;
    }

    /* class for centering an element vertically on its own page */
    .page_center {
      margin: auto;
      width: 100%;
      height: 100%;
      display: flex;
      align-items: center;
      vertical-align: middle;
      break-before: page;
      break-after: page;
    }

    /* always insert a page break before the element */
    .page_break_before {
      break-before: page;
    }

    /* always insert a page break after the element */
    .page_break_after {
      break-after: page;
    }

    /* avoid page break before the element */
    .page_break_before_avoid {
      break-before: avoid;
    }

    /* avoid page break after the element */
    .page_break_after_avoid {
      break-after: avoid;
    }

    /* avoid page break inside the element */
    .page_break_inside_avoid {
      break-inside: avoid;
    }
  }

  /* -------------------------------------------------- */
  /* override pandoc css quirks */
  /* -------------------------------------------------- */

  .sourceCode {
    /* prevent unsightly overflow in wide code blocks */
    overflow: auto !important;
  }

  div.sourceCode {
    /* prevent background fill on top-most code block  container */
    background: none !important;
  }

  .sourceCode * {
    /* force consistent line spacing */
    line-height: 1.5 !important;
  }

  div.sourceCode {
    /* style code block margins same as <pre> element */
    margin-top: 20px;
    margin-bottom: 20px;
  }

  /* -------------------------------------------------- */
  /* tablenos */
  /* -------------------------------------------------- */

  /* tablenos wrapper */
  .tablenos {
    width: 100%;
    margin: 20px 0;
  }

  .tablenos > table {
    /* move margins from table to table_wrapper to allow margin collapsing */
    margin: 0;
  }

  @media only screen {
    /* tablenos wrapper */
    .tablenos {
      /* show scrollbar on tables if necessary to prevent overflow */
      overflow-x: auto !important;
    }

    .tablenos th,
    .tablenos td {
      overflow-wrap: unset !important;
      word-break: unset !important;
    }

    /* table in wrapper */
    .tablenos table,
    .tablenos table * {
      /* don't break table words */
      overflow-wrap: normal !important;
    }
  }
</style>
<!-- 
    Plugin Core

    Functions needed for and shared across all first-party plugins.
-->

<script>
  // get element that is target of hash (from link element or url)
  function getHashTarget(link) {
    const hash = link ? link.hash : window.location.hash;
    const id = hash.slice(1);
    let target = document.querySelector(`[id="${id}"]`);
    if (!target) return;

    // if figure or table, modify target to get expected element
    if (id.indexOf("fig:") === 0) target = target.querySelector("figure");
    if (id.indexOf("tbl:") === 0) target = target.querySelector("table");

    return target;
  }

  // get position/dimensions of element or viewport
  function getRectInView(element) {
    let rect = {};
    rect.left = 0;
    rect.top = 0;
    rect.right = document.documentElement.clientWidth;
    rect.bottom = document.documentElement.clientHeight;
    let style = {};

    if (element instanceof HTMLElement) {
      rect = element.getBoundingClientRect();
      style = window.getComputedStyle(element);
    }

    const margin = {};
    margin.left = parseFloat(style.marginLeftWidth) || 0;
    margin.top = parseFloat(style.marginTopWidth) || 0;
    margin.right = parseFloat(style.marginRightWidth) || 0;
    margin.bottom = parseFloat(style.marginBottomWidth) || 0;

    const border = {};
    border.left = parseFloat(style.borderLeftWidth) || 0;
    border.top = parseFloat(style.borderTopWidth) || 0;
    border.right = parseFloat(style.borderRightWidth) || 0;
    border.bottom = parseFloat(style.borderBottomWidth) || 0;

    const newRect = {};
    newRect.left = rect.left + margin.left + border.left;
    newRect.top = rect.top + margin.top + border.top;
    newRect.right = rect.right + margin.right + border.right;
    newRect.bottom = rect.bottom + margin.bottom + border.bottom;
    newRect.width = newRect.right - newRect.left;
    newRect.height = newRect.bottom - newRect.top;

    return newRect;
  }

  // get position of element relative to page
  function getRectInPage(element) {
    const rect = getRectInView(element);
    const body = getRectInView(document.body);

    const newRect = {};
    newRect.left = rect.left - body.left;
    newRect.top = rect.top - body.top;
    newRect.right = rect.right - body.left;
    newRect.bottom = rect.bottom - body.top;
    newRect.width = rect.width;
    newRect.height = rect.height;

    return newRect;
  }

  // get closest element before specified element that matches query
  function firstBefore(element, query) {
    while (element && element !== document.body && !element.matches(query))
      element = element.previousElementSibling || element.parentNode;

    return element;
  }

  // check if element is part of collapsed heading
  function isCollapsed(element) {
    while (element && element !== document.body) {
      if (element.dataset.collapsed === "true") return true;
      element = element.parentNode;
    }
    return false;
  }

  // expand any collapsed parent containers of element if necessary
  function expandElement(element) {
    if (isCollapsed(element)) {
      // accordion plugin
      const heading = firstBefore(element, "h2");
      if (heading) heading.click();
      // details/summary HTML element
      const summary = firstBefore(element, "summary");
      if (summary) summary.click();
    }
  }

  // scroll to and focus element
  function goToElement(element, offset) {
    // expand accordion section if collapsed
    expandElement(element);
    const y =
      getRectInView(element).top -
      getRectInView(document.documentElement).top -
      (offset || 0);

    // trigger any function listening for "onscroll" event
    window.dispatchEvent(new Event("scroll"));
    window.scrollTo(0, y);
    document.activeElement.blur();
    element.focus();
  }

  // get list of elements after a start element up to element matching query
  function nextUntil(element, query, exclude) {
    const elements = [];
    while (((element = element.nextElementSibling), element)) {
      if (element.matches(query)) break;
      if (!element.matches(exclude)) elements.push(element);
    }
    return elements;
  }
</script>
<!--
  Accordion Plugin

  Allows sections of content under h2 headings to be collapsible.
-->

<script type="module">
  // whether to always start expanded ('false'), always start collapsed
  // ('true'), or start collapsed when screen small ('auto')
  const startCollapsed = "auto";

  // start script
  function start() {
    // run through each <h2> heading
    const headings = document.querySelectorAll("h2");
    for (const heading of headings) {
      addArrow(heading);

      // start expanded/collapsed based on option
      if (
        startCollapsed === "true" ||
        (startCollapsed === "auto" && isSmallScreen()) ||
        heading.dataset.collapsed === "true"
      )
        collapseHeading(heading);
      else expandElement(heading);
    }

    // attach hash change listener to window
    window.addEventListener("hashchange", onHashChange);
  }

  // when hash (eg manuscript.html#introduction) changes
  function onHashChange() {
    const target = getHashTarget();
    if (target) goToElement(target);
  }

  // add arrow to heading
  function addArrow(heading) {
    // add arrow button
    const arrow = document.createElement("button");
    arrow.innerHTML = document.querySelector(".icon_angle_down").innerHTML;
    arrow.classList.add("icon_button", "accordion_arrow");
    heading.insertBefore(arrow, heading.firstChild);

    // attach click listener to heading and button
    heading.addEventListener("click", onHeadingClick);
    arrow.addEventListener("click", onArrowClick);
  }

  // determine if on mobile-like device with small screen
  function isSmallScreen() {
    return Math.min(window.innerWidth, window.innerHeight) < 480;
  }

  // when <h2> heading is clicked
  function onHeadingClick(event) {
    // only collapse if <h2> itself is target of click (eg, user did
    // not click on anchor within <h2>)
    if (event.target === this) toggleCollapse(this);
  }

  // when arrow button is clicked
  function onArrowClick() {
    toggleCollapse(this.parentNode);
  }

  // collapse section if expanded, expand if collapsed
  function toggleCollapse(heading) {
    if (heading.dataset.collapsed === "false") collapseHeading(heading);
    else expandElement(heading);
  }

  // elements to exclude from collapse, such as table of contents panel,
  // hypothesis panel, etc
  const exclude = "#toc_panel, div.annotator-frame, #lightbox_overlay";

  // collapse section
  function collapseHeading(heading) {
    heading.setAttribute("data-collapsed", "true");
    const children = getChildren(heading);
    for (const child of children) child.setAttribute("data-collapsed", "true");
  }

  // expand section
  function expandElement(heading) {
    heading.setAttribute("data-collapsed", "false");
    const children = getChildren(heading);
    for (const child of children) child.setAttribute("data-collapsed", "false");
  }

  // get list of elements between this <h2> and next <h2> or <h1>
  // ("children" of the <h2> section)
  function getChildren(heading) {
    return nextUntil(heading, "h2, h1", exclude);
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- angle down icon -->

<template class="icon_angle_down">
  <!-- modified from: https://fontawesome.com/icons/angle-down -->
  <svg width="16" height="16" viewBox="0 0 448 512">
    <path
      fill="currentColor"
      d="M207.029 381.476L12.686 187.132c-9.373-9.373-9.373-24.569 0-33.941l22.667-22.667c9.357-9.357 24.522-9.375 33.901-.04L224 284.505l154.745-154.021c9.379-9.335 24.544-9.317 33.901.04l22.667 22.667c9.373 9.373 9.373 24.569 0 33.941L240.971 381.476c-9.373 9.372-24.569 9.372-33.942 0z"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* accordion arrow button */
    .accordion_arrow {
      margin-right: 10px;
    }

    /* arrow icon when <h2> data-collapsed attribute true */
    h2[data-collapsed="true"] > .accordion_arrow > svg {
      transform: rotate(-90deg);
    }

    /* all elements (except <h2>'s) when data-collapsed attribute true */
    *:not(h2)[data-collapsed="true"] {
      display: none;
    }

    /* accordion arrow button when hovered and <h2>'s when hovered */
    .accordion_arrow:hover,
    h2[data-collapsed="true"]:hover,
    h2[data-collapsed="false"]:hover {
      cursor: pointer;
    }
  }

  /* always hide accordion arrow button on print */
  @media only print {
    .accordion_arrow {
      display: none;
    }
  }
</style>
<!--
  Anchors Plugin

  Adds an anchor next to each of a certain type of element that provides a
  human-readable url to that specific item/position in the document (e.g.
  "manuscript.html#abstract"). It also makes it such that scrolling out of view
  of a target removes its identifier from the url.
-->

<script type="module">
  // which types of elements to add anchors next to, in "document.querySelector"
  // format
  const typesQuery =
    'h1, h2, h3, div[id^="fig:"], div[id^="tbl:"], span[id^="eq:"]';

  // start script
  function start() {
    // add anchor to each element of specified types
    const elements = document.querySelectorAll(typesQuery);
    for (const element of elements) addAnchor(element);

    // attach scroll listener to window
    window.addEventListener("scroll", onScroll);
  }

  // when window is scrolled
  function onScroll() {
    // if url has hash and user has scrolled out of view of hash
    // target, remove hash from url
    const tolerance = 100;
    const target = getHashTarget();
    if (target) {
      if (
        target.getBoundingClientRect().top > window.innerHeight + tolerance ||
        target.getBoundingClientRect().bottom < 0 - tolerance
      )
        history.pushState(null, null, " ");
    }
  }

  // add anchor to element
  function addAnchor(element) {
    let addTo; // element to add anchor button to

    // if figure or table, modify withId and addTo to get expected
    // elements
    if (element.id.indexOf("fig:") === 0) {
      addTo = element.querySelector("figcaption");
    } else if (element.id.indexOf("tbl:") === 0) {
      addTo = element.querySelector("caption");
    } else if (element.id.indexOf("eq:") === 0) {
      addTo = element.querySelector(".eqnos-number");
    }

    addTo = addTo || element;
    const id = element.id || null;

    // do not add anchor if element doesn't have assigned id.
    // id is generated by pandoc and is assumed to be unique and
    // human-readable
    if (!id) return;

    // create anchor button
    const anchor = document.createElement("a");
    anchor.innerHTML = document.querySelector(".icon_link").innerHTML;
    anchor.title = "Link to this part of the document";
    anchor.classList.add("icon_button", "anchor");
    anchor.dataset.ignore = "true";
    anchor.href = "#" + id;
    addTo.appendChild(anchor);
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- link icon -->

<template class="icon_link">
  <!-- modified from: https://fontawesome.com/icons/link -->
  <svg width="16" height="16" viewBox="0 0 512 512">
    <path
      fill="currentColor"
      d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* anchor button */
    .anchor {
      opacity: 0;
      margin-left: 5px;
    }

    /* anchor buttons within <h2>'s */
    h2 .anchor {
      margin-left: 10px;
    }

    /* anchor buttons when hovered/focused and anything containing an anchor button when hovered */
    *:hover > .anchor,
    .anchor:hover,
    .anchor:focus {
      opacity: 1;
    }

    /* anchor button when hovered */
    .anchor:hover {
      cursor: pointer;
    }
  }

  /* always show anchor button on devices with no mouse/hover ability */
  @media (hover: none) {
    .anchor {
      opacity: 1;
    }
  }

  /* always hide anchor button on print */
  @media only print {
    .anchor {
      display: none;
    }
  }
</style>
<!-- 
    Attributes Plugin

    Allows arbitrary HTML attributes to be attached to (almost) any element.
    Place an HTML comment inside or next to the desired element with the content:
    $attribute="value"
-->

<script type="module">
  // start script
  function start() {
    // get list of comments in document
    const comments = findComments();

    for (const comment of comments)
      if (comment.parentElement)
        addAttributes(comment.parentElement, comment.nodeValue.trim());
  }

  // add html attributes to specified element based on string of
  // html attributes and values
  function addAttributes(element, text) {
    // regex's for finding attribute/value pairs in the format of
    // attribute="value" or attribute='value
    const regex2 = /\$([a-zA-Z\-]+)?=\"(.+?)\"/;
    const regex1 = /\$([a-zA-Z\-]+)?=\'(.+?)\'/;

    // loop through attribute/value pairs
    let match;
    while ((match = text.match(regex2) || text.match(regex1))) {
      // get attribute and value from regex capture groups
      let attribute = match[1];
      let value = match[2];

      // remove from string
      text = text.substring(match.index + match[0].length);

      if (!attribute || !value) break;

      // set attribute of parent element
      try {
        element.setAttribute(attribute, value);
      } catch (error) {
        console.log(error);
      }

      // special case for colspan
      if (attribute === "colspan") removeTableCells(element, value);
    }
  }

  // get list of comment elements in document
  function findComments() {
    const comments = [];

    // iterate over comment nodes in document
    function acceptNode(node) {
      return NodeFilter.FILTER_ACCEPT;
    }
    const iterator = document.createNodeIterator(
      document.body,
      NodeFilter.SHOW_COMMENT,
      acceptNode
    );
    let node;
    while ((node = iterator.nextNode())) comments.push(node);

    return comments;
  }

  // remove certain number of cells after specified cell
  function removeTableCells(cell, number) {
    number = parseInt(number);
    if (!number) return;

    // remove elements
    for (; number > 1; number--) {
      if (cell.nextElementSibling) cell.nextElementSibling.remove();
    }
  }

  // start script on DOMContentLoaded instead of load to ensure this plugins
  // runs before other plugins
  window.addEventListener("DOMContentLoaded", start);
</script>
<!--
  Jump to First Plugin

  Adds a button next to each reference entry, figure, and table that jumps the
  page to the first occurrence of a link to that item in the manuscript.
-->

<script type="module">
  // whether to add buttons next to reference entries
  const references = "true";
  // whether to add buttons next to figures
  const figures = "true";
  // whether to add buttons next to tables
  const tables = "true";

  // start script
  function start() {
    if (references !== "false")
      makeButtons(`div[id^="ref-"]`, ".csl-left-margin", "reference");
    if (figures !== "false")
      makeButtons(`div[id^="fig:"]`, "figcaption", "figure");
    if (tables !== "false") makeButtons(`div[id^="tbl:"]`, "caption", "table");
  }

  // when jump button clicked
  function onButtonClick() {
    const first = getFirstOccurrence(this.dataset.id);
    if (!first) return;

    // update url hash so navigating "back" in history will return user to button
    window.location.hash = this.dataset.id;
    // scroll to link
    const timeout = function () {
      goToElement(first, window.innerHeight * 0.5);
    };
    window.setTimeout(timeout, 0);
  }

  // get first occurrence of link to item in document
  function getFirstOccurrence(id) {
    let query = "a";
    query += '[href="#' + id + '"]';
    // exclude buttons, anchor links, toc links, etc
    query += ":not(.button):not(.icon_button):not(.anchor):not(.toc_link)";
    return document.querySelector(query);
  }

  // add button next to each reference entry, figure, or table
  function makeButtons(query, containerQuery, subject) {
    const elements = document.querySelectorAll(query);
    for (const element of elements) {
      const id = element.id;
      const buttonContainer = element.querySelector(containerQuery);
      const first = getFirstOccurrence(id);

      // if can't find link to reference or place to put button, ignore
      if (!first || !buttonContainer) continue;

      // make jump button
      let button = document.createElement("button");
      button.classList.add("icon_button", "jump_arrow");
      button.title = `Jump to the first occurrence of this ${subject} in the document`;
      const icon = document.querySelector(".icon_angle_double_up");
      button.innerHTML = icon.innerHTML;
      button.dataset.id = id;
      button.dataset.ignore = "true";
      button.addEventListener("click", onButtonClick);
      buttonContainer.prepend(button);
    }
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- angle double up icon -->

<template class="icon_angle_double_up">
  <!-- modified from: https://fontawesome.com/icons/angle-double-up -->
  <svg width="16" height="16" viewBox="0 0 320 512">
    <path
      fill="currentColor"
      d="M177 255.7l136 136c9.4 9.4 9.4 24.6 0 33.9l-22.6 22.6c-9.4 9.4-24.6 9.4-33.9 0L160 351.9l-96.4 96.4c-9.4 9.4-24.6 9.4-33.9 0L7 425.7c-9.4-9.4-9.4-24.6 0-33.9l136-136c9.4-9.5 24.6-9.5 34-.1zm-34-192L7 199.7c-9.4 9.4-9.4 24.6 0 33.9l22.6 22.6c9.4 9.4 24.6 9.4 33.9 0l96.4-96.4 96.4 96.4c9.4 9.4 24.6 9.4 33.9 0l22.6-22.6c9.4-9.4 9.4-24.6 0-33.9l-136-136c-9.2-9.4-24.4-9.4-33.8 0z"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* jump button */
    .jump_arrow {
      position: relative;
      top: 0.125em;
      margin-right: 5px;
    }
  }

  /* always hide jump button on print */
  @media only print {
    .jump_arrow {
      display: none;
    }
  }
</style>
<!-- 
    Lightbox Plugin

    Makes it such that when a user clicks on an image, the image fills the
    screen and the user can pan/drag/zoom the image and navigate between other
    images in the document.
-->

<script type="module">
  // list of possible zoom/scale factors
  const zooms =
    "0.1, 0.25, 0.333333, 0.5, 0.666666, 0.75, 1, 1.25, 1.5, 1.75, 2, 2.5, 3, 3.5, 4, 5, 6, 7, 8";
  // whether to fit image to view ('fit'), display at 100% and shrink if
  // necessary ('shrink'), or always display at 100% ('100')
  const defaultZoom = "fit";
  // whether to zoom in/out toward center of view ('true') or mouse ('false')
  const centerZoom = "false";

  // start script
  function start() {
    // run through each <img> element
    const imgs = document.querySelectorAll("figure > img");
    let count = 1;
    for (const img of imgs) {
      img.classList.add("lightbox_document_img");
      img.dataset.number = count;
      img.dataset.total = imgs.length;
      img.addEventListener("click", openLightbox);
      count++;
    }

    // attach mouse and key listeners to window
    window.addEventListener("mousemove", onWindowMouseMove);
    window.addEventListener("keyup", onKeyUp);
  }

  // when mouse is moved anywhere in window
  function onWindowMouseMove(event) {
    window.mouseX = event.clientX;
    window.mouseY = event.clientY;
  }

  // when key pressed
  function onKeyUp(event) {
    if (!event || !event.key) return;

    switch (event.key) {
      // trigger click of prev button
      case "ArrowLeft":
        const prevButton = document.getElementById("lightbox_prev_button");
        if (prevButton) prevButton.click();
        break;
      // trigger click of next button
      case "ArrowRight":
        const nextButton = document.getElementById("lightbox_next_button");
        if (nextButton) nextButton.click();
        break;
      // close on esc
      case "Escape":
        closeLightbox();
        break;
    }
  }

  // open lightbox
  function openLightbox() {
    const lightbox = makeLightbox(this);
    if (!lightbox) return;

    blurBody(lightbox);
    document.body.appendChild(lightbox);
  }

  // make lightbox
  function makeLightbox(img) {
    // delete lightbox if it exists, start fresh
    closeLightbox();

    // create screen overlay containing lightbox
    const overlay = document.createElement("div");
    overlay.id = "lightbox_overlay";

    // create image info boxes
    const numberInfo = document.createElement("div");
    const zoomInfo = document.createElement("div");
    numberInfo.id = "lightbox_number_info";
    zoomInfo.id = "lightbox_zoom_info";

    // create container for image
    const imageContainer = document.createElement("div");
    imageContainer.id = "lightbox_image_container";
    const lightboxImg = makeLightboxImg(
      img,
      imageContainer,
      numberInfo,
      zoomInfo
    );
    imageContainer.appendChild(lightboxImg);

    // create bottom container for caption and navigation buttons
    const bottomContainer = document.createElement("div");
    bottomContainer.id = "lightbox_bottom_container";
    const caption = makeCaption(img);
    const prevButton = makePrevButton(img);
    const nextButton = makeNextButton(img);
    bottomContainer.appendChild(prevButton);
    bottomContainer.appendChild(caption);
    bottomContainer.appendChild(nextButton);

    // attach top middle and bottom to overlay
    overlay.appendChild(numberInfo);
    overlay.appendChild(zoomInfo);
    overlay.appendChild(imageContainer);
    overlay.appendChild(bottomContainer);

    return overlay;
  }

  // make <img> object that is intuitively draggable and zoomable
  function makeLightboxImg(sourceImg, container, numberInfoBox, zoomInfoBox) {
    // create copy of source <img>
    const img = sourceImg.cloneNode(true);
    img.classList.remove("lightbox_document_img");
    img.removeAttribute("id");
    img.removeAttribute("width");
    img.removeAttribute("height");
    img.style.position = "unset";
    img.style.margin = "0";
    img.style.padding = "0";
    img.style.width = "";
    img.style.height = "";
    img.style.minWidth = "";
    img.style.minHeight = "";
    img.style.maxWidth = "";
    img.style.maxHeight = "";
    img.id = "lightbox_img";

    // build sorted list of zoomSteps
    const zoomSteps = zooms.split(/[^0-9.]/).map((step) => parseFloat(step));
    zoomSteps.sort((a, b) => a - b);

    // <img> object property variables
    let zoom = 1;
    let translateX = 0;
    let translateY = 0;
    let clickMouseX = undefined;
    let clickMouseY = undefined;
    let clickTranslateX = undefined;
    let clickTranslateY = undefined;

    updateNumberInfo();

    // update image numbers displayed in info box
    function updateNumberInfo() {
      numberInfoBox.innerHTML =
        sourceImg.dataset.number + " of " + sourceImg.dataset.total;
    }

    // update zoom displayed in info box
    function updateZoomInfo() {
      let zoomInfo = zoom * 100;
      if (!Number.isInteger(zoomInfo)) zoomInfo = zoomInfo.toFixed(2);
      zoomInfoBox.innerHTML = zoomInfo + "%";
    }

    // move to closest zoom step above current zoom
    const zoomIn = function () {
      for (const zoomStep of zoomSteps) {
        if (zoomStep > zoom) {
          zoom = zoomStep;
          break;
        }
      }
      updateTransform();
    };

    // move to closest zoom step above current zoom
    const zoomOut = function () {
      zoomSteps.reverse();
      for (const zoomStep of zoomSteps) {
        if (zoomStep < zoom) {
          zoom = zoomStep;
          break;
        }
      }
      zoomSteps.reverse();

      updateTransform();
    };

    // update display of <img> based on scale/translate properties
    const updateTransform = function () {
      // set transform
      img.style.transform =
        "translate(" +
        (translateX || 0) +
        "px," +
        (translateY || 0) +
        "px) scale(" +
        (zoom || 1) +
        ")";

      // get new width/height after scale
      const rect = img.getBoundingClientRect();
      // limit translate
      translateX = Math.max(translateX, -rect.width / 2);
      translateX = Math.min(translateX, rect.width / 2);
      translateY = Math.max(translateY, -rect.height / 2);
      translateY = Math.min(translateY, rect.height / 2);

      // set transform
      img.style.transform =
        "translate(" +
        (translateX || 0) +
        "px," +
        (translateY || 0) +
        "px) scale(" +
        (zoom || 1) +
        ")";

      updateZoomInfo();
    };

    // fit <img> to container
    const fit = function () {
      // no x/y offset, 100% zoom by default
      translateX = 0;
      translateY = 0;
      zoom = 1;

      // widths of <img> and container
      const imgWidth = img.naturalWidth;
      const imgHeight = img.naturalHeight;
      const containerWidth = parseFloat(
        window.getComputedStyle(container).width
      );
      const containerHeight = parseFloat(
        window.getComputedStyle(container).height
      );

      // how much zooming is needed to fit <img> to container
      const xRatio = imgWidth / containerWidth;
      const yRatio = imgHeight / containerHeight;
      const maxRatio = Math.max(xRatio, yRatio);
      const newZoom = 1 / maxRatio;

      // fit <img> to container according to option
      if (defaultZoom === "shrink") {
        if (maxRatio > 1) zoom = newZoom;
      } else if (defaultZoom === "fit") zoom = newZoom;

      updateTransform();
    };

    // when mouse wheel is rolled anywhere in container
    const onContainerWheel = function (event) {
      if (!event) return;

      // let ctrl + mouse wheel to zoom behave as normal
      if (event.ctrlKey) return;

      // prevent normal scroll behavior
      event.preventDefault();
      event.stopPropagation();

      // point around which to scale img
      const viewRect = container.getBoundingClientRect();
      const viewX = (viewRect.left + viewRect.right) / 2;
      const viewY = (viewRect.top + viewRect.bottom) / 2;
      const originX = centerZoom === "true" ? viewX : mouseX;
      const originY = centerZoom === "true" ? viewY : mouseY;

      // get point on image under origin
      const oldRect = img.getBoundingClientRect();
      const oldPercentX = (originX - oldRect.left) / oldRect.width;
      const oldPercentY = (originY - oldRect.top) / oldRect.height;

      // increment/decrement zoom
      if (event.deltaY < 0) zoomIn();
      if (event.deltaY > 0) zoomOut();

      // get offset between previous image point and origin
      const newRect = img.getBoundingClientRect();
      const offsetX = originX - (newRect.left + newRect.width * oldPercentX);
      const offsetY = originY - (newRect.top + newRect.height * oldPercentY);

      // translate image to keep image point under origin
      translateX += offsetX;
      translateY += offsetY;

      // perform translate
      updateTransform();
    };

    // when container is clicked
    function onContainerClick(event) {
      // if container itself is target of click, and not other
      // element above it
      if (event.target === this) closeLightbox();
    }

    // when mouse button is pressed on image
    const onImageMouseDown = function (event) {
      // store original mouse position relative to image
      clickMouseX = window.mouseX;
      clickMouseY = window.mouseY;
      clickTranslateX = translateX;
      clickTranslateY = translateY;
      event.stopPropagation();
      event.preventDefault();
    };

    // when mouse button is released anywhere in window
    const onWindowMouseUp = function (event) {
      // reset original mouse position
      clickMouseX = undefined;
      clickMouseY = undefined;
      clickTranslateX = undefined;
      clickTranslateY = undefined;

      // remove global listener if lightbox removed from document
      if (!document.body.contains(container))
        window.removeEventListener("mouseup", onWindowMouseUp);
    };

    // when mouse is moved anywhere in window
    const onWindowMouseMove = function (event) {
      if (
        clickMouseX === undefined ||
        clickMouseY === undefined ||
        clickTranslateX === undefined ||
        clickTranslateY === undefined
      )
        return;

      // offset image based on original and current mouse position
      translateX = clickTranslateX + window.mouseX - clickMouseX;
      translateY = clickTranslateY + window.mouseY - clickMouseY;
      updateTransform();
      event.preventDefault();

      // remove global listener if lightbox removed from document
      if (!document.body.contains(container))
        window.removeEventListener("mousemove", onWindowMouseMove);
    };

    // when window is resized
    const onWindowResize = function (event) {
      fit();

      // remove global listener if lightbox removed from document
      if (!document.body.contains(container))
        window.removeEventListener("resize", onWindowResize);
    };

    // attach the necessary event listeners
    img.addEventListener("dblclick", fit);
    img.addEventListener("mousedown", onImageMouseDown);
    container.addEventListener("wheel", onContainerWheel);
    container.addEventListener("mousedown", onContainerClick);
    container.addEventListener("touchstart", onContainerClick);
    window.addEventListener("mouseup", onWindowMouseUp);
    window.addEventListener("mousemove", onWindowMouseMove);
    window.addEventListener("resize", onWindowResize);

    // run fit() after lightbox atttached to document and <img> Loaded
    // so needed container and img dimensions available
    img.addEventListener("load", fit);

    return img;
  }

  // make caption
  function makeCaption(img) {
    const caption = document.createElement("div");
    caption.id = "lightbox_caption";
    const captionSource = img.nextElementSibling;
    if (captionSource.tagName.toLowerCase() === "figcaption") {
      const captionCopy = makeCopy(captionSource);
      caption.innerHTML = captionCopy.innerHTML;
    }

    caption.addEventListener("touchstart", function (event) {
      event.stopPropagation();
    });

    return caption;
  }

  // make carbon copy of html dom element
  function makeCopy(source) {
    const sourceCopy = source.cloneNode(true);

    // delete elements marked with ignore (eg anchor and jump buttons)
    const deleteFromCopy = sourceCopy.querySelectorAll('[data-ignore="true"]');
    for (const element of deleteFromCopy) element.remove();

    // delete certain element attributes
    const attributes = [
      "id",
      "data-collapsed",
      "data-selected",
      "data-highlighted",
      "data-glow",
    ];
    for (const attribute of attributes) {
      sourceCopy.removeAttribute(attribute);
      const elements = sourceCopy.querySelectorAll("[" + attribute + "]");
      for (const element of elements) element.removeAttribute(attribute);
    }

    return sourceCopy;
  }

  // make button to jump to previous image in document
  function makePrevButton(img) {
    const prevButton = document.createElement("button");
    prevButton.id = "lightbox_prev_button";
    prevButton.title = "Jump to the previous image in the document [←]";
    prevButton.classList.add("icon_button", "lightbox_button");
    prevButton.innerHTML = document.querySelector(".icon_caret_left").innerHTML;

    // attach click listeners to button
    prevButton.addEventListener("click", function () {
      getPrevImg(img).click();
    });

    return prevButton;
  }

  // make button to jump to next image in document
  function makeNextButton(img) {
    const nextButton = document.createElement("button");
    nextButton.id = "lightbox_next_button";
    nextButton.title = "Jump to the next image in the document [→]";
    nextButton.classList.add("icon_button", "lightbox_button");
    nextButton.innerHTML = document.querySelector(
      ".icon_caret_right"
    ).innerHTML;

    // attach click listeners to button
    nextButton.addEventListener("click", function () {
      getNextImg(img).click();
    });

    return nextButton;
  }

  // get previous image in document
  function getPrevImg(img) {
    const imgs = document.querySelectorAll(".lightbox_document_img");

    // find index of provided img
    let index;
    for (index = 0; index < imgs.length; index++) {
      if (imgs[index] === img) break;
    }

    // wrap index to other side if < 1
    if (index - 1 >= 0) index--;
    else index = imgs.length - 1;
    return imgs[index];
  }

  // get next image in document
  function getNextImg(img) {
    const imgs = document.querySelectorAll(".lightbox_document_img");

    // find index of provided img
    let index;
    for (index = 0; index < imgs.length; index++) {
      if (imgs[index] === img) break;
    }

    // wrap index to other side if > total
    if (index + 1 <= imgs.length - 1) index++;
    else index = 0;
    return imgs[index];
  }

  // close lightbox
  function closeLightbox() {
    focusBody();

    const lightbox = document.getElementById("lightbox_overlay");
    if (lightbox) lightbox.remove();
  }

  // make all elements behind lightbox non-focusable
  function blurBody(overlay) {
    const all = document.querySelectorAll("*");
    for (const element of all) element.tabIndex = -1;
    document.body.classList.add("body_no_scroll");
  }

  // make all elements focusable again
  function focusBody() {
    const all = document.querySelectorAll("*");
    for (const element of all) element.removeAttribute("tabIndex");
    document.body.classList.remove("body_no_scroll");
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- caret left icon -->

<template class="icon_caret_left">
  <!-- modified from: https://fontawesome.com/icons/caret-left -->
  <svg width="16" height="16" viewBox="0 0 192 512">
    <path
      fill="currentColor"
      d="M192 127.338v257.324c0 17.818-21.543 26.741-34.142 14.142L29.196 270.142c-7.81-7.81-7.81-20.474 0-28.284l128.662-128.662c12.599-12.6 34.142-3.676 34.142 14.142z"
    ></path>
  </svg>
</template>

<!-- caret right icon -->

<template class="icon_caret_right">
  <!-- modified from: https://fontawesome.com/icons/caret-right -->
  <svg width="16" height="16" viewBox="0 0 192 512">
    <path
      fill="currentColor"
      d="M0 384.662V127.338c0-17.818 21.543-26.741 34.142-14.142l128.662 128.662c7.81 7.81 7.81 20.474 0 28.284L34.142 398.804C21.543 411.404 0 402.48 0 384.662z"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* regular <img> in document when hovered */
    img.lightbox_document_img:hover {
      cursor: pointer;
    }

    .body_no_scroll {
      overflow: hidden !important;
    }

    /* screen overlay */
    #lightbox_overlay {
      display: flex;
      flex-direction: column;
      position: fixed;
      left: 0;
      top: 0;
      right: 0;
      bottom: 0;
      background: rgba(0, 0, 0, 0.75);
      z-index: 3;
    }

    /* middle area containing lightbox image */
    #lightbox_image_container {
      flex-grow: 1;
      display: flex;
      justify-content: center;
      align-items: center;
      overflow: hidden;
      position: relative;
      padding: 20px;
    }

    /* bottom area containing caption */
    #lightbox_bottom_container {
      display: flex;
      justify-content: center;
      align-items: center;
      height: 100px;
      min-height: 100px;
      max-height: 100px;
      background: rgba(0, 0, 0, 0.5);
    }

    /* image number info text box */
    #lightbox_number_info {
      position: absolute;
      color: #ffffff;
      font-weight: 600;
      left: 2px;
      top: 0;
      z-index: 4;
    }

    /* zoom info text box */
    #lightbox_zoom_info {
      position: absolute;
      color: #ffffff;
      font-weight: 600;
      right: 2px;
      top: 0;
      z-index: 4;
    }

    /* copy of image caption */
    #lightbox_caption {
      box-sizing: border-box;
      display: inline-block;
      width: 100%;
      max-height: 100%;
      padding: 10px 0;
      text-align: center;
      overflow-y: auto;
      color: #ffffff;
    }

    /* navigation previous/next button */
    .lightbox_button {
      width: 100px;
      height: 100%;
      min-width: 100px;
      min-height: 100%;
      color: #ffffff;
    }

    /* navigation previous/next button when hovered */
    .lightbox_button:hover {
      background: none !important;
    }

    /* navigation button icon */
    .lightbox_button > svg {
      height: 25px;
    }

    /* figure auto-number */
    #lightbox_caption > span:first-of-type {
      font-weight: bold;
      margin-right: 5px;
    }

    /* lightbox image when hovered */
    #lightbox_img:hover {
      cursor: grab;
    }

    /* lightbox image when grabbed */
    #lightbox_img:active {
      cursor: grabbing;
    }
  }

  /* when on screen < 480px wide */
  @media only screen and (max-width: 480px) {
    /* make navigation buttons skinnier on small screens to make more room for caption text */
    .lightbox_button {
      width: 50px;
      min-width: 50px;
    }
  }

  /* always hide lightbox on print */
  @media only print {
    #lightbox_overlay {
      display: none;
    }
  }
</style>
<!-- 
  Link Highlight Plugin

  Makes it such that when a user hovers or focuses a link, other links that have
  the same target will be highlighted. It also makes it such that when clicking
  a link, the target of the link (eg reference, figure, table) is briefly
  highlighted.
-->

<script type="module">
  // whether to also highlight links that go to external urls
  const externalLinks = "false";
  // whether user must click off to unhighlight instead of just
  // un-hovering
  const clickUnhighlight = "false";
  // whether to also highlight links that are unique
  const highlightUnique = "true";

  // start script
  function start() {
    const links = getLinks();
    for (const link of links) {
      // attach mouse and focus listeners to link
      link.addEventListener("mouseenter", onLinkFocus);
      link.addEventListener("focus", onLinkFocus);
      link.addEventListener("mouseleave", onLinkUnhover);
    }

    // attach click and hash change listeners to window
    window.addEventListener("click", onClick);
    window.addEventListener("touchstart", onClick);
    window.addEventListener("hashchange", onHashChange);

    // run hash change on window load in case user has navigated
    // directly to hash
    onHashChange();
  }

  // when link is focused (tabbed to) or hovered
  function onLinkFocus() {
    highlight(this);
  }

  // when link is unhovered
  function onLinkUnhover() {
    if (clickUnhighlight !== "true") unhighlightAll();
  }

  // when the mouse is clicked anywhere in window
  function onClick(event) {
    unhighlightAll();
  }

  // when hash (eg manuscript.html#introduction) changes
  function onHashChange() {
    const target = getHashTarget();
    if (target) glowElement(target);
  }

  // start glow sequence on an element
  function glowElement(element) {
    const startGlow = function () {
      onGlowEnd();
      element.dataset.glow = "true";
      element.addEventListener("animationend", onGlowEnd);
    };
    const onGlowEnd = function () {
      element.removeAttribute("data-glow");
      element.removeEventListener("animationend", onGlowEnd);
    };
    startGlow();
  }

  // highlight link and all others with same target
  function highlight(link) {
    // force unhighlight all to start fresh
    unhighlightAll();

    // get links with same target
    if (!link) return;
    const sameLinks = getSameLinks(link);

    // if link unique and option is off, exit and don't highlight
    if (sameLinks.length <= 1 && highlightUnique !== "true") return;

    // highlight all same links, and "select" (special highlight) this
    // one
    for (const sameLink of sameLinks) {
      if (sameLink === link) sameLink.setAttribute("data-selected", "true");
      else sameLink.setAttribute("data-highlighted", "true");
    }
  }

  // unhighlight all links
  function unhighlightAll() {
    const links = getLinks();
    for (const link of links) {
      link.setAttribute("data-selected", "false");
      link.setAttribute("data-highlighted", "false");
    }
  }

  // get links with same target
  function getSameLinks(link) {
    const results = [];
    const links = getLinks();
    for (const otherLink of links) {
      if (otherLink.getAttribute("href") === link.getAttribute("href"))
        results.push(otherLink);
    }
    return results;
  }

  // get all links of types we wish to handle
  function getLinks() {
    let query = "a";
    if (externalLinks !== "true") query += '[href^="#"]';
    // exclude buttons, anchor links, toc links, etc
    query += ":not(.button):not(.icon_button):not(.anchor):not(.toc_link)";
    return document.querySelectorAll(query);
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<style>
  @media only screen {
    /* anything with data-highlighted attribute true */
    [data-highlighted="true"] {
      background: #ffeb3b;
    }

    /* anything with data-selected attribute true */
    [data-selected="true"] {
      background: #ff8a65 !important;
    }

    /* animation definition for glow */
    @keyframes highlight_glow {
      0% {
        background: none;
      }
      10% {
        background: #bbdefb;
      }
      100% {
        background: none;
      }
    }

    /* anything with data-glow attribute true */
    [data-glow="true"] {
      animation: highlight_glow 2s;
    }
  }
</style>
<!--
  Table of Contents Plugin

  Provides a "table of contents" (toc) panel on the side of the document that
  allows the user to conveniently navigate between sections of the document.
-->

<script type="module">
  // which types of elements to add links for, in "document.querySelector" format
  const typesQuery = "h1, h2, h3";
  // whether toc starts open. use 'true' or 'false', or 'auto' to
  // use 'true' behavior when screen wide enough and 'false' when not
  const startOpen = "false";
  // whether toc closes when clicking on toc link. use 'true' or
  // 'false', or 'auto' to use 'false' behavior when screen wide
  // enough and 'true' when not
  const clickClose = "auto";
  // if list item is more than this many characters, text will be
  // truncated
  const charLimit = "50";
  // whether or not to show bullets next to each toc item
  const bullets = "false";

  // start script
  function start() {
    // make toc panel and populate with entries (links to document
    // sections)
    const panel = makePanel();
    if (!panel) return;
    makeEntries(panel);
    // attach panel to document after making entries, so 'toc' heading
    // in panel isn't included in toc
    document.body.insertBefore(panel, document.body.firstChild);

    // initial panel state
    if (startOpen === "true" || (startOpen === "auto" && !isSmallScreen()))
      openPanel();
    else closePanel();

    // attach click, scroll, and hash change listeners to window
    window.addEventListener("click", onClick);
    window.addEventListener("scroll", onScroll);
    window.addEventListener("hashchange", onScroll);
    window.addEventListener("keyup", onKeyUp);
    onScroll();

    // add class to push document body down out of way of toc button
    document.body.classList.add("toc_body_nudge");
  }

  // determine if screen wide enough to fit toc panel
  function isSmallScreen() {
    // in default theme:
    // 816px = 8.5in = width of "page" (<body>) element
    // 260px = min width of toc panel (*2 for both sides of <body>)
    return window.innerWidth < 816 + 260 * 2;
  }

  // when mouse is clicked anywhere in window
  function onClick() {
    if (isSmallScreen()) closePanel();
  }

  // when window is scrolled or hash changed
  function onScroll() {
    highlightViewed();
  }

  // when key pressed
  function onKeyUp(event) {
    if (!event || !event.key) return;

    // close on esc
    if (event.key === "Escape") closePanel();
  }

  // find entry of currently viewed document section in toc and highlight
  function highlightViewed() {
    const firstId = getFirstInView(typesQuery);

    // get toc entries (links), unhighlight all, then highlight viewed
    const list = document.getElementById("toc_list");
    if (!firstId || !list) return;
    const links = list.querySelectorAll("a");
    for (const link of links) link.dataset.viewing = "false";
    const link = list.querySelector('a[href="#' + firstId + '"]');
    if (!link) return;
    link.dataset.viewing = "true";
  }

  // get first or previous toc listed element in top half of view
  function getFirstInView(query) {
    // get all elements matching query and with id
    const elements = document.querySelectorAll(query);
    const elementsWithIds = [];
    for (const element of elements) {
      if (element.id) elementsWithIds.push(element);
    }

    // get first or previous element in top half of view
    for (let i = 0; i < elementsWithIds.length; i++) {
      const element = elementsWithIds[i];
      const prevElement = elementsWithIds[Math.max(0, i - 1)];
      if (element.getBoundingClientRect().top >= 0) {
        if (element.getBoundingClientRect().top < window.innerHeight / 2)
          return element.id;
        else return prevElement.id;
      }
    }
  }

  // make panel
  function makePanel() {
    // create panel
    const panel = document.createElement("div");
    panel.id = "toc_panel";
    if (bullets === "true") panel.dataset.bullets = "true";

    // create header
    const header = document.createElement("div");
    header.id = "toc_header";

    // create toc button
    const button = document.createElement("button");
    button.id = "toc_button";
    button.innerHTML = document.querySelector(".icon_th_list").innerHTML;
    button.title = "Table of Contents";
    button.classList.add("icon_button");

    // create header text
    const text = document.createElement("h4");
    text.innerHTML = "Table of Contents";

    // create container for toc list
    const list = document.createElement("div");
    list.id = "toc_list";

    // attach click listeners
    panel.addEventListener("click", onPanelClick);
    header.addEventListener("click", onHeaderClick);
    button.addEventListener("click", onButtonClick);

    // attach elements
    header.appendChild(button);
    header.appendChild(text);
    panel.appendChild(header);
    panel.appendChild(list);

    return panel;
  }

  // create toc entries (links) to each element of the specified types
  function makeEntries(panel) {
    const elements = document.querySelectorAll(typesQuery);
    for (const element of elements) {
      // do not add link if element doesn't have assigned id
      if (!element.id) continue;

      // create link/list item
      const link = document.createElement("a");
      link.classList.add("toc_link");
      switch (element.tagName.toLowerCase()) {
        case "h1":
          link.dataset.level = "1";
          break;
        case "h2":
          link.dataset.level = "2";
          break;
        case "h3":
          link.dataset.level = "3";
          break;
        case "h4":
          link.dataset.level = "4";
          break;
      }
      link.title = element.innerText;
      let text = element.innerText;
      if (text.length > charLimit) text = text.slice(0, charLimit) + "...";
      link.innerHTML = text;
      link.href = "#" + element.id;
      link.addEventListener("click", onLinkClick);

      // attach link
      panel.querySelector("#toc_list").appendChild(link);
    }
  }

  // when panel is clicked
  function onPanelClick(event) {
    // stop click from propagating to window/document and closing panel
    event.stopPropagation();
  }

  // when header itself is clicked
  function onHeaderClick(event) {
    togglePanel();
  }

  // when button is clicked
  function onButtonClick(event) {
    togglePanel();
    // stop header underneath button from also being clicked
    event.stopPropagation();
  }

  // when link is clicked
  function onLinkClick(event) {
    if (clickClose === "true" || (clickClose === "auto" && isSmallScreen()))
      closePanel();
    else openPanel();
  }

  // open panel if closed, close if opened
  function togglePanel() {
    const panel = document.getElementById("toc_panel");
    if (!panel) return;

    if (panel.dataset.open === "true") closePanel();
    else openPanel();
  }

  // open panel
  function openPanel() {
    const panel = document.getElementById("toc_panel");
    if (panel) panel.dataset.open = "true";
  }

  // close panel
  function closePanel() {
    const panel = document.getElementById("toc_panel");
    if (panel) panel.dataset.open = "false";
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- th list icon -->

<template class="icon_th_list">
  <!-- modified from: https://fontawesome.com/icons/th-list -->
  <svg width="16" height="16" viewBox="0 0 512 512" tabindex="-1">
    <path
      fill="currentColor"
      d="M96 96c0 26.51-21.49 48-48 48S0 122.51 0 96s21.49-48 48-48 48 21.49 48 48zM48 208c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48zm0 160c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48zm96-236h352c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H144c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h352c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H144c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h352c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H144c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"
      tabindex="-1"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* toc panel */
    #toc_panel {
      box-sizing: border-box;
      position: fixed;
      top: 0;
      left: 0;
      background: #ffffff;
      box-shadow: 0 0 20px rgba(0, 0, 0, 0.05);
      z-index: 2;
    }

    /* toc panel when closed */
    #toc_panel[data-open="false"] {
      min-width: 60px;
      width: 60px;
      height: 60px;
      border-right: solid 1px #bdbdbd;
      border-bottom: solid 1px #bdbdbd;
    }

    /* toc panel when open */
    #toc_panel[data-open="true"] {
      min-width: 260px;
      max-width: 480px;
      /* keep panel edge consistent distance away from "page" edge */
      width: calc(((100vw - 8.5in) / 2) - 30px - 40px);
      bottom: 0;
      border-right: solid 1px #bdbdbd;
    }

    /* toc panel header */
    #toc_header {
      box-sizing: border-box;
      display: flex;
      flex-direction: row;
      align-items: center;
      height: 60px;
      margin: 0;
      padding: 20px;
    }

    /* toc panel header when hovered */
    #toc_header:hover {
      cursor: pointer;
    }

    /* toc panel header when panel open */
    #toc_panel[data-open="true"] > #toc_header {
      border-bottom: solid 1px #bdbdbd;
    }

    /* toc open/close header button */
    #toc_button {
      margin-right: 20px;
    }

    /* hide toc list and header text when closed */
    #toc_panel[data-open="false"] > #toc_header > *:not(#toc_button),
    #toc_panel[data-open="false"] > #toc_list {
      display: none;
    }

    /* toc list of entries */
    #toc_list {
      box-sizing: border-box;
      width: 100%;
      padding: 20px;
      position: absolute;
      top: calc(60px + 1px);
      bottom: 0;
      overflow: auto;
    }

    /* toc entry, link to section in document */
    .toc_link {
      display: block;
      padding: 5px;
      position: relative;
      font-weight: 600;
      text-decoration: none;
    }

    /* toc entry when hovered or when "viewed" */
    .toc_link:hover,
    .toc_link[data-viewing="true"] {
      background: #f5f5f5;
    }

    /* toc entry, level 1 indentation */
    .toc_link[data-level="1"] {
      margin-left: 0;
    }

    /* toc entry, level 2 indentation */
    .toc_link[data-level="2"] {
      margin-left: 20px;
    }

    /* toc entry, level 3 indentation */
    .toc_link[data-level="3"] {
      margin-left: 40px;
    }

    /* toc entry, level 4 indentation */
    .toc_link[data-level="4"] {
      margin-left: 60px;
    }

    /* toc entry bullets */
    #toc_panel[data-bullets="true"] .toc_link[data-level]:before {
      position: absolute;
      left: -15px;
      top: -1px;
      font-size: 1.5em;
    }

    /* toc entry, level 2 bullet */
    #toc_panel[data-bullets="true"] .toc_link[data-level="2"]:before {
      content: "\2022";
    }

    /* toc entry, level 3 bullet */
    #toc_panel[data-bullets="true"] .toc_link[data-level="3"]:before {
      content: "\25AB";
    }

    /* toc entry, level 4 bullet */
    #toc_panel[data-bullets="true"] .toc_link[data-level="4"]:before {
      content: "-";
    }
  }

  /* when on screen < 8.5in wide */
  @media only screen and (max-width: 8.5in) {
    /* push <body> ("page") element down to make room for toc icon */
    .toc_body_nudge {
      padding-top: 60px;
    }

    /* toc icon when panel closed and not hovered */
    #toc_panel[data-open="false"]:not(:hover) {
      background: rgba(255, 255, 255, 0.75);
    }
  }

  /* always hide toc panel on print */
  @media only print {
    #toc_panel {
      display: none;
    }
  }
</style>
<!-- 
  Tooltips Plugin

  Makes it such that when the user hovers or focuses a link to a citation or
  figure, a tooltip appears with a preview of the reference content, along with
  arrows to navigate between instances of the same reference in the document.
-->

<script type="module">
  // whether user must click off to close tooltip instead of just un-hovering
  const clickClose = "false";
  // delay (in ms) between opening and closing tooltip
  const delay = "100";

  // start script
  function start() {
    const links = getLinks();
    for (const link of links) {
      // attach hover and focus listeners to link
      link.addEventListener("mouseover", onLinkHover);
      link.addEventListener("mouseleave", onLinkUnhover);
      link.addEventListener("focus", onLinkFocus);
      link.addEventListener("touchend", onLinkTouch);
    }

    // attach mouse, key, and resize listeners to window
    window.addEventListener("mousedown", onClick);
    window.addEventListener("touchstart", onClick);
    window.addEventListener("keyup", onKeyUp);
    window.addEventListener("resize", onResize);
  }

  // when link is hovered
  function onLinkHover() {
    // function to open tooltip
    const delayOpenTooltip = function () {
      openTooltip(this);
    }.bind(this);

    // run open function after delay
    this.openTooltipTimer = window.setTimeout(delayOpenTooltip, delay);
  }

  // when mouse leaves link
  function onLinkUnhover() {
    // cancel opening tooltip
    window.clearTimeout(this.openTooltipTimer);

    // don't close on unhover if option specifies
    if (clickClose === "true") return;

    // function to close tooltip
    const delayCloseTooltip = function () {
      // if tooltip open and if mouse isn't over tooltip, close
      const tooltip = document.getElementById("tooltip");
      if (tooltip && !tooltip.matches(":hover")) closeTooltip();
    };

    // run close function after delay
    this.closeTooltipTimer = window.setTimeout(delayCloseTooltip, delay);
  }

  // when link is focused (tabbed to)
  function onLinkFocus(event) {
    openTooltip(this);
  }

  // when link is touched on touch screen
  function onLinkTouch(event) {
    // attempt to force hover state on first tap always, and trigger
    // regular link click (and navigation) on second tap
    if (event.target === document.activeElement) event.target.click();
    else {
      document.activeElement.blur();
      event.target.focus();
    }
    if (event.cancelable) event.preventDefault();
    event.stopPropagation();
    return false;
  }

  // when mouse is clicked anywhere in window
  function onClick(event) {
    closeTooltip();
  }

  // when key pressed
  function onKeyUp(event) {
    if (!event || !event.key) return;

    switch (event.key) {
      // trigger click of prev button
      case "ArrowLeft":
        const prevButton = document.getElementById("tooltip_prev_button");
        if (prevButton) prevButton.click();
        break;
      // trigger click of next button
      case "ArrowRight":
        const nextButton = document.getElementById("tooltip_next_button");
        if (nextButton) nextButton.click();
        break;
      // close on esc
      case "Escape":
        closeTooltip();
        break;
    }
  }

  // when window is resized or zoomed
  function onResize() {
    closeTooltip();
  }

  // get all links of types we wish to handle
  function getLinks() {
    const queries = [];
    // exclude buttons, anchor links, toc links, etc
    const exclude =
      ":not(.button):not(.icon_button):not(.anchor):not(.toc_link)";
    queries.push('a[href^="#ref-"]' + exclude); // citation links
    queries.push('a[href^="#fig:"]' + exclude); // figure links
    const query = queries.join(", ");
    return document.querySelectorAll(query);
  }

  // get links with same target, get index of link in set, get total
  // same links
  function getSameLinks(link) {
    const sameLinks = [];
    const links = getLinks();
    for (const otherLink of links) {
      if (otherLink.getAttribute("href") === link.getAttribute("href"))
        sameLinks.push(otherLink);
    }

    return {
      elements: sameLinks,
      index: sameLinks.indexOf(link),
      total: sameLinks.length,
    };
  }

  // open tooltip
  function openTooltip(link) {
    // delete tooltip if it exists, start fresh
    closeTooltip();

    // make tooltip element
    const tooltip = makeTooltip(link);

    // if source couldn't be found and tooltip not made, exit
    if (!tooltip) return;

    // make navbar elements
    const navBar = makeNavBar(link);
    if (navBar) tooltip.firstElementChild.appendChild(navBar);

    // attach tooltip to page
    document.body.appendChild(tooltip);

    // position tooltip
    const position = function () {
      positionTooltip(link);
    };
    position();

    // if tooltip contains images, position again after they've loaded
    const imgs = tooltip.querySelectorAll("img");
    for (const img of imgs) img.addEventListener("load", position);
  }

  // close (delete) tooltip
  function closeTooltip() {
    const tooltip = document.getElementById("tooltip");
    if (tooltip) tooltip.remove();
  }

  // make tooltip
  function makeTooltip(link) {
    // get target element that link points to
    const source = getSource(link);

    // if source can't be found, exit
    if (!source) return;

    // create new tooltip
    const tooltip = document.createElement("div");
    tooltip.id = "tooltip";
    const tooltipContent = document.createElement("div");
    tooltipContent.id = "tooltip_content";
    tooltip.appendChild(tooltipContent);

    // make copy of source node and put in tooltip
    const sourceCopy = makeCopy(source);
    tooltipContent.appendChild(sourceCopy);

    // attach mouse event listeners
    tooltip.addEventListener("click", onTooltipClick);
    tooltip.addEventListener("mousedown", onTooltipClick);
    tooltip.addEventListener("touchstart", onTooltipClick);
    tooltip.addEventListener("mouseleave", onTooltipUnhover);

    // (for interaction with lightbox plugin)
    // transfer click on tooltip copied img to original img
    const sourceImg = source.querySelector("img");
    const sourceCopyImg = sourceCopy.querySelector("img");
    if (sourceImg && sourceCopyImg) {
      const clickImg = function () {
        sourceImg.click();
        closeTooltip();
      };
      sourceCopyImg.addEventListener("click", clickImg);
    }

    return tooltip;
  }

  // make carbon copy of html dom element
  function makeCopy(source) {
    const sourceCopy = source.cloneNode(true);

    // delete elements marked with ignore (eg anchor and jump buttons)
    const deleteFromCopy = sourceCopy.querySelectorAll('[data-ignore="true"]');
    for (const element of deleteFromCopy) element.remove();

    // delete certain element attributes
    const attributes = [
      "id",
      "data-collapsed",
      "data-selected",
      "data-highlighted",
      "data-glow",
      "class",
    ];
    for (const attribute of attributes) {
      sourceCopy.removeAttribute(attribute);
      const elements = sourceCopy.querySelectorAll("[" + attribute + "]");
      for (const element of elements) element.removeAttribute(attribute);
    }

    return sourceCopy;
  }

  // when tooltip is clicked
  function onTooltipClick(event) {
    // when user clicks on tooltip, stop click from transferring
    // outside of tooltip (eg, click off to close tooltip, or eg click
    // off to unhighlight same refs)
    event.stopPropagation();
  }

  // when tooltip is unhovered
  function onTooltipUnhover(event) {
    if (clickClose === "true") return;

    // make sure new mouse/touch/focus no longer over tooltip or any
    // element within it
    const tooltip = document.getElementById("tooltip");
    if (!tooltip) return;
    if (this.contains(event.relatedTarget)) return;

    closeTooltip();
  }

  // make nav bar to go betwen prev/next instances of same reference
  function makeNavBar(link) {
    // find other links to the same source
    const sameLinks = getSameLinks(link);

    // don't show nav bar when singular reference
    if (sameLinks.total <= 1) return;

    // find prev/next links with same target
    const prevLink = getPrevLink(link, sameLinks);
    const nextLink = getNextLink(link, sameLinks);

    // create nav bar
    const navBar = document.createElement("div");
    navBar.id = "tooltip_nav_bar";
    const text = sameLinks.index + 1 + " of " + sameLinks.total;

    // create nav bar prev/next buttons
    const prevButton = document.createElement("button");
    const nextButton = document.createElement("button");
    prevButton.id = "tooltip_prev_button";
    nextButton.id = "tooltip_next_button";
    prevButton.title =
      "Jump to the previous occurence of this item in the document [←]";
    nextButton.title =
      "Jump to the next occurence of this item in the document [→]";
    prevButton.classList.add("icon_button");
    nextButton.classList.add("icon_button");
    prevButton.innerHTML = document.querySelector(".icon_caret_left").innerHTML;
    nextButton.innerHTML =
      document.querySelector(".icon_caret_right").innerHTML;
    navBar.appendChild(prevButton);
    navBar.appendChild(document.createTextNode(text));
    navBar.appendChild(nextButton);

    // attach click listeners to buttons
    prevButton.addEventListener("click", function () {
      onPrevNextClick(link, prevLink);
    });
    nextButton.addEventListener("click", function () {
      onPrevNextClick(link, nextLink);
    });

    return navBar;
  }

  // get previous link with same target
  function getPrevLink(link, sameLinks) {
    if (!sameLinks) sameLinks = getSameLinks(link);
    // wrap index to other side if < 1
    let index;
    if (sameLinks.index - 1 >= 0) index = sameLinks.index - 1;
    else index = sameLinks.total - 1;
    return sameLinks.elements[index];
  }

  // get next link with same target
  function getNextLink(link, sameLinks) {
    if (!sameLinks) sameLinks = getSameLinks(link);
    // wrap index to other side if > total
    let index;
    if (sameLinks.index + 1 <= sameLinks.total - 1) index = sameLinks.index + 1;
    else index = 0;
    return sameLinks.elements[index];
  }

  // get element that is target of link or url hash
  function getSource(link) {
    const hash = link ? link.hash : window.location.hash;
    const id = hash.slice(1);
    let target = document.querySelector('[id="' + id + '"]');
    if (!target) return;

    // if ref or figure, modify target to get expected element
    if (id.indexOf("ref-") === 0) target = target.querySelector(":nth-child(2)");
    else if (id.indexOf("fig:") === 0) target = target.querySelector("figure");

    return target;
  }

  // when prev/next arrow button is clicked
  function onPrevNextClick(link, prevNextLink) {
    if (link && prevNextLink)
      goToElement(prevNextLink, window.innerHeight * 0.5);
  }

  // scroll to and focus element
  function goToElement(element, offset) {
    // expand accordion section if collapsed
    expandElement(element);
    const y =
      getRectInView(element).top -
      getRectInView(document.documentElement).top -
      (offset || 0);
    // trigger any function listening for "onscroll" event
    window.dispatchEvent(new Event("scroll"));
    window.scrollTo(0, y);
    document.activeElement.blur();
    element.focus();
  }

  // determine position to place tooltip based on link position in
  // viewport and tooltip size
  function positionTooltip(link, left, top) {
    const tooltipElement = document.getElementById("tooltip");
    if (!tooltipElement) return;

    // get convenient vars for position/dimensions of
    // link/tooltip/page/view
    link = getRectInPage(link);
    const tooltip = getRectInPage(tooltipElement);
    const view = getRectInPage();

    // horizontal positioning
    if (left)
      // use explicit value
      left = left;
    else if (link.left + tooltip.width < view.right)
      // fit tooltip to right of link
      left = link.left;
    else if (link.right - tooltip.width > view.left)
      // fit tooltip to left of link
      left = link.right - tooltip.width;
    // center tooltip in view
    else left = (view.right - view.left) / 2 - tooltip.width / 2;

    // vertical positioning
    if (top)
      // use explicit value
      top = top;
    else if (link.top - tooltip.height > view.top)
      // fit tooltip above link
      top = link.top - tooltip.height;
    else if (link.bottom + tooltip.height < view.bottom)
      // fit tooltip below link
      top = link.bottom;
    else {
      // center tooltip in view
      top = view.top + view.height / 2 - tooltip.height / 2;
      // nudge off of link to left/right if possible
      if (link.right + tooltip.width < view.right) left = link.right;
      else if (link.left - tooltip.width > view.left)
        left = link.left - tooltip.width;
    }

    tooltipElement.style.left = left + "px";
    tooltipElement.style.top = top + "px";
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- caret left icon -->

<template class="icon_caret_left">
  <!-- modified from: https://fontawesome.com/icons/caret-left -->
  <svg width="16" height="16" viewBox="0 0 192 512">
    <path
      fill="currentColor"
      d="M192 127.338v257.324c0 17.818-21.543 26.741-34.142 14.142L29.196 270.142c-7.81-7.81-7.81-20.474 0-28.284l128.662-128.662c12.599-12.6 34.142-3.676 34.142 14.142z"
    ></path>
  </svg>
</template>

<!-- caret right icon -->

<template class="icon_caret_right">
  <!-- modified from: https://fontawesome.com/icons/caret-right -->
  <svg width="16" height="16" viewBox="0 0 192 512">
    <path
      fill="currentColor"
      d="M0 384.662V127.338c0-17.818 21.543-26.741 34.142-14.142l128.662 128.662c7.81 7.81 7.81 20.474 0 28.284L34.142 398.804C21.543 411.404 0 402.48 0 384.662z"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* tooltip container */
    #tooltip {
      position: absolute;
      width: 50%;
      min-width: 240px;
      max-width: 75%;
      z-index: 1;
    }

    /* tooltip content */
    #tooltip_content {
      margin-bottom: 5px;
      padding: 20px;
      border-radius: 5px;
      border: solid 1px #bdbdbd;
      box-shadow: 0 0 20px rgba(0, 0, 0, 0.05);
      background: #ffffff;
      overflow-wrap: break-word;
    }

    /* tooltip copy of paragraphs and figures */
    #tooltip_content > p,
    #tooltip_content > figure {
      margin: 0;
      max-height: 320px;
      overflow-y: auto;
    }

    /* tooltip copy of <img> */
    #tooltip_content > figure > img,
    #tooltip_content > figure > svg {
      max-height: 260px;
    }

    /* navigation bar */
    #tooltip_nav_bar {
      margin-top: 10px;
      text-align: center;
    }

    /* navigation bar previous/next buton */
    #tooltip_nav_bar > .icon_button {
      position: relative;
      top: 3px;
    }

    /* navigation bar previous button */
    #tooltip_nav_bar > .icon_button:first-of-type {
      margin-right: 5px;
    }

    /* navigation bar next button */
    #tooltip_nav_bar > .icon_button:last-of-type {
      margin-left: 5px;
    }
  }

  /* always hide tooltip on print */
  @media only print {
    #tooltip {
      display: none;
    }
  }
</style>
<!--
  Analytics Plugin (third-party) 
  
  Copy and paste code from Google Analytics or similar service here.
-->
<!-- 
  Annotations Plugin

  Allows public annotation of the  manuscript. See https://web.hypothes.is/.
-->

<script type="module">
  // configuration
  window.hypothesisConfig = function () {
    return {
      branding: {
        accentColor: "#2196f3",
        appBackgroundColor: "#f8f8f8",
        ctaBackgroundColor: "#f8f8f8",
        ctaTextColor: "#000000",
        selectionFontFamily: "Open Sans, Helvetica, sans serif",
        annotationFontFamily: "Open Sans, Helvetica, sans serif",
      },
    };
  };

  // hypothesis client script
  const embed = "https://hypothes.is/embed.js";
  // hypothesis annotation count query url
  const query = "https://api.hypothes.is/api/search?limit=0&url=";

  // start script
  function start() {
    const button = makeButton();
    document.body.insertBefore(button, document.body.firstChild);
    insertCount(button);
  }

  // make button
  function makeButton() {
    // create button
    const button = document.createElement("button");
    button.id = "hypothesis_button";
    button.innerHTML = document.querySelector(".icon_hypothesis").innerHTML;
    button.title = "Hypothesis annotations";
    button.classList.add("icon_button");

    function onClick(event) {
      onButtonClick(event, button);
    }

    // attach click listeners
    button.addEventListener("click", onClick);

    return button;
  }

  // insert annotations count
  async function insertCount(button) {
    // get annotation count from Hypothesis based on url
    let count = "-";
    try {
      const canonical = document.querySelector('link[rel="canonical"]');
      const location = window.location;
      const url = encodeURIComponent((canonical || location).href);
      const response = await fetch(query + url);
      const json = await response.json();
      count = json.total || "-";
    } catch (error) {
      console.log(error);
    }

    // put count into button
    const counter = document.createElement("span");
    counter.id = "hypothesis_count";
    counter.innerHTML = count;
    button.title = "View " + count + " Hypothesis annotations";
    button.append(counter);
  }

  // when button is clicked
  function onButtonClick(event, button) {
    const script = document.createElement("script");
    script.src = embed;
    document.body.append(script);
    button.remove();
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- hypothesis icon -->

<template class="icon_hypothesis">
  <!-- modified from: https://simpleicons.org/icons/hypothesis.svg / https://git.io/Jf1VB -->
  <svg width="16" height="16" viewBox="0 0 24 24" tabindex="-1">
    <path
      fill="currentColor"
      d="M3.43 0C2.5 0 1.72 .768 1.72 1.72V18.86C1.72 19.8 2.5 20.57 3.43 20.57H9.38L12 24L14.62 20.57H20.57C21.5 20.57 22.29 19.8 22.29 18.86V1.72C22.29 .77 21.5 0 20.57 0H3.43M5.14 3.43H7.72V9.43S8.58 7.72 10.28 7.72C12 7.72 13.74 8.57 13.74 11.24V17.14H11.16V12C11.16 10.61 10.28 10.07 9.43 10.29C8.57 10.5 7.72 11.41 7.72 13.29V17.14H5.14V3.43M18 13.72C18.95 13.72 19.72 14.5 19.72 15.42A1.71 1.71 0 0 1 18 17.13A1.71 1.71 0 0 1 16.29 15.42C16.29 14.5 17.05 13.71 18 13.71Z"
      tabindex="-1"
    ></path>
  </svg>
</template>

<style>
  /* hypothesis activation button */
  #hypothesis_button {
    box-sizing: border-box;
    position: fixed;
    top: 0;
    right: 0;
    width: 60px;
    height: 60px;
    background: #ffffff;
    border-radius: 0;
    border-left: solid 1px #bdbdbd;
    border-bottom: solid 1px #bdbdbd;
    box-shadow: 0 0 20px rgba(0, 0, 0, 0.05);
    z-index: 2;
  }

  /* hypothesis button svg */
  #hypothesis_button > svg {
    position: relative;
    top: -4px;
  }

  /* hypothesis annotation count */
  #hypothesis_count {
    position: absolute;
    left: 0;
    right: 0;
    bottom: 5px;
  }

  /* side panel */
  .annotator-frame {
    width: 280px !important;
  }

  /* match highlight color to rest of theme */
  .annotator-highlights-always-on .annotator-hl {
    background-color: #ffeb3b !important;
  }

  /* match focused color to rest of theme */
  .annotator-hl.annotator-hl-focused {
    background-color: #ff8a65 !important;
  }

  /* match bucket bar color to rest of theme */
  .annotator-bucket-bar {
    background: #f5f5f5 !important;
  }

  /* always hide button, toolbar, and tooltip on print */
  @media only print {
    #hypothesis_button {
      display: none;
    }

    .annotator-frame {
      display: none !important;
    }

    hypothesis-adder {
      display: none !important;
    }
  }
</style>
<!-- 
  Mathjax Plugin (third-party) 

  Allows the proper rendering of math/equations written in LaTeX.
  See https://www.mathjax.org/.
-->

<script type="text/x-mathjax-config">
  // configuration
  MathJax.Hub.Config({
    "CommonHTML": { linebreaks: { automatic: true } },
    "HTML-CSS": { linebreaks: { automatic: true } },
    "SVG": { linebreaks: { automatic: true } },
    "fast-preview": { disabled: true }
  });
</script>

<script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
  integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A=="
  crossorigin="anonymous"
></script>

<style>
  /* mathjax containers */
  .math.display > span:not(.MathJax_Preview) {
    /* turn inline element (no dimensions) into block (allows fixed width and thus scrolling) */
    display: flex !important;
    overflow-x: auto !important;
    overflow-y: hidden !important;
    justify-content: center;
    align-items: center;
    margin: 0 !important;
  }

  /* right click menu */
  .MathJax_Menu {
    border-radius: 5px !important;
    border: solid 1px #bdbdbd !important;
    box-shadow: none !important;
  }

  /* equation auto-number */
  span[id^="eq:"] > span.math.display + span {
    font-weight: 600;
  }

  /* equation */
  span[id^="eq:"] > span.math.display > span {
    /* nudge to make room for equation auto-number and anchor */
    margin-right: 60px !important;
  }
</style>
</body>
</html>
